{"created": 1747448527.2693586, "duration": 88.69425964355469, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 72, "skipped": 4, "failed": 38, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.05553899899996395, "outcome": "passed"}, "call": {"duration": 0.387318735000008, "outcome": "passed"}, "teardown": {"duration": 0.0003603220000059082, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.023337011999956303, "outcome": "passed"}, "call": {"duration": 0.24254711900016446, "outcome": "passed"}, "teardown": {"duration": 0.000278650000154812, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.024034350999954768, "outcome": "passed"}, "call": {"duration": 0.43019327900015014, "outcome": "passed"}, "teardown": {"duration": 0.0002738609998687025, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.023362690000112707, "outcome": "passed"}, "call": {"duration": 0.6564667919999465, "outcome": "passed"}, "teardown": {"duration": 0.0002143889998933446, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.02354494899987003, "outcome": "passed"}, "call": {"duration": 0.7026045800000702, "outcome": "passed"}, "teardown": {"duration": 0.00028047299997524533, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.023648481999998694, "outcome": "passed"}, "call": {"duration": 0.7079535389998455, "outcome": "passed"}, "teardown": {"duration": 0.0002102419998664118, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.024787586000002193, "outcome": "passed"}, "call": {"duration": 0.7704001849999713, "outcome": "passed"}, "teardown": {"duration": 0.0002106219999404857, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.02348381599995264, "outcome": "passed"}, "call": {"duration": 0.48073202599994147, "outcome": "passed"}, "teardown": {"duration": 0.00024360400016121275, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.023195639000050505, "outcome": "passed"}, "call": {"duration": 0.7472742269999344, "outcome": "passed"}, "teardown": {"duration": 0.00021511099998861027, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.02247667899996486, "outcome": "passed"}, "call": {"duration": 0.7700394030000552, "outcome": "passed"}, "teardown": {"duration": 0.00025452499994571554, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.022642426999937015, "outcome": "passed"}, "call": {"duration": 1.0768610939999235, "outcome": "passed"}, "teardown": {"duration": 0.00021151400005692267, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.022353970000040135, "outcome": "passed"}, "call": {"duration": 0.5811062900002071, "outcome": "passed"}, "teardown": {"duration": 0.00024041799997576163, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.02215153400015879, "outcome": "passed"}, "call": {"duration": 0.006124890999899435, "outcome": "passed"}, "teardown": {"duration": 0.00018192900006397394, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0223799489999692, "outcome": "passed"}, "call": {"duration": 0.006925026000089929, "outcome": "passed"}, "teardown": {"duration": 0.00021783600004710024, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022439038999891636, "outcome": "passed"}, "call": {"duration": 0.11460323500000413, "outcome": "passed"}, "teardown": {"duration": 0.00023779299999659997, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02244412900017778, "outcome": "passed"}, "call": {"duration": 0.09876724500009004, "outcome": "passed"}, "teardown": {"duration": 0.0003454840000358672, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023292931000014505, "outcome": "passed"}, "call": {"duration": 0.12285615300015706, "outcome": "passed"}, "teardown": {"duration": 0.0002675990001534956, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.02388492399995812, "outcome": "passed"}, "call": {"duration": 0.006240149999939604, "outcome": "passed"}, "teardown": {"duration": 0.00019162700004926592, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.023442789999990055, "outcome": "passed"}, "call": {"duration": 0.007027298999901177, "outcome": "passed"}, "teardown": {"duration": 0.0002627189999202528, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022831781999911982, "outcome": "passed"}, "call": {"duration": 0.1270666689999871, "outcome": "passed"}, "teardown": {"duration": 0.000359050000042771, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.025071617000094193, "outcome": "passed"}, "call": {"duration": 0.09981897800003026, "outcome": "passed"}, "teardown": {"duration": 0.00031067899999470683, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023255772000084107, "outcome": "passed"}, "call": {"duration": 0.1235783749998518, "outcome": "passed"}, "teardown": {"duration": 0.00021406899986686767, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022587336000015057, "outcome": "passed"}, "call": {"duration": 0.0062277659999381285, "outcome": "passed"}, "teardown": {"duration": 0.00032958399992821796, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02248712899995553, "outcome": "passed"}, "call": {"duration": 0.007098189999851456, "outcome": "passed"}, "teardown": {"duration": 0.00023925699997562333, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022601772999905734, "outcome": "passed"}, "call": {"duration": 0.12621641700002328, "outcome": "passed"}, "teardown": {"duration": 0.00024061899989646918, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02255436399991595, "outcome": "passed"}, "call": {"duration": 0.10100274299998091, "outcome": "passed"}, "teardown": {"duration": 0.00029013100015617965, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02471627499994611, "outcome": "passed"}, "call": {"duration": 0.12409343900003478, "outcome": "passed"}, "teardown": {"duration": 0.0003029550000519521, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.024619383000072048, "outcome": "passed"}, "call": {"duration": 0.006846301999985371, "outcome": "passed"}, "teardown": {"duration": 0.00023100100020201353, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02260494799998014, "outcome": "passed"}, "call": {"duration": 0.00783388400009244, "outcome": "passed"}, "teardown": {"duration": 0.00020302799998717092, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02220691699994859, "outcome": "passed"}, "call": {"duration": 0.10420248600007653, "outcome": "passed"}, "teardown": {"duration": 0.00031641000009585696, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0327400910000506, "outcome": "passed"}, "call": {"duration": 0.08130977700011499, "outcome": "passed"}, "teardown": {"duration": 0.00025391400004082243, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02296601200009718, "outcome": "passed"}, "call": {"duration": 0.10318564699991839, "outcome": "passed"}, "teardown": {"duration": 0.00025506500014671474, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.02348291499993138, "outcome": "passed"}, "call": {"duration": 0.005899690000205737, "outcome": "passed"}, "teardown": {"duration": 0.00021015099991927855, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02257436200011398, "outcome": "passed"}, "call": {"duration": 0.0065587440001309005, "outcome": "passed"}, "teardown": {"duration": 0.00018498400004318682, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022169265999991694, "outcome": "passed"}, "call": {"duration": 0.10283607200017286, "outcome": "passed"}, "teardown": {"duration": 0.00029012100003456, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.024247641000101794, "outcome": "passed"}, "call": {"duration": 0.08333214799995403, "outcome": "passed"}, "teardown": {"duration": 0.0002857420001873834, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023308068999995157, "outcome": "passed"}, "call": {"duration": 0.1043176409998523, "outcome": "passed"}, "teardown": {"duration": 0.0003307160000076692, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.023442278999937116, "outcome": "passed"}, "call": {"duration": 0.006283603999918341, "outcome": "passed"}, "teardown": {"duration": 0.0001841739999690617, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0222641630000453, "outcome": "passed"}, "call": {"duration": 0.006857516999843938, "outcome": "passed"}, "teardown": {"duration": 0.00022313599993140087, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02254173100004664, "outcome": "passed"}, "call": {"duration": 0.10426175299994611, "outcome": "passed"}, "teardown": {"duration": 0.00022982800010140636, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023495818999890616, "outcome": "passed"}, "call": {"duration": 0.08003863099997943, "outcome": "passed"}, "teardown": {"duration": 0.00026102700007868407, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02330185799996798, "outcome": "passed"}, "call": {"duration": 0.10351861499998449, "outcome": "passed"}, "teardown": {"duration": 0.0003026550000413408, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-3.3-70b-versatile-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.033798023999906945, "outcome": "passed"}, "call": {"duration": 0.0001836829999319889, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.00020265700004529208, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.021974082000042472, "outcome": "passed"}, "call": {"duration": 2.1453600209999877, "outcome": "passed"}, "teardown": {"duration": 0.00028836699993917136, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023082650000105787, "outcome": "passed"}, "call": {"duration": 1.9855194360000041, "outcome": "passed"}, "teardown": {"duration": 0.00023153200004344399, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-3.3-70b-versatile-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022677261999888287, "outcome": "passed"}, "call": {"duration": 0.00018426399992677034, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.0001635850001093786, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02179821400000037, "outcome": "passed"}, "call": {"duration": 2.292648388999851, "outcome": "passed"}, "teardown": {"duration": 0.000207917999887286, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02221743600011905, "outcome": "passed"}, "call": {"duration": 4.899561258000176, "outcome": "passed"}, "teardown": {"duration": 0.000302572999999029, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.023702113999888752, "outcome": "passed"}, "call": {"duration": 0.19050111999990804, "outcome": "passed"}, "teardown": {"duration": 0.00020395000001371955, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.02264925100007531, "outcome": "passed"}, "call": {"duration": 0.732429515999911, "outcome": "passed"}, "teardown": {"duration": 0.00020783700006177241, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.023376745999939885, "outcome": "passed"}, "call": {"duration": 0.4354275509999752, "outcome": "passed"}, "teardown": {"duration": 0.00033224899993911094, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.022726247000036892, "outcome": "passed"}, "call": {"duration": 1.13185122699997, "outcome": "passed"}, "teardown": {"duration": 0.00020440100001906103, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.02338837100001001, "outcome": "passed"}, "call": {"duration": 0.4116220590001376, "outcome": "passed"}, "teardown": {"duration": 0.00020263800001885102, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.024250456999880043, "outcome": "passed"}, "call": {"duration": 1.222194681000019, "outcome": "passed"}, "teardown": {"duration": 0.0003186040000855428, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.0347748279998541, "outcome": "passed"}, "call": {"duration": 0.21913827699995636, "outcome": "passed"}, "teardown": {"duration": 0.00026354099986747315, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.02301735900005042, "outcome": "passed"}, "call": {"duration": 0.7315528449998965, "outcome": "passed"}, "teardown": {"duration": 0.00021787600007883157, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.022502520999978515, "outcome": "passed"}, "call": {"duration": 1.4305339690001802, "outcome": "passed"}, "teardown": {"duration": 0.0002193990001160273, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.022176542999886806, "outcome": "passed"}, "call": {"duration": 1.0275787890000174, "outcome": "passed"}, "teardown": {"duration": 0.00028680500008704257, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.02239425800007666, "outcome": "passed"}, "call": {"duration": 0.49300837300006606, "outcome": "passed"}, "teardown": {"duration": 0.00020370900006128068, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.022723602000041865, "outcome": "passed"}, "call": {"duration": 1.2360667059999741, "outcome": "passed"}, "teardown": {"duration": 0.0002077970000300411, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02253690799989272, "outcome": "passed"}, "call": {"duration": 0.15628184100000908, "outcome": "passed"}, "teardown": {"duration": 0.00020506200007730513, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 221, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0229951210001218, "outcome": "passed"}, "call": {"duration": 0.4364448310000171, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError: assert 'brave_search' == 'get_weather'\n  \n  - get_weather\n  + brave_search"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53e45d80>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0\n        assert case[\"output\"] == \"get_weather_tool_call\"\n>       assert response.choices[0].message.tool_calls[0].function.name == \"get_weather\"\nE       AssertionError: assert 'brave_search' == 'get_weather'\nE         \nE         - get_weather\nE         + brave_search\n\ntests/verifications/openai_api/test_chat_completion.py:242: AssertionError"}, "teardown": {"duration": 0.0002454570001191314, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 221, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02253107599995019, "outcome": "passed"}, "call": {"duration": 0.870650102999889, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError: assert 'geolocation' == 'get_weather'\n  \n  - get_weather\n  + geolocation"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d91e70>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0\n        assert case[\"output\"] == \"get_weather_tool_call\"\n>       assert response.choices[0].message.tool_calls[0].function.name == \"get_weather\"\nE       AssertionError: assert 'geolocation' == 'get_weather'\nE         \nE         - get_weather\nE         + geolocation\n\ntests/verifications/openai_api/test_chat_completion.py:242: AssertionError"}, "teardown": {"duration": 0.00023090000013326062, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02262525200012533, "outcome": "passed"}, "call": {"duration": 0.1807823090000511, "outcome": "passed"}, "teardown": {"duration": 0.0002064650000193069, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02286496800002169, "outcome": "passed"}, "call": {"duration": 0.8098997739998595, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 268, "message": "AssertionError: assert ' get_weather' == 'get_weather'\n  \n  - get_weather\n  +  get_weather\n  ? +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 268, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53ca7ac0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n        assert len(tool_calls_buffer) == 1\n        for call in tool_calls_buffer:\n            assert len(call[\"id\"]) > 0\n            function = call[\"function\"]\n>           assert function[\"name\"] == \"get_weather\"\nE           AssertionError: assert ' get_weather' == 'get_weather'\nE             \nE             - get_weather\nE             +  get_weather\nE             ? +\n\ntests/verifications/openai_api/test_chat_completion.py:268: AssertionError"}, "teardown": {"duration": 0.0002268829998683941, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02228554999987864, "outcome": "passed"}, "call": {"duration": 0.5388146769998912, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 263, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53cbe140>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:263: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fdf53dd47c0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00024007799993341905, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022362131999898338, "outcome": "passed"}, "call": {"duration": 0.19034977100000106, "outcome": "passed"}, "teardown": {"duration": 0.00023616899989065132, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02373945699991964, "outcome": "passed"}, "call": {"duration": 0.49144812200006527, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError: assert 'brave_search' == 'get_weather'\n  \n  - get_weather\n  + brave_search"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53dc1060>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert response.choices[0].message.tool_calls[0].function.name == expected_tool_name\nE       AssertionError: assert 'brave_search' == 'get_weather'\nE         \nE         - get_weather\nE         + brave_search\n\ntests/verifications/openai_api/test_chat_completion.py:295: AssertionError"}, "teardown": {"duration": 0.00023156099996413104, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02226148399995509, "outcome": "passed"}, "call": {"duration": 0.7378548290000708, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError: assert 'gefunction=get_weather' == 'get_weather'\n  \n  - get_weather\n  + gefunction=get_weather"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53f5d210>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert response.choices[0].message.tool_calls[0].function.name == expected_tool_name\nE       AssertionError: assert 'gefunction=get_weather' == 'get_weather'\nE         \nE         - get_weather\nE         + gefunction=get_weather\n\ntests/verifications/openai_api/test_chat_completion.py:295: AssertionError"}, "teardown": {"duration": 0.00022513899989462516, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02222319299994524, "outcome": "passed"}, "call": {"duration": 0.1944485630001509, "outcome": "passed"}, "teardown": {"duration": 0.00022279500012700737, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022313461999829087, "outcome": "passed"}, "call": {"duration": 0.872159297999815, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 320, "message": "AssertionError: Expected tool call 'get_weather' not found in stream\nassert False\n +  where False = any(<generator object test_chat_streaming_tool_choice_required.<locals>.<genexpr> at 0x7fdf53d746d0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 320, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53ca35e0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n        assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert any(call[\"function\"][\"name\"] == expected_tool_name for call in tool_calls_buffer), (\n            f\"Expected tool call '{expected_tool_name}' not found in stream\"\n        )\nE       AssertionError: Expected tool call 'get_weather' not found in stream\nE       assert False\nE        +  where False = any(<generator object test_chat_streaming_tool_choice_required.<locals>.<genexpr> at 0x7fdf53d746d0>)\n\ntests/verifications/openai_api/test_chat_completion.py:320: AssertionError"}, "teardown": {"duration": 0.00023250300000654534, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02230136000002858, "outcome": "passed"}, "call": {"duration": 0.78465137399985, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 316, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53f74910>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:316: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fdf53f50a30>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00023124099993765412, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02209125799981848, "outcome": "passed"}, "call": {"duration": 0.867035990000204, "outcome": "passed"}, "teardown": {"duration": 0.00023641999996470986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023080703999994512, "outcome": "passed"}, "call": {"duration": 2.3676445450000756, "outcome": "passed"}, "teardown": {"duration": 0.00022506000004796078, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022736119000001054, "outcome": "passed"}, "call": {"duration": 0.3434318249999251, "outcome": "passed"}, "teardown": {"duration": 0.00021767499993075035, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02400889199998346, "outcome": "passed"}, "call": {"duration": 3.1413435350000327, "outcome": "passed"}, "teardown": {"duration": 0.00038414699997701973, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022247999999990498, "outcome": "passed"}, "call": {"duration": 6.3071058040000025, "outcome": "passed"}, "teardown": {"duration": 0.00030569000000468805, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022361329999966983, "outcome": "passed"}, "call": {"duration": 0.39709253999990324, "outcome": "passed"}, "teardown": {"duration": 0.0002121560000887257, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02247916900000746, "outcome": "passed"}, "call": {"duration": 0.20693708900012098, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_teh5', function=Function(arguments='{\"query\": \"latin name for the Sun\"}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_teh5', function=Function(arguments='{\"query\": \"latin name for the Sun\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_teh5', function=Function(arguments='{\"query\": \"latin name for the Sun\"}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d92830>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_teh5', function=Function(arguments='{\"query\": \"latin name for the Sun\"}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_teh5', function=Function(arguments='{\"query\": \"latin name for the Sun\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_teh5', function=Function(arguments='{\"query\": \"latin name for the Sun\"}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00020676500002991816, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02236298299999362, "outcome": "passed"}, "call": {"duration": 0.3625651629999993, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_baen', function=Function(arguments='{}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_baen', function=Function(arguments='{}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_baen', function=Function(arguments='{}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53f51e40>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_baen', function=Function(arguments='{}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_baen', function=Function(arguments='{}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_baen', function=Function(arguments='{}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00023084000008566363, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.022157700999969165, "outcome": "passed"}, "call": {"duration": 0.6495590360000278, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_hrbz', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_hrbz', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_hrbz', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53f76470>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_hrbz', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_hrbz', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_hrbz', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021407899998848734, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022083883000050264, "outcome": "passed"}, "call": {"duration": 0.4234327280000798, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_dsjb', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_dsjb', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_dsjb', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53e45b10>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_dsjb', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_dsjb', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_dsjb', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00023124099993765412, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022402627000019493, "outcome": "passed"}, "call": {"duration": 0.4947162329999628, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_frc8', function=Function(arguments='{}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_frc8', function=Function(arguments='{}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_frc8', function=Function(arguments='{}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53e1d0f0>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_frc8', function=Function(arguments='{}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_frc8', function=Function(arguments='{}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_frc8', function=Function(arguments='{}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00023414699990098597, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022444315000029746, "outcome": "passed"}, "call": {"duration": 0.4142598650000764, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_awts', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_awts', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_awts', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d51870>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_awts', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_awts', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_awts', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00022690300011163345, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02212273500003903, "outcome": "passed"}, "call": {"duration": 0.9732798930001536, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_kece', function=Function(arguments='{\"query\": \"san francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_ah2q', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_kece', function=Function(arguments='{\"query\": \"san francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_ah2q', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kece', function=Function(arguments='{\"query\": \"san francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_ah2q', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d51ae0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_kece', function=Function(arguments='{\"query\": \"san francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_ah2q', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_kece', function=Function(arguments='{\"query\": \"san francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_ah2q', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kece', function=Function(arguments='{\"query\": \"san francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_ah2q', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.0002274840001064149, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022350349999896935, "outcome": "passed"}, "call": {"duration": 0.7881711489999361, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_mxe2', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_mxe2', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mxe2', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf54121630>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_mxe2', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_mxe2', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mxe2', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00023336499998549698, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02231354199989255, "outcome": "passed"}, "call": {"duration": 0.43059711999990213, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError: Expected tool 'get_event', got 'create_event'\nassert 'create_event' == 'get_event'\n  \n  - get_event\n  ? ^\n  + create_event\n  ? ^^ + +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d47910>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n>               assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\nE               AssertionError: Expected tool 'get_event', got 'create_event'\nE               assert 'create_event' == 'get_event'\nE                 \nE                 - get_event\nE                 ? ^\nE                 + create_event\nE                 ? ^^ + +\n\ntests/verifications/openai_api/test_chat_completion.py:440: AssertionError"}, "teardown": {"duration": 0.00024520700003449747, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0224437649999345, "outcome": "passed"}, "call": {"duration": 0.43780404099993575, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthlyExpenseSummary'\nassert 'monthlyExpenseSummary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ? ^^^^\n  + monthlyExpenseSummary\n  ? ^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53ca2620>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n>               assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthlyExpenseSummary'\nE               assert 'monthlyExpenseSummary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ? ^^^^\nE                 + monthlyExpenseSummary\nE                 ? ^\n\ntests/verifications/openai_api/test_chat_completion.py:440: AssertionError"}, "teardown": {"duration": 0.0002558769999723154, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02237427399995795, "outcome": "passed"}, "call": {"duration": 0.6013386140000421, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 2\nassert 2 == 0\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_fs2r', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_gq77', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_fs2r', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_gq77', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fs2r', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_gq77', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53bf2f80>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 2\nE           assert 2 == 0\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_fs2r', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_gq77', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_fs2r', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_gq77', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fs2r', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_gq77', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.0002479719998973451, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02230255099993883, "outcome": "passed"}, "call": {"duration": 0.4682447669999874, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_yzzb', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='GetWeather'), type='function'), ChatCompletionMessageToolCall(id='call_146w', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_yzzb', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='GetWeather'), type='function'), ChatCompletionMessageToolCall(id='call_146w', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yzzb', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='GetWeather'), type='function'), ChatCompletionMessageToolCall(id='call_146w', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53bf1ba0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_yzzb', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='GetWeather'), type='function'), ChatCompletionMessageToolCall(id='call_146w', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_yzzb', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='GetWeather'), type='function'), ChatCompletionMessageToolCall(id='call_146w', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yzzb', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='GetWeather'), type='function'), ChatCompletionMessageToolCall(id='call_146w', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021552099997279583, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022400994000008723, "outcome": "passed"}, "call": {"duration": 1.1574360680001519, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4ep1', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_4ep1', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4ep1', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d91c30>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4ep1', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_4ep1', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4ep1', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021984999989399512, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02230720000011388, "outcome": "passed"}, "call": {"duration": 0.7554992709999624, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_vq81', function=Function(arguments='{\"date\": \"2025-03-03\", \"location\": \"\", \"name\": \"\", \"participants\": [], \"time\": \"10:00\"}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_1xr4', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_vq81', function=Function(arguments='{\"date\": \"2025-03-03\", \"location\": \"\", \"name\": \"\", \"participants\": [], \"time\": \"10:00\"}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_1xr4', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vq81', function=Function(arguments='{\"date\": \"2025-03-03\", \"location\": \"\", \"name\": \"\", \"participants\": [], \"time\": \"10:00\"}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_1xr4', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d93c40>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_vq81', function=Function(arguments='{\"date\": \"2025-03-03\", \"location\": \"\", \"name\": \"\", \"participants\": [], \"time\": \"10:00\"}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_1xr4', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_vq81', function=Function(arguments='{\"date\": \"2025-03-03\", \"location\": \"\", \"name\": \"\", \"participants\": [], \"time\": \"10:00\"}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_1xr4', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vq81', function=Function(arguments='{\"date\": \"2025-03-03\", \"location\": \"\", \"name\": \"\", \"participants\": [], \"time\": \"10:00\"}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_1xr4', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00026641699992069334, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022484931000008146, "outcome": "passed"}, "call": {"duration": 0.38771153199991204, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'GetMonthlyExpenseSummary'\nassert 'GetMonthlyExpenseSummary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ? ^\n  + GetMonthlyExpenseSummary\n  ? ^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53e312a0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n>               assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'GetMonthlyExpenseSummary'\nE               assert 'GetMonthlyExpenseSummary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ? ^\nE                 + GetMonthlyExpenseSummary\nE                 ? ^\n\ntests/verifications/openai_api/test_chat_completion.py:440: AssertionError"}, "teardown": {"duration": 0.00021257599996715726, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022149565000063376, "outcome": "passed"}, "call": {"duration": 0.21360814199988454, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"query\": \"Latin name for the Sun\"}', 'name': 'brave_search'}, 'id': 'call_pxv9', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53c352d0>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"query\": \"Latin name for the Sun\"}', 'name': 'brave_search'}, 'id': 'call_pxv9', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.000245458000108556, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02229316399984782, "outcome": "passed"}, "call": {"duration": 0.3716594380000515, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{}', 'name': 'brave_search'}, 'id': 'call_e84s', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53cbfeb0>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{}', 'name': 'brave_search'}, 'id': 'call_e84s', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00022971799990045838, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.022223504000066896, "outcome": "passed"}, "call": {"duration": 0.5361173950000193, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_rv6n', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53c06b90>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_rv6n', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021673300011570973, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02250405400013733, "outcome": "passed"}, "call": {"duration": 0.3923569769999631, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"output\": \"No\"}', 'name': 'print'}, 'id': 'call_d63v', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53e57070>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"output\": \"No\"}', 'name': 'print'}, 'id': 'call_d63v', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0002286059998368728, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022074486000065008, "outcome": "passed"}, "call": {"duration": 0.3631810819999828, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{}', 'name': 'none'}, 'id': 'call_tfv9', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53ca7f10>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{}', 'name': 'none'}, 'id': 'call_tfv9', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021831699996255338, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02212510200001816, "outcome": "passed"}, "call": {"duration": 0.5282294420001108, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"query\": \"Sun name in Latin\"}', 'name': 'brave_search'}, 'id': 'call_ky32', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53ccf490>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"query\": \"Sun name in Latin\"}', 'name': 'brave_search'}, 'id': 'call_ky32', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021224600004643435, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022113600000011502, "outcome": "passed"}, "call": {"duration": 0.566012211999805, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([{'function': {'arguments': '{\"query\": \"San Francisco weather today\"}', 'name': 'brave_search'}, 'id': 'call_7zzw', 'type': 'function'}, {'function': {'arguments': '{\"location\": \"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_ytw7', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53ee7d00>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([{'function': {'arguments': '{\"query\": \"San Francisco weather today\"}', 'name': 'brave_search'}, 'id': 'call_7zzw', 'type': 'function'}, {'function': {'arguments': '{\"location\": \"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_ytw7', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00022457899990513397, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022117346999948495, "outcome": "passed"}, "call": {"duration": 1.1146044760000677, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\":19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_saa7', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53bf0430>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\":19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_saa7', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00023431700014953094, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022247309000022142, "outcome": "passed"}, "call": {"duration": 0.4290090249999139, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'get_event', got 'create_event'\nassert 'create_event' == 'get_event'\n  \n  - get_event\n  ? ^\n  + create_event\n  ? ^^ + +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53e44be0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'get_event', got 'create_event'\nE               assert 'create_event' == 'get_event'\nE                 \nE                 - get_event\nE                 ? ^\nE                 + create_event\nE                 ? ^^ + +\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.00021514099989872193, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02210701799981507, "outcome": "passed"}, "call": {"duration": 0.9553438160000951, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthlyExpenseSummary'\nassert 'monthlyExpenseSummary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ? ^^^^\n  + monthlyExpenseSummary\n  ? ^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53bf0fa0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthlyExpenseSummary'\nE               assert 'monthlyExpenseSummary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ? ^^^^\nE                 + monthlyExpenseSummary\nE                 ? ^\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.00021637300005750149, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022035193999954572, "outcome": "passed"}, "call": {"duration": 0.7483129449999524, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 542, "message": "AssertionError: Expected content, but none received.\nassert ('' is not None and '' != '')"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 542, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53da7370>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n>               assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\nE               AssertionError: Expected content, but none received.\nE               assert ('' is not None and '' != '')\n\ntests/verifications/openai_api/test_chat_completion.py:542: AssertionError"}, "teardown": {"duration": 0.00022806499987382267, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022385286999906384, "outcome": "passed"}, "call": {"duration": 0.659422129999939, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d41fc0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fdf53dd69e0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00024953599995569675, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022052556000062395, "outcome": "passed"}, "call": {"duration": 1.0167501779999384, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_mj59', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53d51210>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_mj59', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021825800013175467, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022070980000080453, "outcome": "passed"}, "call": {"duration": 0.7443129989999306, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([{'function': {'arguments': '{\"date\": \"2025-03-03\", \"location\": \"Conference Room\", \"name\": \"Team Meeting\", \"participants\": [\"John\", \"Mary\", \"David\"], \"time\": \"10:00\"}', 'name': 'create_event'}, 'id': 'call_4m8e', 'type': 'function'}, {'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_q8x3', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53de4220>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([{'function': {'arguments': '{\"date\": \"2025-03-03\", \"location\": \"Conference Room\", \"name\": \"Team Meeting\", \"participants\": [\"John\", \"Mary\", \"David\"], \"time\": \"10:00\"}', 'name': 'create_event'}, 'id': 'call_4m8e', 'type': 'function'}, {'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_q8x3', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00022198400006345764, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022307691999913004, "outcome": "passed"}, "call": {"duration": 0.4649425779998637, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'GetMonthlyExpenseSummary'\nassert 'GetMonthlyExpenseSummary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ? ^\n  + GetMonthlyExpenseSummary\n  ? ^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fdf53c7a950>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'GetMonthlyExpenseSummary'\nE               assert 'GetMonthlyExpenseSummary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ? ^\nE                 + GetMonthlyExpenseSummary\nE                 ? ^\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.00023298499991142307, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=False]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "stream=False"}, "setup": {"duration": 0.022961481000038475, "outcome": "passed"}, "call": {"duration": 0.0001980600000024424, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.0001822900001116068, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=True]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "stream=True"}, "setup": {"duration": 0.023844306999990295, "outcome": "passed"}, "call": {"duration": 0.00022109199994702067, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.0001865069998530089, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=False]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.022990653999841015, "outcome": "passed"}, "call": {"duration": 5.785742687000038, "outcome": "passed"}, "teardown": {"duration": 0.00021069200010970235, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=True]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.023845166000000972, "outcome": "passed"}, "call": {"duration": 6.128813160000163, "outcome": "passed"}, "teardown": {"duration": 0.00023944600002323568, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=False]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.023255591999941316, "outcome": "passed"}, "call": {"duration": 1.7507316520000131, "outcome": "passed"}, "teardown": {"duration": 0.00023902600014480413, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=True]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.02410547499994209, "outcome": "passed"}, "call": {"duration": 3.672026007000113, "outcome": "passed"}, "teardown": {"duration": 0.0026521339998453186, "outcome": "passed"}}]}