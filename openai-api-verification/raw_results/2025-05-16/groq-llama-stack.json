{"created": 1747362318.7494328, "duration": 76.31566572189331, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 72, "skipped": 4, "failed": 38, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.05513319400006367, "outcome": "passed"}, "call": {"duration": 0.33832731200004673, "outcome": "passed"}, "teardown": {"duration": 0.00022452700000030745, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.02325250000012602, "outcome": "passed"}, "call": {"duration": 0.3912338809998346, "outcome": "passed"}, "teardown": {"duration": 0.00023195099993245094, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.0237040080000952, "outcome": "passed"}, "call": {"duration": 0.28522077399998125, "outcome": "passed"}, "teardown": {"duration": 0.00031144900003710063, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.02335742500008564, "outcome": "passed"}, "call": {"duration": 0.5666673450000417, "outcome": "passed"}, "teardown": {"duration": 0.0002355669998905796, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.024144567000121242, "outcome": "passed"}, "call": {"duration": 0.526036041999987, "outcome": "passed"}, "teardown": {"duration": 0.0002650119999998424, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.023484329999973852, "outcome": "passed"}, "call": {"duration": 0.7368118350000259, "outcome": "passed"}, "teardown": {"duration": 0.0002713540000058856, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.023613490000116144, "outcome": "passed"}, "call": {"duration": 0.33832437699993534, "outcome": "passed"}, "teardown": {"duration": 0.00021972800004732562, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.023104874999944514, "outcome": "passed"}, "call": {"duration": 0.6350358800000322, "outcome": "passed"}, "teardown": {"duration": 0.00032272099997499026, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.023073584000030678, "outcome": "passed"}, "call": {"duration": 1.0730733010000222, "outcome": "passed"}, "teardown": {"duration": 0.00024686999995537917, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.02302722900003573, "outcome": "passed"}, "call": {"duration": 0.5718162959999518, "outcome": "passed"}, "teardown": {"duration": 0.0002333039999484754, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.02483991100007188, "outcome": "passed"}, "call": {"duration": 0.6075207799999589, "outcome": "passed"}, "teardown": {"duration": 0.00032163899982151634, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[groq/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.022271249999903375, "outcome": "passed"}, "call": {"duration": 0.7482271409999157, "outcome": "passed"}, "teardown": {"duration": 0.0002494350001143175, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.022830913000007058, "outcome": "passed"}, "call": {"duration": 0.005943394000041735, "outcome": "passed"}, "teardown": {"duration": 0.0002803819998007384, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022176804999844535, "outcome": "passed"}, "call": {"duration": 0.00625648999994155, "outcome": "passed"}, "teardown": {"duration": 0.00028145400005996635, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022183598000083293, "outcome": "passed"}, "call": {"duration": 0.11202248500012502, "outcome": "passed"}, "teardown": {"duration": 0.00022171299997353344, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022566519999827506, "outcome": "passed"}, "call": {"duration": 0.0889087410000684, "outcome": "passed"}, "teardown": {"duration": 0.0003068409998832067, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.024484202000166988, "outcome": "passed"}, "call": {"duration": 0.11977316899992729, "outcome": "passed"}, "teardown": {"duration": 0.00029046100007690256, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.024813285000163887, "outcome": "passed"}, "call": {"duration": 0.005986972999835416, "outcome": "passed"}, "teardown": {"duration": 0.00024182100014513708, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02327043200011758, "outcome": "passed"}, "call": {"duration": 0.006527289000132441, "outcome": "passed"}, "teardown": {"duration": 0.00025152900002467504, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02275525300001391, "outcome": "passed"}, "call": {"duration": 0.11041078200014454, "outcome": "passed"}, "teardown": {"duration": 0.00027448199989521527, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.032048286999952325, "outcome": "passed"}, "call": {"duration": 0.09685688500007927, "outcome": "passed"}, "teardown": {"duration": 0.00029070100003991683, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023579196999889973, "outcome": "passed"}, "call": {"duration": 0.12002658100004737, "outcome": "passed"}, "teardown": {"duration": 0.0002547540000250592, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.023112178000019412, "outcome": "passed"}, "call": {"duration": 0.005533971999966525, "outcome": "passed"}, "teardown": {"duration": 0.0002186970000366273, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022546925000142437, "outcome": "passed"}, "call": {"duration": 0.00614978700014035, "outcome": "passed"}, "teardown": {"duration": 0.00020831699998780095, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022317267000062202, "outcome": "passed"}, "call": {"duration": 0.11618178300000181, "outcome": "passed"}, "teardown": {"duration": 0.00020489200005613384, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022292961999937688, "outcome": "passed"}, "call": {"duration": 0.09792976399990039, "outcome": "passed"}, "teardown": {"duration": 0.0003607109999848035, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.024376751999852786, "outcome": "passed"}, "call": {"duration": 0.11990298000000621, "outcome": "passed"}, "teardown": {"duration": 0.00025036600004568754, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.023282835000145496, "outcome": "passed"}, "call": {"duration": 0.005444060999934663, "outcome": "passed"}, "teardown": {"duration": 0.0002300780001860403, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0224414980000347, "outcome": "passed"}, "call": {"duration": 0.006223463999958767, "outcome": "passed"}, "teardown": {"duration": 0.00021062200016785937, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02214772100001028, "outcome": "passed"}, "call": {"duration": 0.10156435099997907, "outcome": "passed"}, "teardown": {"duration": 0.00030761300013182336, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.024098604000073465, "outcome": "passed"}, "call": {"duration": 0.0805979000001571, "outcome": "passed"}, "teardown": {"duration": 0.0002310309998847515, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.024247432000038316, "outcome": "passed"}, "call": {"duration": 0.11505084700002044, "outcome": "passed"}, "teardown": {"duration": 0.0002598949999992328, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022697875999938333, "outcome": "passed"}, "call": {"duration": 0.005379747999995743, "outcome": "passed"}, "teardown": {"duration": 0.00023599899986948003, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.023437864000015907, "outcome": "passed"}, "call": {"duration": 0.006408808999822213, "outcome": "passed"}, "teardown": {"duration": 0.00017912299995259673, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022427483000001303, "outcome": "passed"}, "call": {"duration": 0.10178183399989393, "outcome": "passed"}, "teardown": {"duration": 0.00033871100004034815, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02405263799983004, "outcome": "passed"}, "call": {"duration": 0.0785235839998677, "outcome": "passed"}, "teardown": {"duration": 0.00019937100000788632, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02280337199999849, "outcome": "passed"}, "call": {"duration": 0.10115174199995636, "outcome": "passed"}, "teardown": {"duration": 0.0003360260000135895, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022808832000009716, "outcome": "passed"}, "call": {"duration": 0.005545025999936115, "outcome": "passed"}, "teardown": {"duration": 0.00026413199998387427, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022360456999876988, "outcome": "passed"}, "call": {"duration": 0.006326531000013347, "outcome": "passed"}, "teardown": {"duration": 0.00024101899998640874, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.021893167999905927, "outcome": "passed"}, "call": {"duration": 0.10247935099982897, "outcome": "passed"}, "teardown": {"duration": 0.0002645729998675961, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02312259699988317, "outcome": "passed"}, "call": {"duration": 0.08016617299995232, "outcome": "passed"}, "teardown": {"duration": 0.0002899800001614494, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023263138999936928, "outcome": "passed"}, "call": {"duration": 0.10263171299993701, "outcome": "passed"}, "teardown": {"duration": 0.0003331310001613019, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-3.3-70b-versatile-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022629307999977755, "outcome": "passed"}, "call": {"duration": 0.00016253199987659173, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.00017114899992520805, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.030037452999977177, "outcome": "passed"}, "call": {"duration": 1.5165594729999157, "outcome": "passed"}, "teardown": {"duration": 0.0003261770000335673, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.024117902000170943, "outcome": "passed"}, "call": {"duration": 1.5320279830000345, "outcome": "passed"}, "teardown": {"duration": 0.0002844199998435215, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-3.3-70b-versatile-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02364827599990349, "outcome": "passed"}, "call": {"duration": 0.0002803620000122464, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.0001652470000408357, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.021912773999929414, "outcome": "passed"}, "call": {"duration": 2.5325575439999284, "outcome": "passed"}, "teardown": {"duration": 0.0002062039998236287, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022270380000009027, "outcome": "passed"}, "call": {"duration": 3.5138527560000057, "outcome": "passed"}, "teardown": {"duration": 0.00023514800000157265, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.02239549099999749, "outcome": "passed"}, "call": {"duration": 0.4045502450001095, "outcome": "passed"}, "teardown": {"duration": 0.0002821059999860154, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.022737850999874354, "outcome": "passed"}, "call": {"duration": 0.8956037370001013, "outcome": "passed"}, "teardown": {"duration": 0.00020454999980756838, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.02254365300018435, "outcome": "passed"}, "call": {"duration": 0.3052740209998319, "outcome": "passed"}, "teardown": {"duration": 0.0002310800000486779, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.022634890000063024, "outcome": "passed"}, "call": {"duration": 0.7765892240001904, "outcome": "passed"}, "teardown": {"duration": 0.0002910020000399527, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.02390946399987115, "outcome": "passed"}, "call": {"duration": 0.34779476800008524, "outcome": "passed"}, "teardown": {"duration": 0.00022525899998981913, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.023436410999920554, "outcome": "passed"}, "call": {"duration": 1.1446318069999961, "outcome": "passed"}, "teardown": {"duration": 0.00025270999981330533, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.023514277000003858, "outcome": "passed"}, "call": {"duration": 0.23718366200000673, "outcome": "passed"}, "teardown": {"duration": 0.0001979989999654208, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.03280939599994781, "outcome": "passed"}, "call": {"duration": 1.224341835999894, "outcome": "passed"}, "teardown": {"duration": 0.00021013100013078656, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.02282902499996453, "outcome": "passed"}, "call": {"duration": 0.3210647409998728, "outcome": "passed"}, "teardown": {"duration": 0.0002443060000132391, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.02268226199998935, "outcome": "passed"}, "call": {"duration": 1.102712406000137, "outcome": "passed"}, "teardown": {"duration": 0.0002300190001278679, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.022432206000075894, "outcome": "passed"}, "call": {"duration": 0.3860114789999898, "outcome": "passed"}, "teardown": {"duration": 0.00034111599984498753, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[groq/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.022530519999918397, "outcome": "passed"}, "call": {"duration": 1.0491596389999813, "outcome": "passed"}, "teardown": {"duration": 0.00020119400005569332, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022290682000175366, "outcome": "passed"}, "call": {"duration": 0.2138056440001037, "outcome": "passed"}, "teardown": {"duration": 0.0002404979998118506, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 221, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02336323999998058, "outcome": "passed"}, "call": {"duration": 0.5081096159999561, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError: assert 'brave_search' == 'get_weather'\n  \n  - get_weather\n  + brave_search"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15cf040>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0\n        assert case[\"output\"] == \"get_weather_tool_call\"\n>       assert response.choices[0].message.tool_calls[0].function.name == \"get_weather\"\nE       AssertionError: assert 'brave_search' == 'get_weather'\nE         \nE         - get_weather\nE         + brave_search\n\ntests/verifications/openai_api/test_chat_completion.py:242: AssertionError"}, "teardown": {"duration": 0.00023764200000186975, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 221, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022331428000143205, "outcome": "passed"}, "call": {"duration": 1.0482437349999145, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError: assert 'geather' == 'get_weather'\n  \n  - get_weather\n  ?  ----\n  + geather"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 242, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d144be20>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0\n        assert case[\"output\"] == \"get_weather_tool_call\"\n>       assert response.choices[0].message.tool_calls[0].function.name == \"get_weather\"\nE       AssertionError: assert 'geather' == 'get_weather'\nE         \nE         - get_weather\nE         ?  ----\nE         + geather\n\ntests/verifications/openai_api/test_chat_completion.py:242: AssertionError"}, "teardown": {"duration": 0.00021578199994110037, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02224047800018525, "outcome": "passed"}, "call": {"duration": 0.3526104120001037, "outcome": "passed"}, "teardown": {"duration": 0.0002496150000297348, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022817413999973724, "outcome": "passed"}, "call": {"duration": 0.31631697700004224, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 268, "message": "AssertionError: assert ' get_weather' == 'get_weather'\n  \n  - get_weather\n  +  get_weather\n  ? +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 268, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d14d7d90>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n        assert len(tool_calls_buffer) == 1\n        for call in tool_calls_buffer:\n            assert len(call[\"id\"]) > 0\n            function = call[\"function\"]\n>           assert function[\"name\"] == \"get_weather\"\nE           AssertionError: assert ' get_weather' == 'get_weather'\nE             \nE             - get_weather\nE             +  get_weather\nE             ? +\n\ntests/verifications/openai_api/test_chat_completion.py:268: AssertionError"}, "teardown": {"duration": 0.0002124250001998007, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022253322000096887, "outcome": "passed"}, "call": {"duration": 0.8637919899999815, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 263, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d14044f0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:263: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7f52d1637100>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00022652200004813494, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022288498000079926, "outcome": "passed"}, "call": {"duration": 0.3359468050000487, "outcome": "passed"}, "teardown": {"duration": 0.0003187730001172895, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023532134000106453, "outcome": "passed"}, "call": {"duration": 0.28201267299982646, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError: assert ' get_weather' == 'get_weather'\n  \n  - get_weather\n  +  get_weather\n  ? +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d14e8850>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert response.choices[0].message.tool_calls[0].function.name == expected_tool_name\nE       AssertionError: assert ' get_weather' == 'get_weather'\nE         \nE         - get_weather\nE         +  get_weather\nE         ? +\n\ntests/verifications/openai_api/test_chat_completion.py:295: AssertionError"}, "teardown": {"duration": 0.00022131200012154295, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022222657999918738, "outcome": "passed"}, "call": {"duration": 1.1019830320001347, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError: assert ' get_weather' == 'get_weather'\n  \n  - get_weather\n  +  get_weather\n  ? +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 295, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d152f400>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert response.choices[0].message.tool_calls[0].function.name == expected_tool_name\nE       AssertionError: assert ' get_weather' == 'get_weather'\nE         \nE         - get_weather\nE         +  get_weather\nE         ? +\n\ntests/verifications/openai_api/test_chat_completion.py:295: AssertionError"}, "teardown": {"duration": 0.00021231599998827733, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0225178059999962, "outcome": "passed"}, "call": {"duration": 0.3413705499999651, "outcome": "passed"}, "teardown": {"duration": 0.0002525410000089323, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022402453000040623, "outcome": "passed"}, "call": {"duration": 0.33873280700004216, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 318, "message": "AssertionError: Expected tool call when tool_choice='required'\nassert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 318, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[groq/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15ce440>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n>       assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\nE       AssertionError: Expected tool call when tool_choice='required'\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/verifications/openai_api/test_chat_completion.py:318: AssertionError"}, "teardown": {"duration": 0.0002132670001628867, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022210424999911993, "outcome": "passed"}, "call": {"duration": 0.34102801200015165, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 320, "message": "AssertionError: Expected tool call 'get_weather' not found in stream\nassert False\n +  where False = any(<generator object test_chat_streaming_tool_choice_required.<locals>.<genexpr> at 0x7f52d13fc9e0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 320, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[groq/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15d51e0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n        assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert any(call[\"function\"][\"name\"] == expected_tool_name for call in tool_calls_buffer), (\n            f\"Expected tool call '{expected_tool_name}' not found in stream\"\n        )\nE       AssertionError: Expected tool call 'get_weather' not found in stream\nE       assert False\nE        +  where False = any(<generator object test_chat_streaming_tool_choice_required.<locals>.<genexpr> at 0x7f52d13fc9e0>)\n\ntests/verifications/openai_api/test_chat_completion.py:320: AssertionError"}, "teardown": {"duration": 0.00023686100007580535, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022057838999899104, "outcome": "passed"}, "call": {"duration": 1.2593661370001428, "outcome": "passed"}, "teardown": {"duration": 0.0002998679999564047, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02386216299987609, "outcome": "passed"}, "call": {"duration": 1.6689643310000974, "outcome": "passed"}, "teardown": {"duration": 0.00021424999999908323, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023875737000025765, "outcome": "passed"}, "call": {"duration": 1.5311969829999725, "outcome": "passed"}, "teardown": {"duration": 0.00022814499993728532, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[groq/llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.023643563999939943, "outcome": "passed"}, "call": {"duration": 0.7979292179998083, "outcome": "passed"}, "teardown": {"duration": 0.00021169500018913823, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[groq/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02232858499996837, "outcome": "passed"}, "call": {"duration": 5.481194577999986, "outcome": "passed"}, "teardown": {"duration": 0.000249003999897468, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[groq/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022406656000157454, "outcome": "passed"}, "call": {"duration": 0.3403475079999225, "outcome": "passed"}, "teardown": {"duration": 0.000274580999985119, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02245499600007861, "outcome": "passed"}, "call": {"duration": 0.30418453600009343, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_d32d', function=Function(arguments='{}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_d32d', function=Function(arguments='{}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_d32d', function=Function(arguments='{}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d1417970>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_d32d', function=Function(arguments='{}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_d32d', function=Function(arguments='{}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_d32d', function=Function(arguments='{}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00022364699998433935, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022148093999931007, "outcome": "passed"}, "call": {"duration": 0.7220528049999757, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_vejy', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_vejy', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vejy', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f52d16d2800>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_vejy', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_vejy', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vejy', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021286700007294712, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.022287783999900057, "outcome": "passed"}, "call": {"duration": 1.3197259129999566, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_vn6h', function=Function(arguments='{\"productID\": 123}', name='echoProductID'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_vn6h', function=Function(arguments='{\"productID\": 123}', name='echoProductID'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vn6h', function=Function(arguments='{\"productID\": 123}', name='echoProductID'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15fe290>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_vn6h', function=Function(arguments='{\"productID\": 123}', name='echoProductID'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_vn6h', function=Function(arguments='{\"productID\": 123}', name='echoProductID'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vn6h', function=Function(arguments='{\"productID\": 123}', name='echoProductID'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00024127799997586408, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02211736899994321, "outcome": "passed"}, "call": {"duration": 0.6936393500000122, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_e2dg', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_e2dg', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_e2dg', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d16bc160>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_e2dg', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_e2dg', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_e2dg', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00023923499998090847, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02208957800007738, "outcome": "passed"}, "call": {"duration": 0.662680082000179, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_gr3p', function=Function(arguments='{}', name='does not exist to provide a more detailed breakdown of expenses'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_gr3p', function=Function(arguments='{}', name='does not exist to provide a more detailed breakdown of expenses'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_gr3p', function=Function(arguments='{}', name='does not exist to provide a more detailed breakdown of expenses'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d13eb9a0>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_gr3p', function=Function(arguments='{}', name='does not exist to provide a more detailed breakdown of expenses'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_gr3p', function=Function(arguments='{}', name='does not exist to provide a more detailed breakdown of expenses'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_gr3p', function=Function(arguments='{}', name='does not exist to provide a more detailed breakdown of expenses'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021527100011553557, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02207465000014963, "outcome": "passed"}, "call": {"duration": 0.3448505799999566, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_myam', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_myam', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_myam', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d155b670>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_myam', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_myam', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_myam', function=Function(arguments='{\"query\": \"Sun name in Latin\"}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021679399992535764, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02261486900010823, "outcome": "passed"}, "call": {"duration": 0.3943914640001367, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_kxm1', function=Function(arguments='{\"query\": \"San Francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_dv0r', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_kxm1', function=Function(arguments='{\"query\": \"San Francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_dv0r', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kxm1', function=Function(arguments='{\"query\": \"San Francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_dv0r', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f52d17288b0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_kxm1', function=Function(arguments='{\"query\": \"San Francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_dv0r', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_kxm1', function=Function(arguments='{\"query\": \"San Francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_dv0r', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kxm1', function=Function(arguments='{\"query\": \"San Francisco weather\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_dv0r', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00024611799995000183, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022168758999896454, "outcome": "passed"}, "call": {"duration": 0.9634154400000625, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_3jtg', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_3jtg', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_3jtg', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d14a2290>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_3jtg', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_3jtg', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_3jtg', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021685499996237922, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02220696700010194, "outcome": "passed"}, "call": {"duration": 0.42177923400004147, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_vw1h', function=Function(arguments='{\"name\": \"\", \"date\": \"2025-03-03\", \"time\": \"10:00\", \"location\": \"\", \"participants\": [\"\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_4632', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_vw1h', function=Function(arguments='{\"name\": \"\", \"date\": \"2025-03-03\", \"time\": \"10:00\", \"location\": \"\", \"participants\": [\"\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_4632', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vw1h', function=Function(arguments='{\"name\": \"\", \"date\": \"2025-03-03\", \"time\": \"10:00\", \"location\": \"\", \"participants\": [\"\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_4632', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d14d5cf0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_vw1h', function=Function(arguments='{\"name\": \"\", \"date\": \"2025-03-03\", \"time\": \"10:00\", \"location\": \"\", \"participants\": [\"\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_4632', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_vw1h', function=Function(arguments='{\"name\": \"\", \"date\": \"2025-03-03\", \"time\": \"10:00\", \"location\": \"\", \"participants\": [\"\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_4632', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vw1h', function=Function(arguments='{\"name\": \"\", \"date\": \"2025-03-03\", \"time\": \"10:00\", \"location\": \"\", \"participants\": [\"\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_4632', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00023765300011291401, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02232150300005742, "outcome": "passed"}, "call": {"duration": 0.2866757540000435, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthlyExpenseSummary'\nassert 'monthlyExpenseSummary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ? ^^^^\n  + monthlyExpenseSummary\n  ? ^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d1414f70>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n>               assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthlyExpenseSummary'\nE               assert 'monthlyExpenseSummary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ? ^^^^\nE                 + monthlyExpenseSummary\nE                 ? ^\n\ntests/verifications/openai_api/test_chat_completion.py:440: AssertionError"}, "teardown": {"duration": 0.00021570099988821312, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022112452999863308, "outcome": "passed"}, "call": {"duration": 0.7591579239999646, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 2\nassert 2 == 0\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_51fp', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_44pm', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_51fp', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_44pm', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_51fp', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_44pm', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15d7190>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 2\nE           assert 2 == 0\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_51fp', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_44pm', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_51fp', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_44pm', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_51fp', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function'), ChatCompletionMessageToolCall(id='call_44pm', function=Function(arguments='{\"location\": \"Rome, Italy\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00022014999990460637, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02207899199993335, "outcome": "passed"}, "call": {"duration": 0.4034727880000446, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError: Expected tool 'get_weather', got ' get_weather'\nassert ' get_weather' == 'get_weather'\n  \n  - get_weather\n  +  get_weather\n  ? +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 440, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15d7b50>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n>               assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\nE               AssertionError: Expected tool 'get_weather', got ' get_weather'\nE               assert ' get_weather' == 'get_weather'\nE                 \nE                 - get_weather\nE                 +  get_weather\nE                 ? +\n\ntests/verifications/openai_api/test_chat_completion.py:440: AssertionError"}, "teardown": {"duration": 0.00021937799988336337, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022255259000075966, "outcome": "passed"}, "call": {"duration": 0.8404234590000215, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_0ydf', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_0ydf', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0ydf', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d158a9e0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_0ydf', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_0ydf', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0ydf', function=Function(arguments='{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', name='addProduct'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00022143199998936325, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022338524999895526, "outcome": "passed"}, "call": {"duration": 0.6399070190000202, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_9yvd', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\", \"name\": \"meeting\", \"location\": \"office\", \"participants\": [\"John\", \"Alice\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_96n9', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_9yvd', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\", \"name\": \"meeting\", \"location\": \"office\", \"participants\": [\"John\", \"Alice\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_96n9', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9yvd', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\", \"name\": \"meeting\", \"location\": \"office\", \"participants\": [\"John\", \"Alice\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_96n9', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d144b940>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_9yvd', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\", \"name\": \"meeting\", \"location\": \"office\", \"participants\": [\"John\", \"Alice\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_96n9', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_9yvd', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\", \"name\": \"meeting\", \"location\": \"office\", \"participants\": [\"John\", \"Alice\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_96n9', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9yvd', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\", \"name\": \"meeting\", \"location\": \"office\", \"participants\": [\"John\", \"Alice\"]}', name='create_event'), type='function'), ChatCompletionMessageToolCall(id='call_96n9', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021703400011574558, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0222364749999997, "outcome": "passed"}, "call": {"duration": 0.9540658210000856, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_mx5h', function=Function(arguments='{\"query\": \"January expenses\"}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_mx5h', function=Function(arguments='{\"query\": \"January expenses\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mx5h', function=Function(arguments='{\"query\": \"January expenses\"}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d152c0d0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_mx5h', function=Function(arguments='{\"query\": \"January expenses\"}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_mx5h', function=Function(arguments='{\"query\": \"January expenses\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mx5h', function=Function(arguments='{\"query\": \"January expenses\"}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00022204300012163003, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022115318000032858, "outcome": "passed"}, "call": {"duration": 0.3275350719998187, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{}', 'name': 'brave_search'}, 'id': 'call_sy0h', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d131a4a0>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{}', 'name': 'brave_search'}, 'id': 'call_sy0h', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021118399990882608, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022009285999956774, "outcome": "passed"}, "call": {"duration": 0.6133869569998751, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"weather\": \"70 degrees and foggy\"}', 'name': 'print_weather'}, 'id': 'call_dywh', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15cf940>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"weather\": \"70 degrees and foggy\"}', 'name': 'print_weather'}, 'id': 'call_dywh', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0002541639998980827, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.022069376999979795, "outcome": "passed"}, "call": {"duration": 0.8576977589998478, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d13eb730>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7f52d15a4280>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00024313199992320733, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022461598000063532, "outcome": "passed"}, "call": {"duration": 0.7819598829998995, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_7009', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d1519330>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_7009', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0002530920000936021, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022431380999933026, "outcome": "passed"}, "call": {"duration": 0.7133443919999536, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"message\": \"Your monthly expense for January 2025 was $1000.\"}', 'name': 'console.log'}, 'id': 'call_8g6p', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-3.3-70b-versatile-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15ce500>\nmodel = 'groq/llama-3.3-70b-versatile', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"message\": \"Your monthly expense for January 2025 was $1000.\"}', 'name': 'console.log'}, 'id': 'call_8g6p', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00025026599996635923, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022290138000016668, "outcome": "passed"}, "call": {"duration": 0.31608043300002464, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"query\": \"Sun name in Latin\"}', 'name': 'brave_search'}, 'id': 'call_kx2a', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d16d0d30>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"query\": \"Sun name in Latin\"}', 'name': 'brave_search'}, 'id': 'call_kx2a', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00022958799991101841, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022325816000147825, "outcome": "passed"}, "call": {"duration": 0.34051126100007423, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([{'function': {'arguments': '{\"q\": \"San Francisco weather\"}', 'name': 'brave_search'}, 'id': 'call_35je', 'type': 'function'}, {'function': {'arguments': '{\"location\": \"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_6q8h', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f52d14d7520>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([{'function': {'arguments': '{\"q\": \"San Francisco weather\"}', 'name': 'brave_search'}, 'id': 'call_35je', 'type': 'function'}, {'function': {'arguments': '{\"location\": \"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_6q8h', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00022821500010650198, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02226294900015091, "outcome": "passed"}, "call": {"duration": 0.7115489939999406, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\":19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_hjsf', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d1558af0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\":19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_hjsf', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00023308499999075138, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02209425499995632, "outcome": "passed"}, "call": {"duration": 0.49621494600000915, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([{'function': {'arguments': '{\"name\": \"\", \"date\": \"\", \"time\": \"\", \"location\": \"\", \"participants\": \"\"}', 'name': 'create_event'}, 'id': 'call_fen8', 'type': 'function'}, {'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_nt7b', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d1318ee0>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([{'function': {'arguments': '{\"name\": \"\", \"date\": \"\", \"time\": \"\", \"location\": \"\", \"participants\": \"\"}', 'name': 'create_event'}, 'id': 'call_fen8', 'type': 'function'}, {'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_nt7b', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0002220630001374957, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022215871000071274, "outcome": "passed"}, "call": {"duration": 0.5557307579999815, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthly_expense_summary'\nassert 'monthly_expense_summary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ? ^^^^      ^      ^\n  + monthly_expense_summary\n  ? ^      ^^      ^^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d16ae920>\nmodel = 'groq/llama-4-scout-17b-16e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'monthly_expense_summary'\nE               assert 'monthly_expense_summary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ? ^^^^      ^      ^\nE                 + monthly_expense_summary\nE                 ? ^      ^^      ^^\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.0002237260000583774, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02207734600006006, "outcome": "passed"}, "call": {"duration": 0.42275064099999327, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 542, "message": "AssertionError: Expected content, but none received.\nassert ('' is not None and '' != '')"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 542, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15fd960>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n>               assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\nE               AssertionError: Expected content, but none received.\nE               assert ('' is not None and '' != '')\n\ntests/verifications/openai_api/test_chat_completion.py:542: AssertionError"}, "teardown": {"duration": 0.00026901100000031875, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022122771000113062, "outcome": "passed"}, "call": {"duration": 0.44683548199986944, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'get_weather', got 'fetch_weather'\nassert 'fetch_weather' == 'get_weather'\n  \n  - get_weather\n  ? ^\n  + fetch_weather\n  ? ^  ++"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f52d152cac0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'get_weather', got 'fetch_weather'\nE               assert 'fetch_weather' == 'get_weather'\nE                 \nE                 - get_weather\nE                 ? ^\nE                 + fetch_weather\nE                 ? ^  ++\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.00022491899994747655, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02218384499997228, "outcome": "passed"}, "call": {"duration": 0.9548829640000349, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_wqbq', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15d6fb0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"name\": \"Widget\", \"price\": 19.99, \"inStock\": true, \"tags\": [\"new\", \"sale\"]}', 'name': 'addProduct'}, 'id': 'call_wqbq', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0002526420000776852, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02215222899985747, "outcome": "passed"}, "call": {"duration": 0.6531533550000859, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 1 tool calls, but got 2\nassert 2 == 1\n +  where 2 = len(([{'function': {'arguments': '{\"date\": \"2025-03-03\", \"location\": \"Conference Room\", \"name\": \"Team Meeting\", \"participants\": [\"John\", \"Alice\", \"Bob\"], \"time\": \"10:00\"}', 'name': 'create_event'}, 'id': 'call_fxaf', 'type': 'function'}, {'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_pp2c', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d15453f0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 2\nE           assert 2 == 1\nE            +  where 2 = len(([{'function': {'arguments': '{\"date\": \"2025-03-03\", \"location\": \"Conference Room\", \"name\": \"Team Meeting\", \"participants\": [\"John\", \"Alice\", \"Bob\"], \"time\": \"10:00\"}', 'name': 'create_event'}, 'id': 'call_fxaf', 'type': 'function'}, {'function': {'arguments': '{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', 'name': 'get_event'}, 'id': 'call_pp2c', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00024376500005018897, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02219436800010044, "outcome": "passed"}, "call": {"duration": 0.4686543090001578, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'geMonthlyExpenseSummary'\nassert 'geMonthlyExpenseSummary' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ?   -\n  + geMonthlyExpenseSummary"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[groq/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f52d144b0d0>\nmodel = 'groq/llama-4-maverick-17b-128e-instruct', provider = 'groq-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'geMonthlyExpenseSummary'\nE               assert 'geMonthlyExpenseSummary' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ?   -\nE                 + geMonthlyExpenseSummary\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.0002416900001662725, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=False]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "stream=False"}, "setup": {"duration": 0.022849980000046344, "outcome": "passed"}, "call": {"duration": 0.00015754299988657294, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.00017677999994702986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-3.3-70b-versatile-stream=True]", "parametrize", "pytestmark", "groq/llama-3.3-70b-versatile-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-3.3-70b-versatile", "case_id": "stream=True"}, "setup": {"duration": 0.02319514399982836, "outcome": "passed"}, "call": {"duration": 0.00013955999997961044, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model groq/llama-3.3-70b-versatile on provider groq-llama-stack based on config.')"}, "teardown": {"duration": 0.00017174000004160916, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=False]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.02452017399991746, "outcome": "passed"}, "call": {"duration": 1.67269850699995, "outcome": "passed"}, "teardown": {"duration": 0.00025185999993482255, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-scout-17b-16e-instruct-stream=True]", "parametrize", "pytestmark", "groq/llama-4-scout-17b-16e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-scout-17b-16e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.02347922299986749, "outcome": "passed"}, "call": {"duration": 2.613848581000184, "outcome": "passed"}, "teardown": {"duration": 0.0003206979999959003, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=False]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.02350529699992876, "outcome": "passed"}, "call": {"duration": 2.1622199339999497, "outcome": "passed"}, "teardown": {"duration": 0.0002407179999863729, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[groq/llama-4-maverick-17b-128e-instruct-stream=True]", "parametrize", "pytestmark", "groq/llama-4-maverick-17b-128e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "groq/llama-4-maverick-17b-128e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.023684481999907803, "outcome": "passed"}, "call": {"duration": 4.526732270000139, "outcome": "passed"}, "teardown": {"duration": 0.002666205999958038, "outcome": "passed"}}]}