{"created": 1747361926.335301, "duration": 206.0936086177826, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 98, "skipped": 4, "failed": 12, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "earth"}, "setup": {"duration": 0.05540859099994577, "outcome": "passed"}, "call": {"duration": 0.5184716189999108, "outcome": "passed"}, "teardown": {"duration": 0.0002155020000600416, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "saturn"}, "setup": {"duration": 0.026928741000006085, "outcome": "passed"}, "call": {"duration": 0.7206101130000206, "outcome": "passed"}, "teardown": {"duration": 0.0002088900000671856, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "earth"}, "setup": {"duration": 0.023260907000008046, "outcome": "passed"}, "call": {"duration": 0.46647550800003046, "outcome": "passed"}, "teardown": {"duration": 0.0002483929999925749, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "saturn"}, "setup": {"duration": 0.023335456000040722, "outcome": "passed"}, "call": {"duration": 0.36456372700001793, "outcome": "passed"}, "teardown": {"duration": 0.0003015920000279948, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "earth"}, "setup": {"duration": 0.024284833000024264, "outcome": "passed"}, "call": {"duration": 1.0621535420000328, "outcome": "passed"}, "teardown": {"duration": 0.0002645329999495516, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "saturn"}, "setup": {"duration": 0.0243359190000092, "outcome": "passed"}, "call": {"duration": 0.3520125669999743, "outcome": "passed"}, "teardown": {"duration": 0.00023577000001751003, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "earth"}, "setup": {"duration": 0.024645446000022275, "outcome": "passed"}, "call": {"duration": 0.6087104189999764, "outcome": "passed"}, "teardown": {"duration": 0.00020177599992621253, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "saturn"}, "setup": {"duration": 0.02332596700000522, "outcome": "passed"}, "call": {"duration": 1.185955579999927, "outcome": "passed"}, "teardown": {"duration": 0.0002606659999173644, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "earth"}, "setup": {"duration": 0.02289666700005455, "outcome": "passed"}, "call": {"duration": 1.0299812819999943, "outcome": "passed"}, "teardown": {"duration": 0.0003138050000188741, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "saturn"}, "setup": {"duration": 0.022872011000004022, "outcome": "passed"}, "call": {"duration": 0.3754137800000308, "outcome": "passed"}, "teardown": {"duration": 0.0002222840000740689, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "earth"}, "setup": {"duration": 0.022227169000075264, "outcome": "passed"}, "call": {"duration": 0.9264166650000334, "outcome": "passed"}, "teardown": {"duration": 0.00025606700000935234, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "saturn"}, "setup": {"duration": 0.022187113999962094, "outcome": "passed"}, "call": {"duration": 0.3418773029999329, "outcome": "passed"}, "teardown": {"duration": 0.00032831199996508076, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_missing"}, "setup": {"duration": 0.022613660000047275, "outcome": "passed"}, "call": {"duration": 0.0067719899999474364, "outcome": "passed"}, "teardown": {"duration": 0.00028554200002872676, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022289574999945216, "outcome": "passed"}, "call": {"duration": 0.006556814000077793, "outcome": "passed"}, "teardown": {"duration": 0.0002091400000381327, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022189027999957034, "outcome": "passed"}, "call": {"duration": 0.16957314499995846, "outcome": "passed"}, "teardown": {"duration": 0.00028083300003345357, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022487814999976763, "outcome": "passed"}, "call": {"duration": 0.0973872609999944, "outcome": "passed"}, "teardown": {"duration": 0.00027049400000578316, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023674375999917174, "outcome": "passed"}, "call": {"duration": 0.12684958399995594, "outcome": "passed"}, "teardown": {"duration": 0.00021303699998043157, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022495219000006728, "outcome": "passed"}, "call": {"duration": 0.005195998999965923, "outcome": "passed"}, "teardown": {"duration": 0.00019308999992517784, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02303708900001311, "outcome": "passed"}, "call": {"duration": 0.006936924999990879, "outcome": "passed"}, "teardown": {"duration": 0.0002168139999412233, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02262314699999024, "outcome": "passed"}, "call": {"duration": 0.12027604100001099, "outcome": "passed"}, "teardown": {"duration": 0.00021481999999650725, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.031971046000080605, "outcome": "passed"}, "call": {"duration": 0.09974430800002665, "outcome": "passed"}, "teardown": {"duration": 0.00032273200008603453, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023561664999988352, "outcome": "passed"}, "call": {"duration": 0.12054014700004245, "outcome": "passed"}, "teardown": {"duration": 0.000232162000088465, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_missing"}, "setup": {"duration": 0.02242840399992474, "outcome": "passed"}, "call": {"duration": 0.005774240000050668, "outcome": "passed"}, "teardown": {"duration": 0.0002807330000678121, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022002530000008846, "outcome": "passed"}, "call": {"duration": 0.006638080000016089, "outcome": "passed"}, "teardown": {"duration": 0.00016773300001204916, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02206489600007444, "outcome": "passed"}, "call": {"duration": 0.1180869840000014, "outcome": "passed"}, "teardown": {"duration": 0.00020842799995079986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022212631000002148, "outcome": "passed"}, "call": {"duration": 0.09628614500002186, "outcome": "passed"}, "teardown": {"duration": 0.0003044879999833938, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02427058599994325, "outcome": "passed"}, "call": {"duration": 0.11971090099996218, "outcome": "passed"}, "teardown": {"duration": 0.00021552200007590727, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_missing"}, "setup": {"duration": 0.022218142000042462, "outcome": "passed"}, "call": {"duration": 0.006155776000014157, "outcome": "passed"}, "teardown": {"duration": 0.00021934899996267632, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022292010000001028, "outcome": "passed"}, "call": {"duration": 0.006097686999964935, "outcome": "passed"}, "teardown": {"duration": 0.00023042899999836663, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.021931667999979254, "outcome": "passed"}, "call": {"duration": 0.10222746800002369, "outcome": "passed"}, "teardown": {"duration": 0.000319816000001083, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023856062999925598, "outcome": "passed"}, "call": {"duration": 0.08072193299994979, "outcome": "passed"}, "teardown": {"duration": 0.0003017819999513449, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023861524000039935, "outcome": "passed"}, "call": {"duration": 0.11590789000001678, "outcome": "passed"}, "teardown": {"duration": 0.00029289600001902727, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.023776505000000725, "outcome": "passed"}, "call": {"duration": 0.005660008000063499, "outcome": "passed"}, "teardown": {"duration": 0.0002165930000046501, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.023208018999980595, "outcome": "passed"}, "call": {"duration": 0.006322531000023446, "outcome": "passed"}, "teardown": {"duration": 0.00021056299999600014, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022388489000036316, "outcome": "passed"}, "call": {"duration": 0.11085980699999709, "outcome": "passed"}, "teardown": {"duration": 0.00032556700000441197, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022731718999921213, "outcome": "passed"}, "call": {"duration": 0.08076346799998646, "outcome": "passed"}, "teardown": {"duration": 0.0002172540000628942, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02230319000000236, "outcome": "passed"}, "call": {"duration": 0.10355278600002293, "outcome": "passed"}, "teardown": {"duration": 0.00023014900000362104, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_missing"}, "setup": {"duration": 0.022686884999984613, "outcome": "passed"}, "call": {"duration": 0.006042342999990069, "outcome": "passed"}, "teardown": {"duration": 0.0002168340000707758, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02209493200007273, "outcome": "passed"}, "call": {"duration": 0.006153637000011258, "outcome": "passed"}, "teardown": {"duration": 0.00021101300001191703, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022479680000060398, "outcome": "passed"}, "call": {"duration": 0.10187483199990766, "outcome": "passed"}, "teardown": {"duration": 0.0002985360000593573, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022808671999996477, "outcome": "passed"}, "call": {"duration": 0.0817051569999876, "outcome": "passed"}, "teardown": {"duration": 0.0003406350000432212, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023661140000058367, "outcome": "passed"}, "call": {"duration": 0.1027432639999688, "outcome": "passed"}, "teardown": {"duration": 0.00031262200002402096, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022457508000002235, "outcome": "passed"}, "call": {"duration": 0.0001610200000641271, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.00017204099992795818, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.030559465000010277, "outcome": "passed"}, "call": {"duration": 2.0341708810000227, "outcome": "passed"}, "teardown": {"duration": 0.0002765349998981037, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02381826499993167, "outcome": "passed"}, "call": {"duration": 3.549573541000086, "outcome": "passed"}, "teardown": {"duration": 0.0002318629999535915, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02285001799998554, "outcome": "passed"}, "call": {"duration": 0.0002423119999548362, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.00015967800004546007, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02194014300005165, "outcome": "passed"}, "call": {"duration": 71.35845404700001, "outcome": "passed"}, "teardown": {"duration": 0.00028720499994960846, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022466314999974202, "outcome": "passed"}, "call": {"duration": 4.266439817999981, "outcome": "passed"}, "teardown": {"duration": 0.00020144599989180278, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "calendar"}, "setup": {"duration": 0.022301338000033866, "outcome": "passed"}, "call": {"duration": 0.4889809610001521, "outcome": "passed"}, "teardown": {"duration": 0.00019901000018762716, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "math"}, "setup": {"duration": 0.022227058999988003, "outcome": "passed"}, "call": {"duration": 3.404610140999921, "outcome": "passed"}, "teardown": {"duration": 0.0002635109999573615, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "calendar"}, "setup": {"duration": 0.022653313000091657, "outcome": "passed"}, "call": {"duration": 0.6594032780001271, "outcome": "passed"}, "teardown": {"duration": 0.00023879500008661125, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "math"}, "setup": {"duration": 0.02249903599999925, "outcome": "passed"}, "call": {"duration": 3.4883035109999128, "outcome": "passed"}, "teardown": {"duration": 0.00020593399995050277, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "calendar"}, "setup": {"duration": 0.023155409000082727, "outcome": "passed"}, "call": {"duration": 0.6431459489999725, "outcome": "passed"}, "teardown": {"duration": 0.00020289799999773095, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "math"}, "setup": {"duration": 0.023338179999882414, "outcome": "passed"}, "call": {"duration": 3.0588576529999045, "outcome": "passed"}, "teardown": {"duration": 0.00019962199985457119, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "calendar"}, "setup": {"duration": 0.024053382999909445, "outcome": "passed"}, "call": {"duration": 2.434681280999939, "outcome": "passed"}, "teardown": {"duration": 0.00021705399990423757, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "math"}, "setup": {"duration": 0.032734742999991795, "outcome": "passed"}, "call": {"duration": 1.9569903579999846, "outcome": "passed"}, "teardown": {"duration": 0.00020117499980187858, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "calendar"}, "setup": {"duration": 0.02278496099984295, "outcome": "passed"}, "call": {"duration": 0.5458311859999867, "outcome": "passed"}, "teardown": {"duration": 0.0002553659999193769, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "math"}, "setup": {"duration": 0.022457301000031293, "outcome": "passed"}, "call": {"duration": 4.959127508000165, "outcome": "passed"}, "teardown": {"duration": 0.00023377599995910714, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "calendar"}, "setup": {"duration": 0.022227059999977428, "outcome": "passed"}, "call": {"duration": 0.5947096330000932, "outcome": "passed"}, "teardown": {"duration": 0.0002602849999675527, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "math"}, "setup": {"duration": 0.02248830700000326, "outcome": "passed"}, "call": {"duration": 2.5274929759998486, "outcome": "passed"}, "teardown": {"duration": 0.00020129499989707256, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02261520299998665, "outcome": "passed"}, "call": {"duration": 0.30894983500002127, "outcome": "passed"}, "teardown": {"duration": 0.00023723200001768419, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022144956999909482, "outcome": "passed"}, "call": {"duration": 0.2709659310000916, "outcome": "passed"}, "teardown": {"duration": 0.00027432100000623905, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02344061100006911, "outcome": "passed"}, "call": {"duration": 0.45046814399984214, "outcome": "passed"}, "teardown": {"duration": 0.00020554299999275827, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.023635223000155747, "outcome": "passed"}, "call": {"duration": 0.4889586770000278, "outcome": "passed"}, "teardown": {"duration": 0.00020279799991840264, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022502924000036728, "outcome": "passed"}, "call": {"duration": 0.3213978829999178, "outcome": "passed"}, "teardown": {"duration": 0.0002368919999753416, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02242554999997992, "outcome": "passed"}, "call": {"duration": 0.38085772099998394, "outcome": "passed"}, "teardown": {"duration": 0.0003108490000158781, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022494207000136157, "outcome": "passed"}, "call": {"duration": 0.758612587000016, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'detail': \"Error code: 400 - {'id': 'nuDPAzp-3NKUce-940760ce6a1621a3', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}\"}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 284, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f4a564f2ad0>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:284: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f4a564f2ad0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'detail': \"Error code: 400 - {'id': 'nuDPAzp-3NKUce-940760ce6a1621a3', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}\"}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00024261199996544747, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022456377000025896, "outcome": "passed"}, "call": {"duration": 1.3336961810000503, "outcome": "passed"}, "teardown": {"duration": 0.00023039000006974675, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022774930000196036, "outcome": "passed"}, "call": {"duration": 0.9236113160000059, "outcome": "passed"}, "teardown": {"duration": 0.00024135900002875132, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022736959999974715, "outcome": "passed"}, "call": {"duration": 0.46922422400007235, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 400: Error code: 400 - {'id': 'nuDPBxq-3NKUce-940760e30d6c21a3', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 316, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f4a566e9690>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:316: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7f4a566ab520>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 400: Error code: 400 - {'id': 'nuDPBxq-3NKUce-940760e30d6c21a3', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00021311799991963198, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02206854399992153, "outcome": "passed"}, "call": {"duration": 1.1217196930001592, "outcome": "passed"}, "teardown": {"duration": 0.00019869999982802256, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02221471699999711, "outcome": "passed"}, "call": {"duration": 0.7781956920000539, "outcome": "passed"}, "teardown": {"duration": 0.0002037099998233316, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022666499000024487, "outcome": "passed"}, "call": {"duration": 1.429567711000118, "outcome": "passed"}, "teardown": {"duration": 0.00024196100002882304, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02296706900006029, "outcome": "passed"}, "call": {"duration": 6.119419524000023, "outcome": "passed"}, "teardown": {"duration": 0.00021541199998864613, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02261753800007682, "outcome": "passed"}, "call": {"duration": 0.9370640509998793, "outcome": "passed"}, "teardown": {"duration": 0.00024155999994945887, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022349267000208783, "outcome": "passed"}, "call": {"duration": 2.3407767109999895, "outcome": "passed"}, "teardown": {"duration": 0.00020893900000373833, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02230256100006045, "outcome": "passed"}, "call": {"duration": 9.203146424999886, "outcome": "passed"}, "teardown": {"duration": 0.00028294799994910136, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022347295000145095, "outcome": "passed"}, "call": {"duration": 1.2587480719998894, "outcome": "passed"}, "teardown": {"duration": 0.00030105099995125784, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02227250600003572, "outcome": "passed"}, "call": {"duration": 0.4388754159999735, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_qa6xmxlmgceufw9ax0qyw775', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_qa6xmxlmgceufw9ax0qyw775', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_qa6xmxlmgceufw9ax0qyw775', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a566aa050>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_qa6xmxlmgceufw9ax0qyw775', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_qa6xmxlmgceufw9ax0qyw775', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_qa6xmxlmgceufw9ax0qyw775', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021832600009474845, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022037907999902018, "outcome": "passed"}, "call": {"duration": 0.7302385350001259, "outcome": "passed"}, "teardown": {"duration": 0.00021139400018910237, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "add_product_tool"}, "setup": {"duration": 0.022632475999898816, "outcome": "passed"}, "call": {"duration": 1.107093409000072, "outcome": "passed"}, "teardown": {"duration": 0.000252561000024798, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.025807751999991524, "outcome": "passed"}, "call": {"duration": 3.48292152099998, "outcome": "passed"}, "teardown": {"duration": 0.0002207509999152535, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.023671551999996154, "outcome": "passed"}, "call": {"duration": 2.02399891999994, "outcome": "passed"}, "teardown": {"duration": 0.00021115300000928983, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022376938999968843, "outcome": "passed"}, "call": {"duration": 0.3103093759998501, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_iupuj42eqpsuj39smaek9evs', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_iupuj42eqpsuj39smaek9evs', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_iupuj42eqpsuj39smaek9evs', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a56563fa0>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_iupuj42eqpsuj39smaek9evs', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_iupuj42eqpsuj39smaek9evs', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_iupuj42eqpsuj39smaek9evs', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021010199998272583, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02196048299992981, "outcome": "passed"}, "call": {"duration": 0.5995013570000083, "outcome": "passed"}, "teardown": {"duration": 0.00022985900000094261, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022250753999969675, "outcome": "passed"}, "call": {"duration": 1.5135717360001308, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_mp060pbrqy4nph7jrnbrqtky', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_mp060pbrqy4nph7jrnbrqtky', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mp060pbrqy4nph7jrnbrqtky', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a566ba3e0>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_mp060pbrqy4nph7jrnbrqtky', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_mp060pbrqy4nph7jrnbrqtky', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mp060pbrqy4nph7jrnbrqtky', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00021124399995642307, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022957781000059185, "outcome": "passed"}, "call": {"duration": 1.8925786600000265, "outcome": "passed"}, "teardown": {"duration": 0.00022192299979906238, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.023931486000037694, "outcome": "passed"}, "call": {"duration": 1.4670145990000947, "outcome": "passed"}, "teardown": {"duration": 0.000234067000064897, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022801971000035337, "outcome": "passed"}, "call": {"duration": 1.2334095169999273, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_kgyy9yimqr68wk4acejmbfp8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_kgyy9yimqr68wk4acejmbfp8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kgyy9yimqr68wk4acejmbfp8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a567a5cf0>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_kgyy9yimqr68wk4acejmbfp8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_kgyy9yimqr68wk4acejmbfp8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kgyy9yimqr68wk4acejmbfp8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.000228095000011308, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022180361999971865, "outcome": "passed"}, "call": {"duration": 0.7325426349998452, "outcome": "passed"}, "teardown": {"duration": 0.0002218529998572194, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "add_product_tool"}, "setup": {"duration": 0.022502152000015485, "outcome": "passed"}, "call": {"duration": 1.0242046549999486, "outcome": "passed"}, "teardown": {"duration": 0.0002278849999584054, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02210288700007368, "outcome": "passed"}, "call": {"duration": 1.4069012269999348, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '\"2025-03-03\"', 'location': '\"Main Conference Room\"', 'name': '\"Team Building\"', 'participants': ['Alice', 'Bob', 'Charlie'], 'time': '\"10:00\"'}'\nassert {'date': '\"20...harlie'], ...} == {'date': '202...harlie'], ...}\n  \n  Omitting 1 identical items, use -vv to show\n  Differing items:\n  {'location': '\"Main Conference Room\"'} != {'location': 'Main Conference Room'}\n  {'date': '\"2025-03-03\"'} != {'date': '2025-03-03'}\n  {'name': '\"Team Building\"'} != {'name': 'Team Building'}\n  {'time': '\"10:00\"'} != {'time': '10:00'}\n  \n  Full diff:\n    {\n  -     'date': '2025-03-03',\n  +     'date': '\"2025-03-03\"',\n  ?              +          +\n  -     'location': 'Main Conference Room',\n  +     'location': '\"Main Conference Room\"',\n  ?                  +                    +\n  -     'name': 'Team Building',\n  +     'name': '\"Team Building\"',\n  ?              +             +\n        'participants': [\n            'Alice',\n            'Bob',\n            'Charlie',\n        ],\n  -     'time': '10:00',\n  +     'time': '\"10:00\"',\n  ?              +     +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a565ff910>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '\"2025-03-03\"', 'location': '\"Main Conference Room\"', 'name': '\"Team Building\"', 'participants': ['Alice', 'Bob', 'Charlie'], 'time': '\"10:00\"'}'\nE               assert {'date': '\"20...harlie'], ...} == {'date': '202...harlie'], ...}\nE                 \nE                 Omitting 1 identical items, use -vv to show\nE                 Differing items:\nE                 {'location': '\"Main Conference Room\"'} != {'location': 'Main Conference Room'}\nE                 {'date': '\"2025-03-03\"'} != {'date': '2025-03-03'}\nE                 {'name': '\"Team Building\"'} != {'name': 'Team Building'}\nE                 {'time': '\"10:00\"'} != {'time': '10:00'}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'date': '2025-03-03',\nE                 +     'date': '\"2025-03-03\"',\nE                 ?              +          +\nE                 -     'location': 'Main Conference Room',\nE                 +     'location': '\"Main Conference Room\"',\nE                 ?                  +                    +\nE                 -     'name': 'Team Building',\nE                 +     'name': '\"Team Building\"',\nE                 ?              +             +\nE                       'participants': [\nE                           'Alice',\nE                           'Bob',\nE                           'Charlie',\nE                       ],\nE                 -     'time': '10:00',\nE                 +     'time': '\"10:00\"',\nE                 ?              +     +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:445: AssertionError"}, "teardown": {"duration": 0.00022551000006387767, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02225928899997598, "outcome": "passed"}, "call": {"duration": 1.7408509909998884, "outcome": "passed"}, "teardown": {"duration": 0.00024302299993905763, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.023365630999933273, "outcome": "passed"}, "call": {"duration": 0.7091672640001434, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_e4r8hguhlgrc4n8048p2cekf', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a567ca560>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_e4r8hguhlgrc4n8048p2cekf', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021282699981384212, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022049598999956288, "outcome": "passed"}, "call": {"duration": 1.2015510070000346, "outcome": "passed"}, "teardown": {"duration": 0.0001981600000817707, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "add_product_tool"}, "setup": {"duration": 0.022423375000016676, "outcome": "passed"}, "call": {"duration": 1.352754078000089, "outcome": "passed"}, "teardown": {"duration": 0.0002946900001461472, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022749237999960314, "outcome": "passed"}, "call": {"duration": 3.14189133900004, "outcome": "passed"}, "teardown": {"duration": 0.00023627099994882883, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022433032999970237, "outcome": "passed"}, "call": {"duration": 1.642983021999953, "outcome": "passed"}, "teardown": {"duration": 0.0003488799998194736, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022228449999829536, "outcome": "passed"}, "call": {"duration": 0.8368513399998392, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 545, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'I am not able to complete this task as it falls outside of the scope of the functions I have been given.'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f4a5644ab20>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 545, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a567a7d60>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'I am not able to complete this task as it falls outside of the scope of the functions I have been given.'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f4a5644ab20>)\n\ntests/verifications/openai_api/test_chat_completion.py:545: AssertionError"}, "teardown": {"duration": 0.00021082300008856691, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022507700999994995, "outcome": "passed"}, "call": {"duration": 0.8736030759998812, "outcome": "passed"}, "teardown": {"duration": 0.00023183200005405524, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.023709921000090617, "outcome": "passed"}, "call": {"duration": 2.136329478000107, "outcome": "passed"}, "teardown": {"duration": 0.00025523600015731063, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02246476300001632, "outcome": "passed"}, "call": {"duration": 3.729355195000153, "outcome": "passed"}, "teardown": {"duration": 0.00020166600006632507, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022247647999847686, "outcome": "passed"}, "call": {"duration": 1.6961109020001004, "outcome": "passed"}, "teardown": {"duration": 0.00037596099991787923, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022328558000026533, "outcome": "passed"}, "call": {"duration": 3.1359798419998697, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"location\":\"\"}', 'name': 'get_weather'}, 'id': 'call_f8ao3kdoldafkdw7j4euzc20', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a566d2bf0>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"location\":\"\"}', 'name': 'get_weather'}, 'id': 'call_f8ao3kdoldafkdw7j4euzc20', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0002204099998834863, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02212958899986006, "outcome": "passed"}, "call": {"duration": 0.810628731000179, "outcome": "passed"}, "teardown": {"duration": 0.0002299190000485396, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "add_product_tool"}, "setup": {"duration": 0.022737802000165175, "outcome": "passed"}, "call": {"duration": 1.2091156030001002, "outcome": "passed"}, "teardown": {"duration": 0.0002955810000457859, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02208141899996008, "outcome": "passed"}, "call": {"duration": 1.665802092000149, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 528, "message": "AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '\"2025-03-03\"', 'location': '\"Main Conference Room\"', 'name': '\"Team Building\"', 'participants': ['Alice', 'Bob', 'Charlie'], 'time': '\"10:00\"'}'\nassert {'date': '\"20...harlie'], ...} == {'date': '202...harlie'], ...}\n  \n  Omitting 1 identical items, use -vv to show\n  Differing items:\n  {'location': '\"Main Conference Room\"'} != {'location': 'Main Conference Room'}\n  {'date': '\"2025-03-03\"'} != {'date': '2025-03-03'}\n  {'name': '\"Team Building\"'} != {'name': 'Team Building'}\n  {'time': '\"10:00\"'} != {'time': '10:00'}\n  \n  Full diff:\n    {\n  -     'date': '2025-03-03',\n  +     'date': '\"2025-03-03\"',\n  ?              +          +\n  -     'location': 'Main Conference Room',\n  +     'location': '\"Main Conference Room\"',\n  ?                  +                    +\n  -     'name': 'Team Building',\n  +     'name': '\"Team Building\"',\n  ?              +             +\n        'participants': [\n            'Alice',\n            'Bob',\n            'Charlie',\n        ],\n  -     'time': '10:00',\n  +     'time': '\"10:00\"',\n  ?              +     +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 528, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a566ea020>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '\"2025-03-03\"', 'location': '\"Main Conference Room\"', 'name': '\"Team Building\"', 'participants': ['Alice', 'Bob', 'Charlie'], 'time': '\"10:00\"'}'\nE               assert {'date': '\"20...harlie'], ...} == {'date': '202...harlie'], ...}\nE                 \nE                 Omitting 1 identical items, use -vv to show\nE                 Differing items:\nE                 {'location': '\"Main Conference Room\"'} != {'location': 'Main Conference Room'}\nE                 {'date': '\"2025-03-03\"'} != {'date': '2025-03-03'}\nE                 {'name': '\"Team Building\"'} != {'name': 'Team Building'}\nE                 {'time': '\"10:00\"'} != {'time': '10:00'}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'date': '2025-03-03',\nE                 +     'date': '\"2025-03-03\"',\nE                 ?              +          +\nE                 -     'location': 'Main Conference Room',\nE                 +     'location': '\"Main Conference Room\"',\nE                 ?                  +                    +\nE                 -     'name': 'Team Building',\nE                 +     'name': '\"Team Building\"',\nE                 ?              +             +\nE                       'participants': [\nE                           'Alice',\nE                           'Bob',\nE                           'Charlie',\nE                       ],\nE                 -     'time': '10:00',\nE                 +     'time': '\"10:00\"',\nE                 ?              +     +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:528: AssertionError"}, "teardown": {"duration": 0.00021019199994043447, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022179691000019375, "outcome": "passed"}, "call": {"duration": 2.6213292469999487, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 528, "message": "AssertionError: Expected arguments '{'month': 2, 'year': 2024}', got '{'month': 2, 'year': None}'\nassert {'month': 2, 'year': None} == {'month': 2, 'year': 2024}\n  \n  Omitting 1 identical items, use -vv to show\n  Differing items:\n  {'year': None} != {'year': 2024}\n  \n  Full diff:\n    {\n        'month': 2,\n  -     'year': 2024,\n  ?             ^^^^\n  +     'year': None,\n  ?             ^^^^\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 528, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4a565c1150>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 2, 'year': 2024}', got '{'month': 2, 'year': None}'\nE               assert {'month': 2, 'year': None} == {'month': 2, 'year': 2024}\nE                 \nE                 Omitting 1 identical items, use -vv to show\nE                 Differing items:\nE                 {'year': None} != {'year': 2024}\nE                 \nE                 Full diff:\nE                   {\nE                       'month': 2,\nE                 -     'year': 2024,\nE                 ?             ^^^^\nE                 +     'year': None,\nE                 ?             ^^^^\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:528: AssertionError"}, "teardown": {"duration": 0.00020915000004606554, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "stream=False"}, "setup": {"duration": 0.022889355999950567, "outcome": "passed"}, "call": {"duration": 0.00016620999986116658, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.00015985799996087735, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "stream=True"}, "setup": {"duration": 0.02304253199986306, "outcome": "passed"}, "call": {"duration": 0.00014976800002841628, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.0001578240000981168, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "stream=False"}, "setup": {"duration": 0.022577815999966333, "outcome": "passed"}, "call": {"duration": 2.5228774110000813, "outcome": "passed"}, "teardown": {"duration": 0.00020893900000373833, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "stream=True"}, "setup": {"duration": 0.02619244300012724, "outcome": "passed"}, "call": {"duration": 2.9969618489999448, "outcome": "passed"}, "teardown": {"duration": 0.000326918999917325, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "stream=False"}, "setup": {"duration": 0.02300509300016529, "outcome": "passed"}, "call": {"duration": 2.6098231980001856, "outcome": "passed"}, "teardown": {"duration": 0.00021561199991992908, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "stream=True"}, "setup": {"duration": 0.024553904999947918, "outcome": "passed"}, "call": {"duration": 3.1514853420001145, "outcome": "passed"}, "teardown": {"duration": 0.002446328999894831, "outcome": "passed"}}]}