{"created": 1747102020.678085, "duration": 264.52594566345215, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 53, "failed": 57, "skipped": 4, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "earth"}, "setup": {"duration": 0.053698335999996516, "outcome": "passed"}, "call": {"duration": 0.6486902949999944, "outcome": "passed"}, "teardown": {"duration": 0.0002143099999898368, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "saturn"}, "setup": {"duration": 0.022610436000007894, "outcome": "passed"}, "call": {"duration": 0.7283838050000213, "outcome": "passed"}, "teardown": {"duration": 0.0002150120000123934, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "earth"}, "setup": {"duration": 0.023360898999982282, "outcome": "passed"}, "call": {"duration": 0.28543811800000185, "outcome": "passed"}, "teardown": {"duration": 0.00020812899998645662, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "saturn"}, "setup": {"duration": 0.023375175999973408, "outcome": "passed"}, "call": {"duration": 0.42451081799998747, "outcome": "passed"}, "teardown": {"duration": 0.00021788699999092387, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "earth"}, "setup": {"duration": 0.023444686000004822, "outcome": "passed"}, "call": {"duration": 1.008151381999994, "outcome": "passed"}, "teardown": {"duration": 0.00023596100001554987, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "saturn"}, "setup": {"duration": 0.023385324000003038, "outcome": "passed"}, "call": {"duration": 0.3000443950000147, "outcome": "passed"}, "teardown": {"duration": 0.00019205899999974463, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "earth"}, "setup": {"duration": 0.02320945600001778, "outcome": "passed"}, "call": {"duration": 0.4021730810000008, "outcome": "passed"}, "teardown": {"duration": 0.00021411000000171043, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "saturn"}, "setup": {"duration": 0.02332623399999534, "outcome": "passed"}, "call": {"duration": 0.7223691599999995, "outcome": "passed"}, "teardown": {"duration": 0.00022002100001827785, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "lineno": 65, "outcome": "failed", "keywords": ["test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "earth"}, "setup": {"duration": 0.029612844000013183, "outcome": "passed"}, "call": {"duration": 0.6074403720000134, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7eed3f0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'earth', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}]}, 'output': 'Earth'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n        content = \"\"\n        for chunk in response:\n>           content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:83: IndexError"}, "teardown": {"duration": 0.0002400890000160416, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "lineno": 65, "outcome": "failed", "keywords": ["test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "saturn"}, "setup": {"duration": 0.022497514000008323, "outcome": "passed"}, "call": {"duration": 0.5065573219999919, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]>>\nopenai_client = <openai.OpenAI object at 0x7f01d822cd90>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'saturn', 'input': {'messages': [{'content': 'Which planet has rings around it with a name starting with letter S?', 'role': 'user'}]}, 'output': 'Saturn'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n        content = \"\"\n        for chunk in response:\n>           content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:83: IndexError"}, "teardown": {"duration": 0.0002204009999786649, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "lineno": 65, "outcome": "failed", "keywords": ["test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "earth"}, "setup": {"duration": 0.022582352999990007, "outcome": "passed"}, "call": {"duration": 1.0240333170000042, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]>>\nopenai_client = <openai.OpenAI object at 0x7f01d816c580>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'earth', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}]}, 'output': 'Earth'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n        content = \"\"\n        for chunk in response:\n>           content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:83: IndexError"}, "teardown": {"duration": 0.00021562300000255163, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "lineno": 65, "outcome": "failed", "keywords": ["test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "saturn"}, "setup": {"duration": 0.022235775000012836, "outcome": "passed"}, "call": {"duration": 0.280089342999986, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 83, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f57d60>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'saturn', 'input': {'messages': [{'content': 'Which planet has rings around it with a name starting with letter S?', 'role': 'user'}]}, 'output': 'Saturn'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n        content = \"\"\n        for chunk in response:\n>           content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:83: IndexError"}, "teardown": {"duration": 0.00020672700000545774, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_missing"}, "setup": {"duration": 0.022546916999999667, "outcome": "passed"}, "call": {"duration": 0.7131868830000201, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7fa6020>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0002085099999931117, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02231081500002574, "outcome": "passed"}, "call": {"duration": 0.5186722550000127, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7fa4640>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0002220449999867924, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022578186000004052, "outcome": "passed"}, "call": {"duration": 0.13831779800000277, "outcome": "passed"}, "teardown": {"duration": 0.00023993899998231427, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022813594000012927, "outcome": "passed"}, "call": {"duration": 0.12777071300001808, "outcome": "passed"}, "teardown": {"duration": 0.0001953549999882398, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022890209000024697, "outcome": "passed"}, "call": {"duration": 0.43388258599998153, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f7b6d0>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00020634599999880265, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022629039999998213, "outcome": "passed"}, "call": {"duration": 4.487028595999988, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "assert 400 == 500\n +  where 500 = InternalServerError(\"Error code: 500 - {'id': 'ntpQFEi-6UHjtw-93ee9370bc3beb28', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").status_code\n +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQFEi-6UHjtw-93ee9370bc3beb28', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQFEi-6UHjtw-93ee9370bc3beb28', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81a7d60>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       assert 400 == 500\nE        +  where 500 = InternalServerError(\"Error code: 500 - {'id': 'ntpQFEi-6UHjtw-93ee9370bc3beb28', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").status_code\nE        +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQFEi-6UHjtw-93ee9370bc3beb28', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQFEi-6UHjtw-93ee9370bc3beb28', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.00021186500001135755, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022817761999988306, "outcome": "passed"}, "call": {"duration": 0.5314988760000006, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f7a710>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0002189189999910468, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022529624000014792, "outcome": "passed"}, "call": {"duration": 0.2534928519999937, "outcome": "passed"}, "teardown": {"duration": 0.00021586400001183392, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022587292000025627, "outcome": "passed"}, "call": {"duration": 0.1579383650000068, "outcome": "passed"}, "teardown": {"duration": 0.00019786000001431603, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022837871000007226, "outcome": "passed"}, "call": {"duration": 0.3290939580000156, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f5e290>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0002142410000089967, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_missing"}, "setup": {"duration": 0.022737301999995907, "outcome": "passed"}, "call": {"duration": 4.524163940000022, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "assert 400 == 500\n +  where 500 = InternalServerError(\"Error code: 500 - {'id': 'ntpQH1t-28Eivz-93ee93957d7fcf7a', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").status_code\n +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQH1t-28Eivz-93ee93957d7fcf7a', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQH1t-28Eivz-93ee93957d7fcf7a', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f5cd00>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       assert 400 == 500\nE        +  where 500 = InternalServerError(\"Error code: 500 - {'id': 'ntpQH1t-28Eivz-93ee93957d7fcf7a', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").status_code\nE        +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQH1t-28Eivz-93ee93957d7fcf7a', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQH1t-28Eivz-93ee93957d7fcf7a', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.0002121969999961948, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022586782000018957, "outcome": "passed"}, "call": {"duration": 0.9231772100000057, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f48f10>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0002098120000084691, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022414573000020255, "outcome": "passed"}, "call": {"duration": 0.1163635399999805, "outcome": "passed"}, "teardown": {"duration": 0.00019916300001909804, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022113439999998263, "outcome": "passed"}, "call": {"duration": 0.09448747800001911, "outcome": "passed"}, "teardown": {"duration": 0.00017095900000185793, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022049910999982103, "outcome": "passed"}, "call": {"duration": 0.2998862059999965, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f9c940>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00021885900000029324, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_missing"}, "setup": {"duration": 0.022446221999985028, "outcome": "passed"}, "call": {"duration": 1.2835155269999916, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7fa7520>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.0002557680000165874, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02265013400000271, "outcome": "passed"}, "call": {"duration": 0.39614520399999265, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81083a0>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00021116499999607186, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022301280999982964, "outcome": "passed"}, "call": {"duration": 0.1193079439999849, "outcome": "passed"}, "teardown": {"duration": 0.00018421400000079302, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022037839000006443, "outcome": "passed"}, "call": {"duration": 0.23918139499997437, "outcome": "passed"}, "teardown": {"duration": 0.00021344799998246344, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022522124000005306, "outcome": "passed"}, "call": {"duration": 0.5803986559999998, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f9e980>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00022457000000031258, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022392110999987835, "outcome": "passed"}, "call": {"duration": 4.558556745000004, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "assert '400' in \"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\"\n +  where '400' = str(400)\n +  and   \"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\" = InternalServerError(\"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").message\n +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7eee2f0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       assert '400' in \"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\"\nE        +  where '400' = str(400)\nE        +  and   \"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\" = InternalServerError(\"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").message\nE        +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQKfa-2j9zxn-93ee93cdfa99944c', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 0.00023785500002304616, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022382426999996596, "outcome": "passed"}, "call": {"duration": 0.2684276319999981, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d82d0b80>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00020982300000582654, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022371535999980097, "outcome": "passed"}, "call": {"duration": 0.21252280600000972, "outcome": "passed"}, "teardown": {"duration": 0.00020345000001498192, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022389679000013984, "outcome": "passed"}, "call": {"duration": 0.26968237599999156, "outcome": "passed"}, "teardown": {"duration": 0.00021435000002156812, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022725828000005777, "outcome": "passed"}, "call": {"duration": 0.29534543200000485, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f3d780>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.0002118460000133382, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_missing"}, "setup": {"duration": 0.022580476999991106, "outcome": "passed"}, "call": {"duration": 4.424218808999996, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "assert '400' in \"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\"\n +  where '400' = str(400)\n +  and   \"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\" = InternalServerError(\"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").message\n +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7f01d816fa00>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       assert '400' in \"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\"\nE        +  where '400' = str(400)\nE        +  and   \"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\" = InternalServerError(\"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\").message\nE        +    where InternalServerError(\"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'id': 'ntpQMLt-zqrih-93ee93f139db1975', 'error': {'message': 'Internal server error', 'type': 'server_error', 'param': None, 'code': None}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 0.00022480000001223743, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02258919300001594, "outcome": "passed"}, "call": {"duration": 0.6795961019999766, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f42050>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00023538899998243323, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022417412000010017, "outcome": "passed"}, "call": {"duration": 0.14359730200001763, "outcome": "passed"}, "teardown": {"duration": 0.00020309900000370362, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02288837200001126, "outcome": "passed"}, "call": {"duration": 0.09565025500000957, "outcome": "passed"}, "teardown": {"duration": 0.00018900300000268544, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023371615000002066, "outcome": "passed"}, "call": {"duration": 0.299947665000019, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e67bb0>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.000249064999991333, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.026127594999991288, "outcome": "passed"}, "call": {"duration": 0.0001668010000059894, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together based on config.')"}, "teardown": {"duration": 0.00016632099999469574, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022931712999991305, "outcome": "passed"}, "call": {"duration": 1.6248258199999839, "outcome": "passed"}, "teardown": {"duration": 0.00022069199999918965, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022599812999999358, "outcome": "passed"}, "call": {"duration": 3.569164766, "outcome": "passed"}, "teardown": {"duration": 0.00020782800001484247, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022673381000004156, "outcome": "passed"}, "call": {"duration": 0.00015991899999789894, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together based on config.')"}, "teardown": {"duration": 0.00016333499999632295, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 152, "outcome": "failed", "keywords": ["test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022223629000023948, "outcome": "passed"}, "call": {"duration": 1.573497433, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 170, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 170, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_image[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f7b280>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': [{'text': 'What is in this image?', 'type': 'text'}, {'image_url': {...}, 'type': 'image_url'}], 'role': 'user'}]}, 'output': 'llama'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_image\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_image(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n        content = \"\"\n        for chunk in response:\n>           content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:170: IndexError"}, "teardown": {"duration": 0.00021827800000551179, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 152, "outcome": "failed", "keywords": ["test_chat_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022460743000010552, "outcome": "passed"}, "call": {"duration": 4.103149967999997, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 170, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 170, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_image[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e07e50>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': [{'text': 'What is in this image?', 'type': 'text'}, {'image_url': {...}, 'type': 'image_url'}], 'role': 'user'}]}, 'output': 'llama'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_image\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_image(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n        content = \"\"\n        for chunk in response:\n>           content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:170: IndexError"}, "teardown": {"duration": 0.0002247199999771965, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "calendar"}, "setup": {"duration": 0.022776591999985385, "outcome": "passed"}, "call": {"duration": 0.4820440180000105, "outcome": "passed"}, "teardown": {"duration": 0.00020908099997996032, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "math"}, "setup": {"duration": 0.0227263980000032, "outcome": "passed"}, "call": {"duration": 1.4586565859999894, "outcome": "passed"}, "teardown": {"duration": 0.00020194699999365184, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "calendar"}, "setup": {"duration": 0.022396230999987665, "outcome": "passed"}, "call": {"duration": 0.5511143380000192, "outcome": "passed"}, "teardown": {"duration": 0.0002052030000072591, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "lineno": 176, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "math"}, "setup": {"duration": 0.02229403100000127, "outcome": "passed"}, "call": {"duration": 97.50890976999997, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 667, "message": "assert None is not None"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 197, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 667, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f573a0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        maybe_json_content = response.choices[0].message.content\n    \n>       validate_structured_output(maybe_json_content, case[\"output\"])\n\ntests/verifications/openai_api/test_chat_completion.py:197: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmaybe_json_content = '{ \\r\\n\\r\\n   \\r\\n  \\r\\n \\r\\n    \\r\\n    \\r\\n    \\r\\n   \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n  ...\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    \\r\\n    '\nschema_name = 'valid_math_reasoning'\n\n    def validate_structured_output(maybe_json_content: str, schema_name: str) -> None:\n        structured_output = get_structured_output(maybe_json_content, schema_name)\n>       assert structured_output is not None\nE       assert None is not None\n\ntests/verifications/openai_api/test_chat_completion.py:667: AssertionError"}, "teardown": {"duration": 0.0002042919999780679, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "calendar"}, "setup": {"duration": 0.02256416799997396, "outcome": "passed"}, "call": {"duration": 0.5518617809999569, "outcome": "passed"}, "teardown": {"duration": 0.0002241889999936575, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "math"}, "setup": {"duration": 0.022518622999996296, "outcome": "passed"}, "call": {"duration": 2.4415174260000185, "outcome": "passed"}, "teardown": {"duration": 0.00022483999998712534, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "calendar"}, "setup": {"duration": 0.02242224700000861, "outcome": "passed"}, "call": {"duration": 0.8957716670000195, "outcome": "passed"}, "teardown": {"duration": 0.0002237280000372266, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "math"}, "setup": {"duration": 0.02232843199999479, "outcome": "passed"}, "call": {"duration": 3.5343185480000443, "outcome": "passed"}, "teardown": {"duration": 0.0002113560000225334, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "lineno": 199, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "calendar"}, "setup": {"duration": 0.027099821000035718, "outcome": "passed"}, "call": {"duration": 0.561658953999995, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81a7a60>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n        maybe_json_content = \"\"\n        for chunk in response:\n>           maybe_json_content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:218: IndexError"}, "teardown": {"duration": 0.00022266700000272976, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "lineno": 199, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "math"}, "setup": {"duration": 0.022771519000002627, "outcome": "passed"}, "call": {"duration": 2.904617884000004, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/Llama-4-Scout-17B-16E-Instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f6b5b0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n        maybe_json_content = \"\"\n        for chunk in response:\n>           maybe_json_content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:218: IndexError"}, "teardown": {"duration": 0.0002166359999478118, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "lineno": 199, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "calendar"}, "setup": {"duration": 0.022639755999989575, "outcome": "passed"}, "call": {"duration": 0.5615906959999961, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7f01d816ea40>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n        maybe_json_content = \"\"\n        for chunk in response:\n>           maybe_json_content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:218: IndexError"}, "teardown": {"duration": 0.0002111149999564077, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "lineno": 199, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "math"}, "setup": {"duration": 0.02239055100000087, "outcome": "passed"}, "call": {"duration": 3.364842321000026, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 218, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7fa5150>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n        maybe_json_content = \"\"\n        for chunk in response:\n>           maybe_json_content += chunk.choices[0].delta.content or \"\"\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:218: IndexError"}, "teardown": {"duration": 0.00021311800003331882, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02265710200003923, "outcome": "passed"}, "call": {"duration": 0.5280499280000299, "outcome": "passed"}, "teardown": {"duration": 0.00022719399999004963, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022556280000003426, "outcome": "passed"}, "call": {"duration": 1.4912100369999735, "outcome": "passed"}, "teardown": {"duration": 0.00021400000002813613, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02266131400000404, "outcome": "passed"}, "call": {"duration": 0.5871112949999997, "outcome": "passed"}, "teardown": {"duration": 0.0002123369999935676, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022601822000012817, "outcome": "passed"}, "call": {"duration": 0.47139583800003493, "outcome": "passed"}, "teardown": {"duration": 0.0002162839999755306, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022602074000019456, "outcome": "passed"}, "call": {"duration": 0.3671255669999596, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 263, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d82d1c60>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:263: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d822fd90>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00023376800004371034, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022707840999999007, "outcome": "passed"}, "call": {"duration": 0.28292072400000734, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 263, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f48bb0>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:263: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d82d1390>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00020954199999323464, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02239186899998913, "outcome": "passed"}, "call": {"duration": 0.29994377600002053, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'id': 'ntpR2fP-3NKUce-93ee97281ad5645f', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 284, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e6f700>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:284: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f01d7e6f700>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'id': 'ntpR2fP-3NKUce-93ee97281ad5645f', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002471319999699517, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022659290000035526, "outcome": "passed"}, "call": {"duration": 0.4527355139999827, "outcome": "passed"}, "teardown": {"duration": 0.00020406200002298647, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022515801999986707, "outcome": "passed"}, "call": {"duration": 6.277654757999983, "outcome": "passed"}, "teardown": {"duration": 0.0002055140000152278, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022665119999999206, "outcome": "passed"}, "call": {"duration": 0.2514772429999539, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'id': 'ntpR4oa-3NKUce-93ee97552b85944c', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 308, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81c7a30>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:308: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f01d81c7a30>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'id': 'ntpR4oa-3NKUce-93ee97552b85944c', 'error': {'message': 'invalid tools grammar: Model supports only tool_choice auto', 'type': 'invalid_request_error', 'param': 'tools', 'code': None}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00022976000002472574, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02262395299999298, "outcome": "passed"}, "call": {"duration": 0.49919261899998446, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 316, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d822f550>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:316: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7f78160>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00021467199997005082, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022451521999983015, "outcome": "passed"}, "call": {"duration": 1.2159196080000356, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 316, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7dee560>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:316: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7e6e110>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00021064300000261937, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 324, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02239537600001995, "outcome": "passed"}, "call": {"duration": 1.266878362, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content='I\\'m a large language model, I don\\'t have have access to real-time weather information. But I can suggest some ways for you to find out the current weather in San Francisco.\\n\\nYou can check the weather forecast on websites such as:\\n\\n* AccuWeather (accuweather.com)\\n* Weather.com (weather.com)\\n* National Weather Service (weather.gov)\\n\\nYou can also use voice assistants like Siri, Google Assistant, or Alexa to get the current weather in San Francisco. Simply ask them \"What\\'s the weather like in San Francisco?\" and they will provide you with the latest forecast.\\n\\nAdditionally, you can check the weather forecast on your smartphone by downloading weather apps such as Dark Sky or Weather Underground.\\n\\nPlease note that the weather in San Francisco can be quite unpredictable, with foggy mornings and cool temperatures, especially near the Pacific Ocean. It\\'s always a good idea to check the forecast before heading out.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\n +    where ChatCompletionMessage(content='I\\'m a large language model, I don\\'t have have access to real-time weather information. But I can suggest some ways for you to find out the current weather in San Francisco.\\n\\nYou can check the weather forecast on websites such as:\\n\\n* AccuWeather (accuweather.com)\\n* Weather.com (weather.com)\\n* National Weather Service (weather.gov)\\n\\nYou can also use voice assistants like Siri, Google Assistant, or Alexa to get the current weather in San Francisco. Simply ask them \"What\\'s the weather like in San Francisco?\" and they will provide you with the latest forecast.\\n\\nAdditionally, you can check the weather forecast on your smartphone by downloading weather apps such as Dark Sky or Weather Underground.\\n\\nPlease note that the weather in San Francisco can be quite unpredictable, with foggy mornings and cool temperatures, especially near the Pacific Ocean. It\\'s always a good idea to check the forecast before heading out.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I\\'m a large language model, I don\\'t have have access to real-time weather information. But I can suggest some ways for you to find out the current weather in San Francisco.\\n\\nYou can check the weather forecast on websites such as:\\n\\n* AccuWeather (accuweather.com)\\n* Weather.com (weather.com)\\n* National Weather Service (weather.gov)\\n\\nYou can also use voice assistants like Siri, Google Assistant, or Alexa to get the current weather in San Francisco. Simply ask them \"What\\'s the weather like in San Francisco?\" and they will provide you with the latest forecast.\\n\\nAdditionally, you can check the weather forecast on your smartphone by downloading weather apps such as Dark Sky or Weather Underground.\\n\\nPlease note that the weather in San Francisco can be quite unpredictable, with foggy mornings and cool temperatures, especially near the Pacific Ocean. It\\'s always a good idea to check the forecast before heading out.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=135965431675537090).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81331c0>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content='I\\'m a large language model, I don\\'t have have access to real-time weather information. But I can suggest some ways for you to find out the current weather in San Francisco.\\n\\nYou can check the weather forecast on websites such as:\\n\\n* AccuWeather (accuweather.com)\\n* Weather.com (weather.com)\\n* National Weather Service (weather.gov)\\n\\nYou can also use voice assistants like Siri, Google Assistant, or Alexa to get the current weather in San Francisco. Simply ask them \"What\\'s the weather like in San Francisco?\" and they will provide you with the latest forecast.\\n\\nAdditionally, you can check the weather forecast on your smartphone by downloading weather apps such as Dark Sky or Weather Underground.\\n\\nPlease note that the weather in San Francisco can be quite unpredictable, with foggy mornings and cool temperatures, especially near the Pacific Ocean. It\\'s always a good idea to check the forecast before heading out.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\nE        +    where ChatCompletionMessage(content='I\\'m a large language model, I don\\'t have have access to real-time weather information. But I can suggest some ways for you to find out the current weather in San Francisco.\\n\\nYou can check the weather forecast on websites such as:\\n\\n* AccuWeather (accuweather.com)\\n* Weather.com (weather.com)\\n* National Weather Service (weather.gov)\\n\\nYou can also use voice assistants like Siri, Google Assistant, or Alexa to get the current weather in San Francisco. Simply ask them \"What\\'s the weather like in San Francisco?\" and they will provide you with the latest forecast.\\n\\nAdditionally, you can check the weather forecast on your smartphone by downloading weather apps such as Dark Sky or Weather Underground.\\n\\nPlease note that the weather in San Francisco can be quite unpredictable, with foggy mornings and cool temperatures, especially near the Pacific Ocean. It\\'s always a good idea to check the forecast before heading out.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I\\'m a large language model, I don\\'t have have access to real-time weather information. But I can suggest some ways for you to find out the current weather in San Francisco.\\n\\nYou can check the weather forecast on websites such as:\\n\\n* AccuWeather (accuweather.com)\\n* Weather.com (weather.com)\\n* National Weather Service (weather.gov)\\n\\nYou can also use voice assistants like Siri, Google Assistant, or Alexa to get the current weather in San Francisco. Simply ask them \"What\\'s the weather like in San Francisco?\" and they will provide you with the latest forecast.\\n\\nAdditionally, you can check the weather forecast on your smartphone by downloading weather apps such as Dark Sky or Weather Underground.\\n\\nPlease note that the weather in San Francisco can be quite unpredictable, with foggy mornings and cool temperatures, especially near the Pacific Ocean. It\\'s always a good idea to check the forecast before heading out.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=135965431675537090).message\n\ntests/verifications/openai_api/test_chat_completion.py:344: AssertionError"}, "teardown": {"duration": 0.0002512700000352197, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 324, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.023041926000018975, "outcome": "passed"}, "call": {"duration": 4.947518675000026, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content=\"The weather in San Francisco! San Francisco is known for its mild and pleasant climate, often referred to as a Mediterranean climate. Here's what you can expect:\\n\\n**Overall:**\\n\\n* San Francisco has a cool and foggy climate, with significant marine influence.\\n* The city experiences a narrow temperature range, with mild winters and cool summers.\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 46\u00b0F (8\u00b0C). Expect some rain, with an average of 3-4 inches (76-102 mm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 62\u00b0F (17\u00b0C) and lows around 50\u00b0F (10\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some sunshine, but also foggy mornings and cool afternoons.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Notable Weather Features:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. The fog often burns off by midday, but it can make mornings and evenings cool and chilly.\\n* **Wind:** San Francisco can be quite windy, especially during the afternoon and evening hours.\\n* **Microclimates:** San Francisco has many microclimates, meaning that weather conditions can vary significantly depending on the neighborhood or location. For example, the Haight-Ashbury neighborhood tends to be cooler and foggier than the Mission District.\\n\\n**Tips:**\\n\\n* Pack layers, as the temperature can drop significantly at night or when the fog rolls in.\\n* Bring a light jacket or sweater, even in the summer.\\n* If you're planning to visit outdoor attractions, try to schedule your visit during the late morning or early afternoon when the fog has burned off.\\n\\nOverall, San Francisco's climate is mild and pleasant, making it a great destination to visit year-round. Just be prepared for the fog and cooler temperatures!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\n +    where ChatCompletionMessage(content=\"The weather in San Francisco! San Francisco is known for its mild and pleasant climate, often referred to as a Mediterranean climate. Here's what you can expect:\\n\\n**Overall:**\\n\\n* San Francisco has a cool and foggy climate, with significant marine influence.\\n* The city experiences a narrow temperature range, with mild winters and cool summers.\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 46\u00b0F (8\u00b0C). Expect some rain, with an average of 3-4 inches (76-102 mm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 62\u00b0F (17\u00b0C) and lows around 50\u00b0F (10\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some sunshine, but also foggy mornings and cool afternoons.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Notable Weather Features:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. The fog often burns off by midday, but it can make mornings and evenings cool and chilly.\\n* **Wind:** San Francisco can be quite windy, especially during the afternoon and evening hours.\\n* **Microclimates:** San Francisco has many microclimates, meaning that weather conditions can vary significantly depending on the neighborhood or location. For example, the Haight-Ashbury neighborhood tends to be cooler and foggier than the Mission District.\\n\\n**Tips:**\\n\\n* Pack layers, as the temperature can drop significantly at night or when the fog rolls in.\\n* Bring a light jacket or sweater, even in the summer.\\n* If you're planning to visit outdoor attractions, try to schedule your visit during the late morning or early afternoon when the fog has burned off.\\n\\nOverall, San Francisco's climate is mild and pleasant, making it a great destination to visit year-round. Just be prepared for the fog and cooler temperatures!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The weather in San Francisco! San Francisco is known for its mild and pleasant climate, often referred to as a Mediterranean climate. Here's what you can expect:\\n\\n**Overall:**\\n\\n* San Francisco has a cool and foggy climate, with significant marine influence.\\n* The city experiences a narrow temperature range, with mild winters and cool summers.\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 46\u00b0F (8\u00b0C). Expect some rain, with an average of 3-4 inches (76-102 mm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 62\u00b0F (17\u00b0C) and lows around 50\u00b0F (10\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some sunshine, but also foggy mornings and cool afternoons.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Notable Weather Features:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. The fog often burns off by midday, but it can make mornings and evenings cool and chilly.\\n* **Wind:** San Francisco can be quite windy, especially during the afternoon and evening hours.\\n* **Microclimates:** San Francisco has many microclimates, meaning that weather conditions can vary significantly depending on the neighborhood or location. For example, the Haight-Ashbury neighborhood tends to be cooler and foggier than the Mission District.\\n\\n**Tips:**\\n\\n* Pack layers, as the temperature can drop significantly at night or when the fog rolls in.\\n* Bring a light jacket or sweater, even in the summer.\\n* If you're planning to visit outdoor attractions, try to schedule your visit during the late morning or early afternoon when the fog has burned off.\\n\\nOverall, San Francisco's climate is mild and pleasant, making it a great destination to visit year-round. Just be prepared for the fog and cooler temperatures!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f4b4f0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content=\"The weather in San Francisco! San Francisco is known for its mild and pleasant climate, often referred to as a Mediterranean climate. Here's what you can expect:\\n\\n**Overall:**\\n\\n* San Francisco has a cool and foggy climate, with significant marine influence.\\n* The city experiences a narrow temperature range, with mild winters and cool summers.\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 46\u00b0F (8\u00b0C). Expect some rain, with an average of 3-4 inches (76-102 mm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 62\u00b0F (17\u00b0C) and lows around 50\u00b0F (10\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some sunshine, but also foggy mornings and cool afternoons.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Notable Weather Features:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. The fog often burns off by midday, but it can make mornings and evenings cool and chilly.\\n* **Wind:** San Francisco can be quite windy, especially during the afternoon and evening hours.\\n* **Microclimates:** San Francisco has many microclimates, meaning that weather conditions can vary significantly depending on the neighborhood or location. For example, the Haight-Ashbury neighborhood tends to be cooler and foggier than the Mission District.\\n\\n**Tips:**\\n\\n* Pack layers, as the temperature can drop significantly at night or when the fog rolls in.\\n* Bring a light jacket or sweater, even in the summer.\\n* If you're planning to visit outdoor attractions, try to schedule your visit during the late morning or early afternoon when the fog has burned off.\\n\\nOverall, San Francisco's climate is mild and pleasant, making it a great destination to visit year-round. Just be prepared for the fog and cooler temperatures!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\nE        +    where ChatCompletionMessage(content=\"The weather in San Francisco! San Francisco is known for its mild and pleasant climate, often referred to as a Mediterranean climate. Here's what you can expect:\\n\\n**Overall:**\\n\\n* San Francisco has a cool and foggy climate, with significant marine influence.\\n* The city experiences a narrow temperature range, with mild winters and cool summers.\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 46\u00b0F (8\u00b0C). Expect some rain, with an average of 3-4 inches (76-102 mm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 62\u00b0F (17\u00b0C) and lows around 50\u00b0F (10\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some sunshine, but also foggy mornings and cool afternoons.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Notable Weather Features:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. The fog often burns off by midday, but it can make mornings and evenings cool and chilly.\\n* **Wind:** San Francisco can be quite windy, especially during the afternoon and evening hours.\\n* **Microclimates:** San Francisco has many microclimates, meaning that weather conditions can vary significantly depending on the neighborhood or location. For example, the Haight-Ashbury neighborhood tends to be cooler and foggier than the Mission District.\\n\\n**Tips:**\\n\\n* Pack layers, as the temperature can drop significantly at night or when the fog rolls in.\\n* Bring a light jacket or sweater, even in the summer.\\n* If you're planning to visit outdoor attractions, try to schedule your visit during the late morning or early afternoon when the fog has burned off.\\n\\nOverall, San Francisco's climate is mild and pleasant, making it a great destination to visit year-round. Just be prepared for the fog and cooler temperatures!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The weather in San Francisco! San Francisco is known for its mild and pleasant climate, often referred to as a Mediterranean climate. Here's what you can expect:\\n\\n**Overall:**\\n\\n* San Francisco has a cool and foggy climate, with significant marine influence.\\n* The city experiences a narrow temperature range, with mild winters and cool summers.\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 46\u00b0F (8\u00b0C). Expect some rain, with an average of 3-4 inches (76-102 mm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 62\u00b0F (17\u00b0C) and lows around 50\u00b0F (10\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some sunshine, but also foggy mornings and cool afternoons.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Notable Weather Features:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. The fog often burns off by midday, but it can make mornings and evenings cool and chilly.\\n* **Wind:** San Francisco can be quite windy, especially during the afternoon and evening hours.\\n* **Microclimates:** San Francisco has many microclimates, meaning that weather conditions can vary significantly depending on the neighborhood or location. For example, the Haight-Ashbury neighborhood tends to be cooler and foggier than the Mission District.\\n\\n**Tips:**\\n\\n* Pack layers, as the temperature can drop significantly at night or when the fog rolls in.\\n* Bring a light jacket or sweater, even in the summer.\\n* If you're planning to visit outdoor attractions, try to schedule your visit during the late morning or early afternoon when the fog has burned off.\\n\\nOverall, San Francisco's climate is mild and pleasant, making it a great destination to visit year-round. Just be prepared for the fog and cooler temperatures!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:344: AssertionError"}, "teardown": {"duration": 0.0002139900000202033, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 324, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.022358537999991768, "outcome": "passed"}, "call": {"duration": 1.6067309509999745, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content='Let me check the current weather in San Francisco for you.\\n\\nThe current weather in San Francisco is mostly cloudy with a temperature of 58\u00b0F (14\u00b0C) and a humidity level of 73%. The wind is blowing at a moderate 12 mph.\\n\\nFor a more detailed forecast, I can check the weather forecast for the next few days. \\n\\nToday: \\n- High: 63\u00b0F (17\u00b0C)\\n- Low: 54\u00b0F (12\u00b0C)\\n- Precipitation: 20% chance of light rain\\n\\nTomorrow: \\n- High: 65\u00b0F (18\u00b0C)\\n- Low: 55\u00b0F (13\u00b0C)\\n- Precipitation: 10% chance of light rain\\n\\nWould you like more information or a forecast for a specific date?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\n +    where ChatCompletionMessage(content='Let me check the current weather in San Francisco for you.\\n\\nThe current weather in San Francisco is mostly cloudy with a temperature of 58\u00b0F (14\u00b0C) and a humidity level of 73%. The wind is blowing at a moderate 12 mph.\\n\\nFor a more detailed forecast, I can check the weather forecast for the next few days. \\n\\nToday: \\n- High: 63\u00b0F (17\u00b0C)\\n- Low: 54\u00b0F (12\u00b0C)\\n- Precipitation: 20% chance of light rain\\n\\nTomorrow: \\n- High: 65\u00b0F (18\u00b0C)\\n- Low: 55\u00b0F (13\u00b0C)\\n- Precipitation: 10% chance of light rain\\n\\nWould you like more information or a forecast for a specific date?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Let me check the current weather in San Francisco for you.\\n\\nThe current weather in San Francisco is mostly cloudy with a temperature of 58\u00b0F (14\u00b0C) and a humidity level of 73%. The wind is blowing at a moderate 12 mph.\\n\\nFor a more detailed forecast, I can check the weather forecast for the next few days. \\n\\nToday: \\n- High: 63\u00b0F (17\u00b0C)\\n- Low: 54\u00b0F (12\u00b0C)\\n- Precipitation: 20% chance of light rain\\n\\nTomorrow: \\n- High: 65\u00b0F (18\u00b0C)\\n- Low: 55\u00b0F (13\u00b0C)\\n- Precipitation: 10% chance of light rain\\n\\nWould you like more information or a forecast for a specific date?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d8088ee0>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content='Let me check the current weather in San Francisco for you.\\n\\nThe current weather in San Francisco is mostly cloudy with a temperature of 58\u00b0F (14\u00b0C) and a humidity level of 73%. The wind is blowing at a moderate 12 mph.\\n\\nFor a more detailed forecast, I can check the weather forecast for the next few days. \\n\\nToday: \\n- High: 63\u00b0F (17\u00b0C)\\n- Low: 54\u00b0F (12\u00b0C)\\n- Precipitation: 20% chance of light rain\\n\\nTomorrow: \\n- High: 65\u00b0F (18\u00b0C)\\n- Low: 55\u00b0F (13\u00b0C)\\n- Precipitation: 10% chance of light rain\\n\\nWould you like more information or a forecast for a specific date?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\nE        +    where ChatCompletionMessage(content='Let me check the current weather in San Francisco for you.\\n\\nThe current weather in San Francisco is mostly cloudy with a temperature of 58\u00b0F (14\u00b0C) and a humidity level of 73%. The wind is blowing at a moderate 12 mph.\\n\\nFor a more detailed forecast, I can check the weather forecast for the next few days. \\n\\nToday: \\n- High: 63\u00b0F (17\u00b0C)\\n- Low: 54\u00b0F (12\u00b0C)\\n- Precipitation: 20% chance of light rain\\n\\nTomorrow: \\n- High: 65\u00b0F (18\u00b0C)\\n- Low: 55\u00b0F (13\u00b0C)\\n- Precipitation: 10% chance of light rain\\n\\nWould you like more information or a forecast for a specific date?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Let me check the current weather in San Francisco for you.\\n\\nThe current weather in San Francisco is mostly cloudy with a temperature of 58\u00b0F (14\u00b0C) and a humidity level of 73%. The wind is blowing at a moderate 12 mph.\\n\\nFor a more detailed forecast, I can check the weather forecast for the next few days. \\n\\nToday: \\n- High: 63\u00b0F (17\u00b0C)\\n- Low: 54\u00b0F (12\u00b0C)\\n- Precipitation: 20% chance of light rain\\n\\nTomorrow: \\n- High: 65\u00b0F (18\u00b0C)\\n- Low: 55\u00b0F (13\u00b0C)\\n- Precipitation: 10% chance of light rain\\n\\nWould you like more information or a forecast for a specific date?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:344: AssertionError"}, "teardown": {"duration": 0.00024425600003041836, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022878979999973126, "outcome": "passed"}, "call": {"duration": 2.2057219959999657, "outcome": "passed"}, "teardown": {"duration": 0.00022289700001465462, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022539928000014697, "outcome": "passed"}, "call": {"duration": 5.949427205999996, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7bf91e0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n>           delta = chunk.choices[0].delta\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:368: IndexError"}, "teardown": {"duration": 0.0002485250000177075, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.023490553999977237, "outcome": "passed"}, "call": {"duration": 1.126775729999963, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e987f0>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n>           delta = chunk.choices[0].delta\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:368: IndexError"}, "teardown": {"duration": 0.0002308019999759381, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02297489899996208, "outcome": "passed"}, "call": {"duration": 0.41931920899997976, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_84ne629ki9jeldzh0vt2ubn0', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_84ne629ki9jeldzh0vt2ubn0', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_84ne629ki9jeldzh0vt2ubn0', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e64100>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_84ne629ki9jeldzh0vt2ubn0', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_84ne629ki9jeldzh0vt2ubn0', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_84ne629ki9jeldzh0vt2ubn0', function=Function(arguments='{\"location\":\"San Francisco, CA\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00022762499997952546, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.025564369999983683, "outcome": "passed"}, "call": {"duration": 0.8601630979999868, "outcome": "passed"}, "teardown": {"duration": 0.00020517400002972863, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "add_product_tool"}, "setup": {"duration": 0.02270523400000002, "outcome": "passed"}, "call": {"duration": 1.6510353440000358, "outcome": "passed"}, "teardown": {"duration": 0.0002285169999822756, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.023014885000009144, "outcome": "passed"}, "call": {"duration": 2.5410528250000084, "outcome": "passed"}, "teardown": {"duration": 0.0002436759999682181, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02287535399995022, "outcome": "passed"}, "call": {"duration": 4.59654469100002, "outcome": "passed"}, "teardown": {"duration": 0.00021956099999442813, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022694765999972333, "outcome": "passed"}, "call": {"duration": 2.0453088499999694, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_ohosyh9tzmd7pldwxv02sb37', function=Function(arguments='{\"location\":\"Sun, NA\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_ohosyh9tzmd7pldwxv02sb37', function=Function(arguments='{\"location\":\"Sun, NA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_ohosyh9tzmd7pldwxv02sb37', function=Function(arguments='{\"location\":\"Sun, NA\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f3e380>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_ohosyh9tzmd7pldwxv02sb37', function=Function(arguments='{\"location\":\"Sun, NA\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_ohosyh9tzmd7pldwxv02sb37', function=Function(arguments='{\"location\":\"Sun, NA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_ohosyh9tzmd7pldwxv02sb37', function=Function(arguments='{\"location\":\"Sun, NA\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00020771800001284646, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022527023000009194, "outcome": "passed"}, "call": {"duration": 0.7901522450000016, "outcome": "passed"}, "teardown": {"duration": 0.0002251410000440046, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02252469700005122, "outcome": "passed"}, "call": {"duration": 1.7488108130000342, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 0\nassert 0 == 1\n +  where 0 = len(([] or []))\n +    where [] = ChatCompletionMessage(content='To fulfill your request, I will provide a response in the required JSON format that calls the `addProduct` function with the specified arguments. Here it is:\\n\\n```json\\n{\\n    \"name\": \"addProduct\",\\n    \"parameters\": {\\n        \"name\": \"Widget\",\\n        \"price\": 19.99,\\n        \"inStock\": true,\\n        \"tags\": [\"new\", \"sale\"]\\n    }\\n}\\n```\\n\\nThis JSON object specifies a function call to `addProduct` with the name \\'Widget\\', a price of 19.99, the product being in stock (`true`), and the tags \\'new\\' and \\'sale\\'. The response from this function call would presumably include the product ID of the newly added product.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e37b20>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 0\nE           assert 0 == 1\nE            +  where 0 = len(([] or []))\nE            +    where [] = ChatCompletionMessage(content='To fulfill your request, I will provide a response in the required JSON format that calls the `addProduct` function with the specified arguments. Here it is:\\n\\n```json\\n{\\n    \"name\": \"addProduct\",\\n    \"parameters\": {\\n        \"name\": \"Widget\",\\n        \"price\": 19.99,\\n        \"inStock\": true,\\n        \"tags\": [\"new\", \"sale\"]\\n    }\\n}\\n```\\n\\nThis JSON object specifies a function call to `addProduct` with the name \\'Widget\\', a price of 19.99, the product being in stock (`true`), and the tags \\'new\\' and \\'sale\\'. The response from this function call would presumably include the product ID of the newly added product.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.0002215239999827645, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022644823999996788, "outcome": "passed"}, "call": {"duration": 1.9996835110000006, "outcome": "passed"}, "teardown": {"duration": 0.00020924099999319878, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022702241000047252, "outcome": "passed"}, "call": {"duration": 1.8653604289999635, "outcome": "passed"}, "teardown": {"duration": 0.00021119499996302693, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022581393999985266, "outcome": "passed"}, "call": {"duration": 0.25668878999999833, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError: Expected one of ['sol'] in content, but got: '{\"name\": null, \"parameters\": null}'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f01d7e420a0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f56290>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: '{\"name\": null, \"parameters\": null}'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f01d7e420a0>)\n\ntests/verifications/openai_api/test_chat_completion.py:462: AssertionError"}, "teardown": {"duration": 0.00020986299995229274, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02268678100000443, "outcome": "passed"}, "call": {"duration": 0.5824769230000015, "outcome": "passed"}, "teardown": {"duration": 0.00020895100004736378, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "add_product_tool"}, "setup": {"duration": 0.022592264000024898, "outcome": "passed"}, "call": {"duration": 0.8793147590000103, "outcome": "passed"}, "teardown": {"duration": 0.00021447099999249986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022498788999996577, "outcome": "passed"}, "call": {"duration": 21.530840692000027, "outcome": "passed"}, "teardown": {"duration": 0.00020808900001156871, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02277906400001939, "outcome": "passed"}, "call": {"duration": 1.1281471690000444, "outcome": "passed"}, "teardown": {"duration": 0.000211284999977579, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022483050999994703, "outcome": "passed"}, "call": {"duration": 0.3055326340000306, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_i0bnrkhbkxs74mxkwyvdaffv', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7f570a0>\nmodel = 'meta-llama/Llama-3.3-70B-Instruct-Turbo', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_i0bnrkhbkxs74mxkwyvdaffv', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00021080399994843901, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022961624999993546, "outcome": "passed"}, "call": {"duration": 2.3328077099999973, "outcome": "passed"}, "teardown": {"duration": 0.00021116499999607186, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "add_product_tool"}, "setup": {"duration": 0.02266370900002812, "outcome": "passed"}, "call": {"duration": 1.2802732630000264, "outcome": "passed"}, "teardown": {"duration": 0.00021083500001850553, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02255200000001878, "outcome": "passed"}, "call": {"duration": 2.887012316000039, "outcome": "passed"}, "teardown": {"duration": 0.0002088200000116558, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02296656300001132, "outcome": "passed"}, "call": {"duration": 1.856094717000019, "outcome": "passed"}, "teardown": {"duration": 0.00021235700000943325, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022672532999990835, "outcome": "passed"}, "call": {"duration": 0.44665481199996293, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e067d0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7eed300>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00021776700003783844, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02235096300000805, "outcome": "passed"}, "call": {"duration": 0.5286462469999833, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81c6590>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7f419f0>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00021283700004914863, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022934031999966464, "outcome": "passed"}, "call": {"duration": 0.9913244349999673, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d82d20e0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7f55000>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00022119399994835476, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022550988000034522, "outcome": "passed"}, "call": {"duration": 0.45870145799995043, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d81c55d0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7f555a0>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00021213600001601662, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02223904400000265, "outcome": "passed"}, "call": {"duration": 0.4025360859999978, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7c283d0>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7c29e10>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.0002154029999701379, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022291752000000997, "outcome": "passed"}, "call": {"duration": 3.9028826659999822, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7bfa9e0>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7fa5540>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.0002323949999549768, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02269111900000098, "outcome": "passed"}, "call": {"duration": 0.49666110100002925, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7c128c0>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d82d1960>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.000224510000009559, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "add_product_tool"}, "setup": {"duration": 0.023100204999991547, "outcome": "passed"}, "call": {"duration": 0.5097359709999978, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7e6fe80>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7f3e560>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.00021467100003746964, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022574050999992323, "outcome": "passed"}, "call": {"duration": 0.43632571699998834, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7c11030>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7f5e830>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.0002491259999715112, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0226625060000174, "outcome": "passed"}, "call": {"duration": 0.703369796000004, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 683, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f01d7eed270>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nstream = <openai.Stream object at 0x7f01d7e36890>\n\n    def _accumulate_streaming_tool_calls(stream):\n        \"\"\"Accumulates tool calls and content from a streaming ChatCompletion response.\"\"\"\n        tool_calls_buffer = {}\n        current_id = None\n        full_content = \"\"  # Initialize content accumulator\n        # Process streaming chunks\n        for chunk in stream:\n>           choice = chunk.choices[0]\nE           IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:683: IndexError"}, "teardown": {"duration": 0.0002107840000462602, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "stream=False"}, "setup": {"duration": 0.023221671999976934, "outcome": "passed"}, "call": {"duration": 0.0001651090000223121, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together based on config.')"}, "teardown": {"duration": 0.00017473600001949308, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "parametrize", "pytestmark", "meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "stream=True"}, "setup": {"duration": 0.025812305000044944, "outcome": "passed"}, "call": {"duration": 0.0001667619999921044, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together based on config.')"}, "teardown": {"duration": 0.00017313300003252152, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "stream=False"}, "setup": {"duration": 0.025397378000036497, "outcome": "passed"}, "call": {"duration": 3.0941751940000017, "outcome": "passed"}, "teardown": {"duration": 0.0002118060000384503, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "lineno": 549, "outcome": "failed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "parametrize", "pytestmark", "meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "stream=True"}, "setup": {"duration": 0.023152171000049293, "outcome": "passed"}, "call": {"duration": 1.999056730999996, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 591, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 591, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]>>\nopenai_client = <openai.OpenAI object at 0x7f01d819df90>\nmodel = 'meta-llama/Llama-4-Scout-17B-16E-Instruct', provider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\nmulti_image_data = ['data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGC...6pH9jaTzNv7vfRRXzubfxj9f8Pv8AkTz/AMX/ALbEz5Ly38lfMk/5Z/u64PxhqEZh+z/6rzvn2UUV5EvgPuzy/wAc6p5dt5ccibJpNkkdFFFec27mZ//Z']\nstream = True\n\n    @pytest.mark.parametrize(\"stream\", [False, True], ids=[\"stream=False\", \"stream=True\"])\n    def test_chat_multi_turn_multiple_images(\n        request, openai_client, model, provider, verification_config, multi_image_data, stream\n    ):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages_turn1 = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[0],\n                        },\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[1],\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"What furniture is in the first image that is not in the second image?\",\n                    },\n                ],\n            },\n        ]\n    \n        # First API call\n        response1 = openai_client.chat.completions.create(\n            model=model,\n            messages=messages_turn1,\n            stream=stream,\n        )\n        if stream:\n            message_content1 = \"\"\n            for chunk in response1:\n>               message_content1 += chunk.choices[0].delta.content or \"\"\nE               IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:591: IndexError"}, "teardown": {"duration": 0.00022761599996101722, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "stream=False"}, "setup": {"duration": 0.023231218999967496, "outcome": "passed"}, "call": {"duration": 3.057669354999973, "outcome": "passed"}, "teardown": {"duration": 0.00021372999998447995, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "lineno": 549, "outcome": "failed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "parametrize", "pytestmark", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "stream=True"}, "setup": {"duration": 0.023376321000000644, "outcome": "passed"}, "call": {"duration": 1.752342280999983, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 591, "message": "IndexError: list index out of range"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 591, "message": "IndexError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_multi_turn_multiple_images[meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]>>\nopenai_client = <openai.OpenAI object at 0x7f01d822f940>\nmodel = 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\nmulti_image_data = ['data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGC...6pH9jaTzNv7vfRRXzubfxj9f8Pv8AkTz/AMX/ALbEz5Ly38lfMk/5Z/u64PxhqEZh+z/6rzvn2UUV5EvgPuzy/wAc6p5dt5ccibJpNkkdFFFec27mZ//Z']\nstream = True\n\n    @pytest.mark.parametrize(\"stream\", [False, True], ids=[\"stream=False\", \"stream=True\"])\n    def test_chat_multi_turn_multiple_images(\n        request, openai_client, model, provider, verification_config, multi_image_data, stream\n    ):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages_turn1 = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[0],\n                        },\n                    },\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": multi_image_data[1],\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"What furniture is in the first image that is not in the second image?\",\n                    },\n                ],\n            },\n        ]\n    \n        # First API call\n        response1 = openai_client.chat.completions.create(\n            model=model,\n            messages=messages_turn1,\n            stream=stream,\n        )\n        if stream:\n            message_content1 = \"\"\n            for chunk in response1:\n>               message_content1 += chunk.choices[0].delta.content or \"\"\nE               IndexError: list index out of range\n\ntests/verifications/openai_api/test_chat_completion.py:591: IndexError"}, "teardown": {"duration": 0.002305460000002313, "outcome": "passed"}}]}