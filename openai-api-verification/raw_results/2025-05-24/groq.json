{"created": 1748052659.0042188, "duration": 33.686766147613525, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 95, "failed": 15, "skipped": 4, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=True]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.11508424299995568, "outcome": "passed"}, "call": {"duration": 0.19539047299997492, "outcome": "passed"}, "teardown": {"duration": 0.00014363799994043802, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.0003333810000185622, "outcome": "passed"}, "call": {"duration": 0.3163151090000156, "outcome": "passed"}, "teardown": {"duration": 0.0001324370000475028, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.0004039020000163873, "outcome": "passed"}, "call": {"duration": 0.15685383300001376, "outcome": "passed"}, "teardown": {"duration": 0.00012630599997009995, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.00039235099995948985, "outcome": "passed"}, "call": {"duration": 0.15935421699998642, "outcome": "passed"}, "teardown": {"duration": 0.00012468199997783813, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.0003738860000339628, "outcome": "passed"}, "call": {"duration": 0.15704242200001772, "outcome": "passed"}, "teardown": {"duration": 0.00012158599997746933, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.00039737999998123996, "outcome": "passed"}, "call": {"duration": 0.11003186499999629, "outcome": "passed"}, "teardown": {"duration": 0.000124561999996331, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.00039242100001501967, "outcome": "passed"}, "call": {"duration": 0.3272370710000132, "outcome": "passed"}, "teardown": {"duration": 0.00012626499994894402, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.00037061000000448985, "outcome": "passed"}, "call": {"duration": 0.29271672200002286, "outcome": "passed"}, "teardown": {"duration": 0.0001412930000697088, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.00038886499999080115, "outcome": "passed"}, "call": {"duration": 0.2111141379999708, "outcome": "passed"}, "teardown": {"duration": 0.00012053399996148073, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.00037571999996544037, "outcome": "passed"}, "call": {"duration": 0.132497748999981, "outcome": "passed"}, "teardown": {"duration": 0.0001177000000325279, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.00039115899994612846, "outcome": "passed"}, "call": {"duration": 0.04280984599995463, "outcome": "passed"}, "teardown": {"duration": 0.0001326169999629201, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.00037093100002039137, "outcome": "passed"}, "call": {"duration": 0.09791840100001536, "outcome": "passed"}, "teardown": {"duration": 0.0001424860000724948, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.000399073000039607, "outcome": "passed"}, "call": {"duration": 0.028452676000028987, "outcome": "passed"}, "teardown": {"duration": 0.0001515320000180509, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0003794870000319861, "outcome": "passed"}, "call": {"duration": 0.03097256799992465, "outcome": "passed"}, "teardown": {"duration": 0.0001551190000554925, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.0002786190000279021, "outcome": "passed"}, "call": {"duration": 0.0315641610000057, "outcome": "passed"}, "teardown": {"duration": 0.00014058099998237594, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0003016210000623687, "outcome": "passed"}, "call": {"duration": 0.2764969759999758, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00016010799993182445, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.00033281000003171357, "outcome": "passed"}, "call": {"duration": 0.0353325199999972, "outcome": "passed"}, "teardown": {"duration": 0.00014384799999334064, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.00029098200002408703, "outcome": "passed"}, "call": {"duration": 0.028996839000001273, "outcome": "passed"}, "teardown": {"duration": 0.00014094200003000878, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.00029274500002429704, "outcome": "passed"}, "call": {"duration": 0.038529130000029, "outcome": "passed"}, "teardown": {"duration": 0.00013976000002458022, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.00031089999993128004, "outcome": "passed"}, "call": {"duration": 0.027615235999974175, "outcome": "passed"}, "teardown": {"duration": 0.00012295900000935944, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0002873849999787126, "outcome": "passed"}, "call": {"duration": 0.15922424600000795, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0001424260000248978, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.0002927749999344087, "outcome": "passed"}, "call": {"duration": 0.029857572999958393, "outcome": "passed"}, "teardown": {"duration": 0.00011787899995852058, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.0002913920000082726, "outcome": "passed"}, "call": {"duration": 0.030232368000042698, "outcome": "passed"}, "teardown": {"duration": 0.00014175399996929627, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0002882359999603068, "outcome": "passed"}, "call": {"duration": 0.02905944700000873, "outcome": "passed"}, "teardown": {"duration": 0.00013275700007397973, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.0002789789999724235, "outcome": "passed"}, "call": {"duration": 0.035346539000101984, "outcome": "passed"}, "teardown": {"duration": 0.0001204149999693982, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.00030883499994160957, "outcome": "passed"}, "call": {"duration": 0.21149738500002968, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00014947900001516246, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.000338179999971544, "outcome": "passed"}, "call": {"duration": 0.028808118000029026, "outcome": "passed"}, "teardown": {"duration": 0.00014420900004097348, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.0002881870000237541, "outcome": "passed"}, "call": {"duration": 0.029180383000039, "outcome": "passed"}, "teardown": {"duration": 0.00011618600001384038, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.00028384899997035973, "outcome": "passed"}, "call": {"duration": 0.029845837000038955, "outcome": "passed"}, "teardown": {"duration": 0.00012878000006821821, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.0003171310000880112, "outcome": "passed"}, "call": {"duration": 0.02895504200000687, "outcome": "passed"}, "teardown": {"duration": 0.0001255040000387453, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0002938869999979943, "outcome": "passed"}, "call": {"duration": 0.27135252199991555, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00012412099999892234, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.0002924240000083955, "outcome": "passed"}, "call": {"duration": 0.026388192000013078, "outcome": "passed"}, "teardown": {"duration": 0.00012630499998067535, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.00031275300000288553, "outcome": "passed"}, "call": {"duration": 0.029574243999945793, "outcome": "passed"}, "teardown": {"duration": 0.00017490599998382095, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.00032172900000659865, "outcome": "passed"}, "call": {"duration": 0.029066356999919662, "outcome": "passed"}, "teardown": {"duration": 0.00014621299999362236, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.00032035599997470854, "outcome": "passed"}, "call": {"duration": 0.03072304499994516, "outcome": "passed"}, "teardown": {"duration": 0.00011763899999550631, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.00030059999994591635, "outcome": "passed"}, "call": {"duration": 0.13202305799995884, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00012490299991441134, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.00030937700000777113, "outcome": "passed"}, "call": {"duration": 0.029023569999935717, "outcome": "passed"}, "teardown": {"duration": 0.0001243609999619366, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.00029949800000395044, "outcome": "passed"}, "call": {"duration": 0.02806606699994063, "outcome": "passed"}, "teardown": {"duration": 0.00012360999994598387, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0003022530000862389, "outcome": "passed"}, "call": {"duration": 0.03228448300001219, "outcome": "passed"}, "teardown": {"duration": 0.00012111599994568678, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.00029875699999593053, "outcome": "passed"}, "call": {"duration": 0.02921546599998237, "outcome": "passed"}, "teardown": {"duration": 0.00012271900004634517, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0003165389999821855, "outcome": "passed"}, "call": {"duration": 0.04268581499991342, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00015157300003920682, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.0003808099999105252, "outcome": "passed"}, "call": {"duration": 0.03037482499996713, "outcome": "passed"}, "teardown": {"duration": 0.0001434179999932894, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama-3.3-70b-versatile-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.00037345600003391155, "outcome": "passed"}, "call": {"duration": 0.00014061199999559904, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.00011637600005087734, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00035086400009731733, "outcome": "passed"}, "call": {"duration": 0.9635858469999903, "outcome": "passed"}, "teardown": {"duration": 0.0001761490000262711, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00033699799996611546, "outcome": "passed"}, "call": {"duration": 1.2077883119999342, "outcome": "passed"}, "teardown": {"duration": 0.0001547089999576201, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama-3.3-70b-versatile-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0003935949999913646, "outcome": "passed"}, "call": {"duration": 0.00015253500009748677, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.00011647700000594341, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0003485109999701308, "outcome": "passed"}, "call": {"duration": 0.8486515619999864, "outcome": "passed"}, "teardown": {"duration": 0.0003313390000130312, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00041074699993259856, "outcome": "passed"}, "call": {"duration": 1.1863553580000143, "outcome": "passed"}, "teardown": {"duration": 0.00016675100005159038, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "lineno": 176, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.0004408810000313679, "outcome": "passed"}, "call": {"duration": 0.027978193000080864, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 187, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fe4d0a8ead0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00012935100005506683, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]", "lineno": 176, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.00036870700000690704, "outcome": "passed"}, "call": {"duration": 0.03155379600002561, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 187, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fe4d0a8ead0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00017647900006068085, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.0003870609999694352, "outcome": "passed"}, "call": {"duration": 0.22745517999999265, "outcome": "passed"}, "teardown": {"duration": 0.0001342699999895558, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.0004032310000638972, "outcome": "passed"}, "call": {"duration": 0.5259789259999934, "outcome": "passed"}, "teardown": {"duration": 0.00013772700003755745, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.00038389500002722343, "outcome": "passed"}, "call": {"duration": 0.1383221800000456, "outcome": "passed"}, "teardown": {"duration": 0.00012667600003624102, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.0003611319999663465, "outcome": "passed"}, "call": {"duration": 0.3366797730000144, "outcome": "passed"}, "teardown": {"duration": 0.00017283099998621765, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "lineno": 199, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.00039913299997351714, "outcome": "passed"}, "call": {"duration": 0.028557388999956856, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 210, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:210: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fe4d0a8ead0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00012654599993311422, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]", "lineno": 199, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.00036682299992207845, "outcome": "passed"}, "call": {"duration": 0.031266286000004584, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 210, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:210: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fe4d0a8ead0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': 'This model does not support response format `json_schema`', 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00012564400003611809, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.00036874700003863836, "outcome": "passed"}, "call": {"duration": 0.16979831499998, "outcome": "passed"}, "teardown": {"duration": 0.00013290800006870995, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.00033259999997881096, "outcome": "passed"}, "call": {"duration": 0.6999012369999491, "outcome": "passed"}, "teardown": {"duration": 0.00014260600005400192, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.00034912099999928614, "outcome": "passed"}, "call": {"duration": 0.0704451199999312, "outcome": "passed"}, "teardown": {"duration": 0.00014423799996166053, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.00030199299999367213, "outcome": "passed"}, "call": {"duration": 0.33166654100000414, "outcome": "passed"}, "teardown": {"duration": 0.00013563300001351308, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0003410250000115411, "outcome": "passed"}, "call": {"duration": 0.17743950600004155, "outcome": "passed"}, "teardown": {"duration": 0.00012630499998067535, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00031054900000526686, "outcome": "passed"}, "call": {"duration": 0.21155995200001598, "outcome": "passed"}, "teardown": {"duration": 0.00012380099997244542, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00032657799999924464, "outcome": "passed"}, "call": {"duration": 0.1929963669999779, "outcome": "passed"}, "teardown": {"duration": 0.00012201700008063199, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0003218890000198371, "outcome": "passed"}, "call": {"duration": 0.18106974700003775, "outcome": "passed"}, "teardown": {"duration": 0.00012687599996752397, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00032622800006265606, "outcome": "passed"}, "call": {"duration": 0.14048500300009437, "outcome": "passed"}, "teardown": {"duration": 0.00012916100001802988, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00030604000005496346, "outcome": "passed"}, "call": {"duration": 0.08540809399994487, "outcome": "passed"}, "teardown": {"duration": 0.00012377099994864693, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0003305750000208718, "outcome": "passed"}, "call": {"duration": 0.1700797869999633, "outcome": "passed"}, "teardown": {"duration": 0.00012424099998042948, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0003300650000710448, "outcome": "passed"}, "call": {"duration": 0.15258824800002913, "outcome": "passed"}, "teardown": {"duration": 0.00014338699998006632, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0003050779999966835, "outcome": "passed"}, "call": {"duration": 0.08043405300008999, "outcome": "passed"}, "teardown": {"duration": 0.0001222879999431825, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.00033438299999488663, "outcome": "passed"}, "call": {"duration": 0.1722001979999277, "outcome": "passed"}, "teardown": {"duration": 0.00012728600006539637, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0003193940000301154, "outcome": "passed"}, "call": {"duration": 0.18088399999999183, "outcome": "passed"}, "teardown": {"duration": 0.00011719799999809766, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0003341629999340512, "outcome": "passed"}, "call": {"duration": 0.07924182300007487, "outcome": "passed"}, "teardown": {"duration": 0.00012521399992238003, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0003285010000126931, "outcome": "passed"}, "call": {"duration": 0.5748608369999602, "outcome": "passed"}, "teardown": {"duration": 0.0001325869999391216, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00033734799990270403, "outcome": "passed"}, "call": {"duration": 0.3020838069999172, "outcome": "passed"}, "teardown": {"duration": 0.00012805899996237713, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0003281120000337978, "outcome": "passed"}, "call": {"duration": 0.1252646550000236, "outcome": "passed"}, "teardown": {"duration": 0.00012788899994120584, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.0003125530000716026, "outcome": "passed"}, "call": {"duration": 0.836531358000002, "outcome": "passed"}, "teardown": {"duration": 0.00018714899999849877, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.00043082299998786766, "outcome": "passed"}, "call": {"duration": 0.3331235870000455, "outcome": "passed"}, "teardown": {"duration": 0.0002133579999963331, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.0004150050000362171, "outcome": "passed"}, "call": {"duration": 0.11874043099999199, "outcome": "passed"}, "teardown": {"duration": 0.00018566599999303435, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.00039620899997316883, "outcome": "passed"}, "call": {"duration": 0.742390938000085, "outcome": "passed"}, "teardown": {"duration": 0.00015922700004011858, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0004170480000311727, "outcome": "passed"}, "call": {"duration": 0.3875091079999038, "outcome": "passed"}, "teardown": {"duration": 0.00012836999997034582, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.000357926999981828, "outcome": "passed"}, "call": {"duration": 0.4758615709999958, "outcome": "passed"}, "teardown": {"duration": 0.0001356029999897146, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00036912799998845003, "outcome": "passed"}, "call": {"duration": 1.0154016840000395, "outcome": "passed"}, "teardown": {"duration": 0.00016064999999798601, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0003417159999798969, "outcome": "passed"}, "call": {"duration": 0.9741421499999205, "outcome": "passed"}, "teardown": {"duration": 0.0001508610000655608, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.0002744110000776345, "outcome": "passed"}, "call": {"duration": 0.2666566460000013, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=brave_search>{\"query\": \"Latin name of the Sun\"}</function>'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 416, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:416: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fe4d0a8ead0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<function=brave_search>{\"query\": \"Latin name of the Sun\"}</function>'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00019023499999093474, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.00028933900000538415, "outcome": "passed"}, "call": {"duration": 0.27886169700002483, "outcome": "passed"}, "teardown": {"duration": 0.00012714599995433673, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.00026085599995440134, "outcome": "passed"}, "call": {"duration": 0.5035455920000231, "outcome": "passed"}, "teardown": {"duration": 0.00014071200007492735, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00026770900001338305, "outcome": "passed"}, "call": {"duration": 0.6650300430000016, "outcome": "passed"}, "teardown": {"duration": 0.00016461700010950153, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.00037824399998953595, "outcome": "passed"}, "call": {"duration": 0.691722307999953, "outcome": "passed"}, "teardown": {"duration": 0.00016739099999085738, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.0004043030000957515, "outcome": "passed"}, "call": {"duration": 0.28063708999991377, "outcome": "passed"}, "teardown": {"duration": 0.00015973699998994562, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.00037806399996043183, "outcome": "passed"}, "call": {"duration": 0.1460674669999662, "outcome": "passed"}, "teardown": {"duration": 0.000182730000005904, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.00038281299998743634, "outcome": "passed"}, "call": {"duration": 0.19358255099996313, "outcome": "passed"}, "teardown": {"duration": 0.00015960700000050565, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00038491700001941354, "outcome": "passed"}, "call": {"duration": 0.35339045099999566, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_9j98', function=Function(arguments='{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_9j98', function=Function(arguments='{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9j98', function=Function(arguments='{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_9j98', function=Function(arguments='{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_9j98', function=Function(arguments='{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9j98', function=Function(arguments='{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.0001702869999462564, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.00040473399997154047, "outcome": "passed"}, "call": {"duration": 0.3521264150000434, "outcome": "passed"}, "teardown": {"duration": 0.00016311299998506001, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.00038432500002727465, "outcome": "passed"}, "call": {"duration": 0.5399614189999511, "outcome": "passed"}, "teardown": {"duration": 0.0001983099999733895, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0003861389999428866, "outcome": "passed"}, "call": {"duration": 0.15568917099994906, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 91, "message": "openai.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 91, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fe4d0623880>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n                    raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\n    \n                yield process_data(data=data, cast_to=cast_to, response=response)\n    \n            else:\n                data = sse.json()\n    \n                if sse.event == \"error\" and is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:91: APIError"}, "teardown": {"duration": 0.0001724910000575619, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.0003767819999893618, "outcome": "passed"}, "call": {"duration": 0.8204811640000571, "outcome": "passed"}, "teardown": {"duration": 0.00016526800004612596, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00039983399994980573, "outcome": "passed"}, "call": {"duration": 0.8638217380000697, "outcome": "passed"}, "teardown": {"duration": 0.00016000800007986982, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0004063470000801317, "outcome": "passed"}, "call": {"duration": 0.7117939739999883, "outcome": "passed"}, "teardown": {"duration": 0.0001568519999182172, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.00042718600002444873, "outcome": "passed"}, "call": {"duration": 0.1469749730000558, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 91, "message": "openai.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 91, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fe4d0849300>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n                    raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\n    \n                yield process_data(data=data, cast_to=cast_to, response=response)\n    \n            else:\n                data = sse.json()\n    \n                if sse.event == \"error\" and is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:91: APIError"}, "teardown": {"duration": 0.00012558400010220794, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.00037908599995262193, "outcome": "passed"}, "call": {"duration": 0.32192777400007344, "outcome": "passed"}, "teardown": {"duration": 0.00013770700002169178, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.0003912889999355684, "outcome": "passed"}, "call": {"duration": 0.3772178359999998, "outcome": "passed"}, "teardown": {"duration": 0.00014852599997539073, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.0003598099999635451, "outcome": "passed"}, "call": {"duration": 0.6348396729999877, "outcome": "passed"}, "teardown": {"duration": 0.00021474000004673144, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0003690680000545399, "outcome": "passed"}, "call": {"duration": 0.5876372469999751, "outcome": "passed"}, "teardown": {"duration": 0.00015585999994982558, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.00037110200003098726, "outcome": "passed"}, "call": {"duration": 0.18880425800000467, "outcome": "passed"}, "teardown": {"duration": 0.00012421199994605558, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0003541590000395445, "outcome": "passed"}, "call": {"duration": 0.13333587699992222, "outcome": "passed"}, "teardown": {"duration": 0.00012659599997277837, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.0003541600000289691, "outcome": "passed"}, "call": {"duration": 0.19127256199999465, "outcome": "passed"}, "teardown": {"duration": 0.0001231490000463964, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00039139900002282957, "outcome": "passed"}, "call": {"duration": 0.3315219009999737, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', 'name': 'get_event'}, 'id': 'call_y0sc', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fe4d0a8ead0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"date\":\"2025-03-03\",\"time\":\"10:00\"}', 'name': 'get_event'}, 'id': 'call_y0sc', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00012335899998561217, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0003559830000767761, "outcome": "passed"}, "call": {"duration": 0.33088129899999785, "outcome": "passed"}, "teardown": {"duration": 0.00019452200001524034, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=False]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "stream=False"}, "setup": {"duration": 0.0010647020000078555, "outcome": "passed"}, "call": {"duration": 0.00019748700003674458, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.00011749800000870891, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=True]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "stream=True"}, "setup": {"duration": 0.0009264950000442695, "outcome": "passed"}, "call": {"duration": 0.00013474100001076295, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.0003019119999407849, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=False]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.0009278379999386743, "outcome": "passed"}, "call": {"duration": 0.8948841019999918, "outcome": "passed"}, "teardown": {"duration": 0.0001773400000502079, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=True]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.0010282150000193724, "outcome": "passed"}, "call": {"duration": 0.9104449959999101, "outcome": "passed"}, "teardown": {"duration": 0.00016830300000947318, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=False]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.0010412999999971362, "outcome": "passed"}, "call": {"duration": 1.2821852820000004, "outcome": "passed"}, "teardown": {"duration": 0.00016503699998793309, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=True]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.001041500000042106, "outcome": "passed"}, "call": {"duration": 0.7602209330000278, "outcome": "passed"}, "teardown": {"duration": 0.0003961580000577669, "outcome": "passed"}}]}