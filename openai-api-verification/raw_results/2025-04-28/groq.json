{"created": 1745806126.842583, "duration": 74.9472143650055, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 72, "failed": 38, "skipped": 4, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=True]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=True]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=True]", "type": "Function", "lineno": 599}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.05814959999997882, "outcome": "passed"}, "call": {"duration": 0.49883971700000984, "outcome": "passed"}, "teardown": {"duration": 0.0002831990000231599, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama-3.3-70b-versatile-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.023906945999954132, "outcome": "passed"}, "call": {"duration": 0.42932321699998965, "outcome": "passed"}, "teardown": {"duration": 0.00020874000000503656, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.0232856149999634, "outcome": "passed"}, "call": {"duration": 0.4834915239999873, "outcome": "passed"}, "teardown": {"duration": 0.00020809900001950155, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.02310212100002218, "outcome": "passed"}, "call": {"duration": 0.3620527830000242, "outcome": "passed"}, "teardown": {"duration": 0.00026957399995808373, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 96, "outcome": "failed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.024292967999997472, "outcome": "passed"}, "call": {"duration": 0.07595882099997198, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 107, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]>>\nopenai_client = <openai.OpenAI object at 0x7faa80ab4a30>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'earth', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}]}, 'output': 'Earth'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:107: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80ab4a30>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.0002721379999570672, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 96, "outcome": "failed", "keywords": ["test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.02343129600001248, "outcome": "passed"}, "call": {"duration": 0.0603896679999707, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 107, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a99ff0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'saturn', 'input': {'messages': [{'content': 'Which planet has rings around it with a name starting with letter S?', 'role': 'user'}]}, 'output': 'Saturn'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:107: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80a99ff0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00034294099998533056, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama-3.3-70b-versatile-earth]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "earth"}, "setup": {"duration": 0.023224380000044675, "outcome": "passed"}, "call": {"duration": 0.31082997500004694, "outcome": "passed"}, "teardown": {"duration": 0.00021628400003237402, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama-3.3-70b-versatile-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama-3.3-70b-versatile-saturn]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "saturn"}, "setup": {"duration": 0.022515735000013137, "outcome": "passed"}, "call": {"duration": 0.5100503929999718, "outcome": "passed"}, "teardown": {"duration": 0.000283450000040375, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "earth"}, "setup": {"duration": 0.02319302100005416, "outcome": "passed"}, "call": {"duration": 0.3853154540000219, "outcome": "passed"}, "teardown": {"duration": 0.00026377299997193404, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-scout-17b-16e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.022803403000011713, "outcome": "passed"}, "call": {"duration": 0.3090848380000466, "outcome": "passed"}, "teardown": {"duration": 0.0003113719999987552, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "lineno": 115, "outcome": "failed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "earth"}, "setup": {"duration": 0.023008887000003142, "outcome": "passed"}, "call": {"duration": 0.10679795999999442, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 126, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-earth]>>\nopenai_client = <openai.OpenAI object at 0x7faa807e9720>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'earth', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}]}, 'output': 'Earth'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:126: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa807e9720>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.000290422999967177, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "lineno": 115, "outcome": "failed", "keywords": ["test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "saturn"}, "setup": {"duration": 0.02298423100000946, "outcome": "passed"}, "call": {"duration": 0.12459119900000815, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 126, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_basic[meta-llama/llama-4-maverick-17b-128e-instruct-saturn]>>\nopenai_client = <openai.OpenAI object at 0x7faa8092b790>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'saturn', 'input': {'messages': [{'content': 'Which planet has rings around it with a name starting with letter S?', 'role': 'user'}]}, 'output': 'Saturn'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_basic\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_basic(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:126: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa8092b790>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00034859099997675, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.023216945999990912, "outcome": "passed"}, "call": {"duration": 0.06861432800002376, "outcome": "passed"}, "teardown": {"duration": 0.00027650699996684125, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0230918330000236, "outcome": "passed"}, "call": {"duration": 0.07574237099998982, "outcome": "passed"}, "teardown": {"duration": 0.0002188089999890508, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022758719999956156, "outcome": "passed"}, "call": {"duration": 0.06844633700001168, "outcome": "passed"}, "teardown": {"duration": 0.0002859749999970518, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 139, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023348432000034336, "outcome": "passed"}, "call": {"duration": 0.33652010900004825, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 150, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 150, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7faa80ac4c10>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:150: Failed"}, "teardown": {"duration": 0.00034584600001608123, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023473475999992388, "outcome": "passed"}, "call": {"duration": 0.10423056700000188, "outcome": "passed"}, "teardown": {"duration": 0.0002542249999919477, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.023104225000054157, "outcome": "passed"}, "call": {"duration": 0.05287413299998889, "outcome": "passed"}, "teardown": {"duration": 0.00035151699995594754, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02356032900001992, "outcome": "passed"}, "call": {"duration": 0.09458411899998964, "outcome": "passed"}, "teardown": {"duration": 0.0002654670000197257, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02346419899998864, "outcome": "passed"}, "call": {"duration": 0.058733050999990155, "outcome": "passed"}, "teardown": {"duration": 0.00021405000001095686, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 139, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023448217999998633, "outcome": "passed"}, "call": {"duration": 0.38196321499998476, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 150, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 150, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a9b400>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:150: Failed"}, "teardown": {"duration": 0.000278250000008029, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023326099999962935, "outcome": "passed"}, "call": {"duration": 0.1376718189999906, "outcome": "passed"}, "teardown": {"duration": 0.00029152499996598635, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.023463106000008338, "outcome": "passed"}, "call": {"duration": 0.07427118999999038, "outcome": "passed"}, "teardown": {"duration": 0.00023556000002145083, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.023357869000051323, "outcome": "passed"}, "call": {"duration": 0.10999332299996922, "outcome": "passed"}, "teardown": {"duration": 0.00030046199998423617, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.023553486000025714, "outcome": "passed"}, "call": {"duration": 0.06745088600001736, "outcome": "passed"}, "teardown": {"duration": 0.0002839710000444029, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 139, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023028954999972484, "outcome": "passed"}, "call": {"duration": 0.0923539859999778, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 158, "message": "assert 400 == 503\n +  where 503 = InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\").status_code\n +    where InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") tblen=6>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 158, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7faa807be200>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       assert 400 == 503\nE        +  where 503 = InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\").status_code\nE        +    where InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") tblen=6>.value\n\ntests/verifications/openai_api/test_chat_completion.py:158: AssertionError"}, "teardown": {"duration": 0.00026916299998447357, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023226624000017182, "outcome": "passed"}, "call": {"duration": 0.1152542639999865, "outcome": "passed"}, "teardown": {"duration": 0.00028160599998727776, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_missing]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_missing"}, "setup": {"duration": 0.022805988000015986, "outcome": "passed"}, "call": {"duration": 0.05342131000003292, "outcome": "passed"}, "teardown": {"duration": 0.00023314599997092955, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-messages_role_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02539389499997924, "outcome": "passed"}, "call": {"duration": 0.06414180300004091, "outcome": "passed"}, "teardown": {"duration": 0.00023637200001758174, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.023615260999974907, "outcome": "passed"}, "call": {"duration": 0.12301741999999649, "outcome": "passed"}, "teardown": {"duration": 0.00027878100001998973, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023453559000017776, "outcome": "passed"}, "call": {"duration": 0.30201793800000587, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 171, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 171, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama-3.3-70b-versatile-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7faa80ac4be0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:171: Failed"}, "teardown": {"duration": 0.00029397899999139554, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama-3.3-70b-versatile-tools_type_invalid]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023634345999994366, "outcome": "passed"}, "call": {"duration": 0.05883472300001813, "outcome": "passed"}, "teardown": {"duration": 0.00022940899998502573, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022893931999988126, "outcome": "passed"}, "call": {"duration": 0.06195514900002763, "outcome": "passed"}, "teardown": {"duration": 0.00022161399999731657, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02305624699999953, "outcome": "passed"}, "call": {"duration": 0.06032545799996569, "outcome": "passed"}, "teardown": {"duration": 0.00024263300002758115, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.023123671000007562, "outcome": "passed"}, "call": {"duration": 0.05220996300005254, "outcome": "passed"}, "teardown": {"duration": 0.0002211640000382431, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02288984500000879, "outcome": "passed"}, "call": {"duration": 0.3058750029999828, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 171, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 171, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7faa80ab6ef0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:171: Failed"}, "teardown": {"duration": 0.00031420699997397605, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023200975999998263, "outcome": "passed"}, "call": {"duration": 0.06312792099998887, "outcome": "passed"}, "teardown": {"duration": 0.00023847599999271551, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022653423000008388, "outcome": "passed"}, "call": {"duration": 0.060201276000043435, "outcome": "passed"}, "teardown": {"duration": 0.00022126400000388458, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022782163999977456, "outcome": "passed"}, "call": {"duration": 0.05985279500004026, "outcome": "passed"}, "teardown": {"duration": 0.0002314019999971606, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02300118300001941, "outcome": "passed"}, "call": {"duration": 0.06156956199998831, "outcome": "passed"}, "teardown": {"duration": 0.00024980699998877753, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02311506600000257, "outcome": "passed"}, "call": {"duration": 0.08263366900001756, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 181, "message": "assert '400' in \"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\"\n +  where '400' = str(400)\n +  and   \"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\" = InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\").message\n +    where InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") tblen=6>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 181, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7faa80925660>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       assert '400' in \"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\"\nE        +  where '400' = str(400)\nE        +  and   \"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\" = InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\").message\nE        +    where InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\") tblen=6>.value\n\ntests/verifications/openai_api/test_chat_completion.py:181: AssertionError"}, "teardown": {"duration": 0.00026918399998976383, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.023283931000037228, "outcome": "passed"}, "call": {"duration": 0.10136786399999664, "outcome": "passed"}, "teardown": {"duration": 0.00025948500001504726, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama-3.3-70b-versatile-case0]", "lineno": 183, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022992746999989322, "outcome": "passed"}, "call": {"duration": 0.0002380050000283518, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 192, 'Skipped: Skipping test_chat_non_streaming_image for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.00041829200000620403, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 183, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023419204999981957, "outcome": "passed"}, "call": {"duration": 3.0077103939999574, "outcome": "passed"}, "teardown": {"duration": 0.00024070999995728926, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 183, "outcome": "failed", "keywords": ["test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023561287000006814, "outcome": "passed"}, "call": {"duration": 0.3952210460000174, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 194, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a13070>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': [{'text': 'What is in this image?', 'type': 'text'}, {'image_url': {...}, 'type': 'image_url'}], 'role': 'user'}]}, 'output': 'llama'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_image\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_image(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:194: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80a13070>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00026816100000814913, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama-3.3-70b-versatile-case0]", "lineno": 202, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.023590971999965404, "outcome": "passed"}, "call": {"duration": 0.00023257400005149975, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 211, 'Skipped: Skipping test_chat_streaming_image for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.0001754080000182512, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 202, "outcome": "passed", "keywords": ["test_chat_streaming_image[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.024441211000009844, "outcome": "passed"}, "call": {"duration": 3.3605216420000374, "outcome": "passed"}, "teardown": {"duration": 0.0002518409999652249, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 202, "outcome": "failed", "keywords": ["test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.024479479999968135, "outcome": "passed"}, "call": {"duration": 0.38020689300003596, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 213, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_image[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa807ea620>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': [{'text': 'What is in this image?', 'type': 'text'}, {'image_url': {...}, 'type': 'image_url'}], 'role': 'user'}]}, 'output': 'llama'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_image\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_image(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:213: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa807ea620>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.0002772180000079061, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "lineno": 226, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.023490543999969304, "outcome": "passed"}, "call": {"duration": 0.05820674000000281, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 237, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7faa80926800>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80926800>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00027752899995903135, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]", "lineno": 226, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.023305376999985583, "outcome": "passed"}, "call": {"duration": 0.05433352400001468, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 237, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[llama-3.3-70b-versatile-math]>>\nopenai_client = <openai.OpenAI object at 0x7faa80931a50>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80931a50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002652659999853313, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 226, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.022158814999954757, "outcome": "passed"}, "call": {"duration": 0.061596043999998074, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 237, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7faa80ac5510>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80ac5510>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002978669999720296, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "lineno": 226, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.02348386100004518, "outcome": "passed"}, "call": {"duration": 0.0584178049999764, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 237, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x7faa80933e50>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80933e50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002783809999868936, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 226, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.022667845999990277, "outcome": "passed"}, "call": {"duration": 0.060776545000010174, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 237, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a98520>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80a98520>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0003401650000114387, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "lineno": 226, "outcome": "failed", "keywords": ["test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.0233601799999974, "outcome": "passed"}, "call": {"duration": 0.052731945000005, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 237, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x7faa80703ac0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:237: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80703ac0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00028543399997715824, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "lineno": 249, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "calendar"}, "setup": {"duration": 0.023312700999952085, "outcome": "passed"}, "call": {"duration": 0.10489494600000171, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 260, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[llama-3.3-70b-versatile-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7faa807700a0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:260: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa807700a0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002798929999698885, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]", "lineno": 249, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "math"}, "setup": {"duration": 0.022945475000028637, "outcome": "passed"}, "call": {"duration": 0.12284108199997945, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 260, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[llama-3.3-70b-versatile-math]>>\nopenai_client = <openai.OpenAI object at 0x7faa8077f850>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:260: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa8077f850>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00028677599999582526, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "lineno": 249, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.02287243899996838, "outcome": "passed"}, "call": {"duration": 0.12103050799998982, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 260, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7faa809a1180>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:260: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa809a1180>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00027899100001604893, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "lineno": 249, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "math"}, "setup": {"duration": 0.023302843000010398, "outcome": "passed"}, "call": {"duration": 0.09014087099996004, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 260, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/llama-4-scout-17b-16e-instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x7faa807a7100>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:260: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa807a7100>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00035467300000391333, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "lineno": 249, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "calendar"}, "setup": {"duration": 0.0230131309999706, "outcome": "passed"}, "call": {"duration": 0.06329368499996235, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 260, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-calendar]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a88dc0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'calendar', 'input': {'messages': [{'content': 'Extract the event information.', 'role': 'system'}, {'cont...articipants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'type': 'json_schema'}}, 'output': 'valid_calendar_event'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:260: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80a88dc0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002777079999987109, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "lineno": 249, "outcome": "failed", "keywords": ["test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "math"}, "setup": {"duration": 0.02316451399997277, "outcome": "passed"}, "call": {"duration": 0.07800722100000712, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 260, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_structured_output[meta-llama/llama-4-maverick-17b-128e-instruct-math]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a88760>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'math', 'input': {'messages': [{'content': 'You are a helpful math tutor. Guide the user through the solut... ['steps', 'final_answer'], 'title': 'MathReasoning', ...}}, 'type': 'json_schema'}}, 'output': 'valid_math_reasoning'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_structured_output\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_structured_output(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            response_format=case[\"input\"][\"response_format\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:260: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80a88760>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.0002758250000169937, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.022972338999977637, "outcome": "passed"}, "call": {"duration": 0.43275529299995696, "outcome": "passed"}, "teardown": {"duration": 0.0002921309999805999, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02325565899997173, "outcome": "passed"}, "call": {"duration": 0.41789592499998207, "outcome": "passed"}, "teardown": {"duration": 0.00020895100004736378, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 271, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022564877000036176, "outcome": "passed"}, "call": {"duration": 0.09083595999999261, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 282, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa80a11030>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:282: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80a11030>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.0002700539999977991, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02342738999999483, "outcome": "passed"}, "call": {"duration": 0.33928486299998895, "outcome": "passed"}, "teardown": {"duration": 0.00025824300001886513, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022931442000015068, "outcome": "passed"}, "call": {"duration": 0.4153662099999451, "outcome": "passed"}, "teardown": {"duration": 0.00021899900002608774, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 295, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023084363999998914, "outcome": "passed"}, "call": {"duration": 0.07735873600000787, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 306, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa808855a0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:306: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa808855a0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.000264735000030214, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02323751500000526, "outcome": "passed"}, "call": {"duration": 0.450232423999978, "outcome": "passed"}, "teardown": {"duration": 0.00023138300002756296, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 323, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02299856800004818, "outcome": "passed"}, "call": {"duration": 0.3091494249999869, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 345, "message": "AssertionError: assert ' get_weather' == 'get_weather'\n  \n  - get_weather\n  +  get_weather\n  ? +"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 345, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa807a6770>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n        assert len(response.choices[0].message.tool_calls) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert response.choices[0].message.tool_calls[0].function.name == expected_tool_name\nE       AssertionError: assert ' get_weather' == 'get_weather'\nE         \nE         - get_weather\nE         +  get_weather\nE         ? +\n\ntests/verifications/openai_api/test_chat_completion.py:345: AssertionError"}, "teardown": {"duration": 0.00026356299997587485, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 323, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02323808599999211, "outcome": "passed"}, "call": {"duration": 0.10996646099999907, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 334, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa8056cf40>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:334: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa8056cf40>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00026898299995536945, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02300991999999269, "outcome": "passed"}, "call": {"duration": 0.3352782819999902, "outcome": "passed"}, "teardown": {"duration": 0.0002501469999742767, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02285720399999036, "outcome": "passed"}, "call": {"duration": 0.49424943000002486, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 370, "message": "AssertionError: Expected tool call 'get_weather' not found in stream\nassert False\n +  where False = any(<generator object test_chat_streaming_tool_choice_required.<locals>.<genexpr> at 0x7faa80a87450>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 370, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[meta-llama/llama-4-scout-17b-16e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa807a8e20>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n        assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\n        expected_tool_name = case[\"input\"][\"tools\"][0][\"function\"][\"name\"]\n>       assert any(call[\"function\"][\"name\"] == expected_tool_name for call in tool_calls_buffer), (\n            f\"Expected tool call '{expected_tool_name}' not found in stream\"\n        )\nE       AssertionError: Expected tool call 'get_weather' not found in stream\nE       assert False\nE        +  where False = any(<generator object test_chat_streaming_tool_choice_required.<locals>.<genexpr> at 0x7faa80a87450>)\n\ntests/verifications/openai_api/test_chat_completion.py:370: AssertionError"}, "teardown": {"duration": 0.00026222000002462664, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023292787999992015, "outcome": "passed"}, "call": {"duration": 0.05919084300001032, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 358, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa8054fa00>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:358: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa8054fa00>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.0002887590000000273, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "lineno": 374, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.023104796999973587, "outcome": "passed"}, "call": {"duration": 1.4388743599999998, "outcome": "passed"}, "teardown": {"duration": 0.00034170800000765666, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 374, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.023095248000004176, "outcome": "passed"}, "call": {"duration": 1.8018793840000171, "outcome": "passed"}, "teardown": {"duration": 0.000250808999965102, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02614947699998993, "outcome": "passed"}, "call": {"duration": 0.11177472600002147, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 385, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa805a15d0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:385: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa805a15d0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00022440999998707412, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "lineno": 397, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[llama-3.3-70b-versatile-case0]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "case0"}, "setup": {"duration": 0.02212103799996612, "outcome": "passed"}, "call": {"duration": 0.6553861830000187, "outcome": "passed"}, "teardown": {"duration": 0.0002619910000021264, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "lineno": 397, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/llama-4-scout-17b-16e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "case0"}, "setup": {"duration": 0.02317579499998601, "outcome": "passed"}, "call": {"duration": 0.6938401290000229, "outcome": "passed"}, "teardown": {"duration": 0.0002515920000405458, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "lineno": 397, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "case0"}, "setup": {"duration": 0.022747246999983872, "outcome": "passed"}, "call": {"duration": 0.06115389199999299, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 408, "message": ""}, {"path": ".venv/lib/python3.10/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib/python3.10/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[meta-llama/llama-4-maverick-17b-128e-instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7faa80893af0>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:408: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib/python3.10/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa80893af0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n\n.venv/lib/python3.10/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.0003167940000139424, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.023377342999992834, "outcome": "passed"}, "call": {"duration": 0.6571496979999552, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 1 tool calls, but got 0\nassert 0 == 1\n +  where 0 = len((None or []))\n +    where None = ChatCompletionMessage(content='<function=get_weather{\"location\": \"San Francisco, CA\"}</function>', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7faa808c7eb0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 0\nE           assert 0 == 1\nE            +  where 0 = len((None or []))\nE            +    where None = ChatCompletionMessage(content='<function=get_weather{\"location\": \"San Francisco, CA\"}</function>', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.0002766970000038782, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022932470999990073, "outcome": "passed"}, "call": {"duration": 0.44107196900000645, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 1 tool calls, but got 0\nassert 0 == 1\n +  where 0 = len((None or []))\n +    where None = ChatCompletionMessage(content='<function=get_weather {\"location\": \"San Francisco, CA\"} </function>', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7faa8054c310>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 0\nE           assert 0 == 1\nE            +  where 0 = len((None or []))\nE            +    where None = ChatCompletionMessage(content='<function=get_weather {\"location\": \"San Francisco, CA\"} </function>', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.0002818270000375378, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.023169946999985314, "outcome": "passed"}, "call": {"duration": 0.9682065989999842, "outcome": "passed"}, "teardown": {"duration": 0.00028446199996778887, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.023679820000040763, "outcome": "passed"}, "call": {"duration": 1.9315171199999668, "outcome": "passed"}, "teardown": {"duration": 0.00025290300004598976, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.023820562999958383, "outcome": "passed"}, "call": {"duration": 1.370288372999994, "outcome": "passed"}, "teardown": {"duration": 0.0002453989999935402, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02304807900003425, "outcome": "passed"}, "call": {"duration": 0.3107991829999719, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_9xpy', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_9xpy', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9xpy', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7faa80ac79a0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_9xpy', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_9xpy', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9xpy', function=Function(arguments='{\"query\": \"Latin name of the Sun\"}', name='brave_search'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.0003412180000168519, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.023385501000007025, "outcome": "passed"}, "call": {"duration": 0.5792316399999891, "outcome": "passed"}, "teardown": {"duration": 0.00024340499999198073, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02312801200002923, "outcome": "passed"}, "call": {"duration": 0.6369656960000043, "outcome": "passed"}, "teardown": {"duration": 0.0002482239999608282, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022821690999990096, "outcome": "passed"}, "call": {"duration": 1.3837174290000007, "outcome": "passed"}, "teardown": {"duration": 0.0002806649999911315, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02288757399998076, "outcome": "passed"}, "call": {"duration": 1.2110676329999706, "outcome": "passed"}, "teardown": {"duration": 0.0002667589999418851, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022859061000076508, "outcome": "passed"}, "call": {"duration": 2.0687653029999638, "outcome": "passed"}, "teardown": {"duration": 0.0002350190000015573, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0234641910000164, "outcome": "passed"}, "call": {"duration": 0.6502822920000426, "outcome": "passed"}, "teardown": {"duration": 0.00028006299999105977, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02304632900006709, "outcome": "passed"}, "call": {"duration": 1.1825753159999977, "outcome": "passed"}, "teardown": {"duration": 0.00024013899997044064, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.023700781999991705, "outcome": "passed"}, "call": {"duration": 2.632415740000056, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4syz', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='call_4syz', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4syz', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7faa80926d70>\nmodel = 'meta-llama/llama-4-maverick-17b-128e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4syz', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='call_4syz', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4syz', function=Function(arguments='{\"date\": \"2025-03-03\", \"time\": \"10:00\"}', name='get_event'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.0002776490000542253, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02337987400005659, "outcome": "passed"}, "call": {"duration": 1.6015815450000446, "outcome": "passed"}, "teardown": {"duration": 0.00027903199998036143, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-text_then_weather_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.023413806999997178, "outcome": "passed"}, "call": {"duration": 0.9636081849999982, "outcome": "passed"}, "teardown": {"duration": 0.00025290200005656516, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-weather_tool_then_text]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.023497027000075832, "outcome": "passed"}, "call": {"duration": 0.6588558779999403, "outcome": "passed"}, "teardown": {"duration": 0.00028529300004720426, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-add_product_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "add_product_tool"}, "setup": {"duration": 0.023075598999980684, "outcome": "passed"}, "call": {"duration": 0.730305033000036, "outcome": "passed"}, "teardown": {"duration": 0.00020763800000622723, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-get_then_create_event_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02208866900002704, "outcome": "passed"}, "call": {"duration": 1.5579970489999369, "outcome": "passed"}, "teardown": {"duration": 0.00020484300000589428, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02198862199998075, "outcome": "passed"}, "call": {"duration": 0.9942424259999143, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/.venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 91, "message": "openai.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 551, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 732, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib/python3.10/site-packages/openai/_streaming.py", "lineno": 91, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama-3.3-70b-versatile-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7faa807bfcd0>\nmodel = 'llama-3.3-70b-versatile', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:551: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:732: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib/python3.10/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7faa80893ca0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n                    raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\n    \n                yield process_data(data=data, cast_to=cast_to, response=response)\n    \n            else:\n                data = sse.json()\n    \n                if sse.event == \"error\" and is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\n\n.venv/lib/python3.10/site-packages/openai/_streaming.py:91: APIError"}, "teardown": {"duration": 0.0002565400000094087, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.023158379999927092, "outcome": "passed"}, "call": {"duration": 0.40083286599997336, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'I'm not able to complete this task as it falls outside of the scope of the functions I have been given.'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7faa80588270>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7faa8077fcd0>\nmodel = 'meta-llama/llama-4-scout-17b-16e-instruct', provider = 'groq'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'I'm not able to complete this task as it falls outside of the scope of the functions I have been given.'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7faa80588270>)\n\ntests/verifications/openai_api/test_chat_completion.py:595: AssertionError"}, "teardown": {"duration": 0.0002660070000501946, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022848760999977458, "outcome": "passed"}, "call": {"duration": 0.6037042519999432, "outcome": "passed"}, "teardown": {"duration": 0.0002593949999436518, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02276004499992723, "outcome": "passed"}, "call": {"duration": 0.8183429019999267, "outcome": "passed"}, "teardown": {"duration": 0.0002322740000408885, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.023153290999971432, "outcome": "passed"}, "call": {"duration": 1.28212070699999, "outcome": "passed"}, "teardown": {"duration": 0.00020783799993751018, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02212066100003085, "outcome": "passed"}, "call": {"duration": 1.256430145999957, "outcome": "passed"}, "teardown": {"duration": 0.00021423099997264217, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022200969999971676, "outcome": "passed"}, "call": {"duration": 0.9589515029999802, "outcome": "passed"}, "teardown": {"duration": 0.0002434960000528008, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022720010999933038, "outcome": "passed"}, "call": {"duration": 1.0166739019999795, "outcome": "passed"}, "teardown": {"duration": 0.0002603359999966415, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.023040554999965934, "outcome": "passed"}, "call": {"duration": 0.8129517009999745, "outcome": "passed"}, "teardown": {"duration": 0.00028715600001305575, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02351403400007257, "outcome": "passed"}, "call": {"duration": 1.8198240330000317, "outcome": "passed"}, "teardown": {"duration": 0.0002562689999194845, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.024057842000047458, "outcome": "passed"}, "call": {"duration": 1.5495074709999699, "outcome": "passed"}, "teardown": {"duration": 0.00025455599995893863, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=False]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=False]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "stream=False"}, "setup": {"duration": 0.025332093000088207, "outcome": "passed"}, "call": {"duration": 0.000421727999992072, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.0002472620000162351, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=True]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama-3.3-70b-versatile-stream=True]", "parametrize", "pytestmark", "llama-3.3-70b-versatile-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "llama-3.3-70b-versatile", "case_id": "stream=True"}, "setup": {"duration": 0.039951549999955205, "outcome": "passed"}, "call": {"duration": 0.00020299900006648386, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama-3.3-70b-versatile on provider groq based on config.')"}, "teardown": {"duration": 0.00020810900002743438, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=False]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=False]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.02396692100001019, "outcome": "passed"}, "call": {"duration": 2.3765875309999274, "outcome": "passed"}, "teardown": {"duration": 0.0003694309999673351, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=True]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-scout-17b-16e-instruct-stream=True]", "parametrize", "pytestmark", "meta-llama/llama-4-scout-17b-16e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-scout-17b-16e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.02380924600004164, "outcome": "passed"}, "call": {"duration": 2.0572469129999718, "outcome": "passed"}, "teardown": {"duration": 0.0002520109999295528, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=False]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=False]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "stream=False"}, "setup": {"duration": 0.02379397799995786, "outcome": "passed"}, "call": {"duration": 4.178629408999996, "outcome": "passed"}, "teardown": {"duration": 0.0002477430000453751, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=True]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[meta-llama/llama-4-maverick-17b-128e-instruct-stream=True]", "parametrize", "pytestmark", "meta-llama/llama-4-maverick-17b-128e-instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "meta-llama/llama-4-maverick-17b-128e-instruct", "case_id": "stream=True"}, "setup": {"duration": 0.023329594999950132, "outcome": "passed"}, "call": {"duration": 3.9087815240000054, "outcome": "passed"}, "teardown": {"duration": 0.002302515999986099, "outcome": "passed"}}]}