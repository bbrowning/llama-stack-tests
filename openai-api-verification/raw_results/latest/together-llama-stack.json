{"created": 1745547359.2039607, "duration": 205.02541303634644, "exitcode": 1, "root": "/home/runner/work/llama-stack-tests/llama-stack-tests", "environment": {}, "summary": {"passed": 96, "skipped": 4, "failed": 14, "total": 114, "collected": 114}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "type": "Function", "lineno": 599}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "earth"}, "setup": {"duration": 0.054092982000042866, "outcome": "passed"}, "call": {"duration": 0.46331714900009047, "outcome": "passed"}, "teardown": {"duration": 0.0002058240000906153, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "saturn"}, "setup": {"duration": 0.023328175000074225, "outcome": "passed"}, "call": {"duration": 0.5733643780000648, "outcome": "passed"}, "teardown": {"duration": 0.0002086490000010599, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "earth"}, "setup": {"duration": 0.023633125000060318, "outcome": "passed"}, "call": {"duration": 0.4968017730000156, "outcome": "passed"}, "teardown": {"duration": 0.00021370799993292167, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "saturn"}, "setup": {"duration": 0.02368741499992666, "outcome": "passed"}, "call": {"duration": 0.9431072720000202, "outcome": "passed"}, "teardown": {"duration": 0.0002740709999216051, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "earth"}, "setup": {"duration": 0.023347931999978755, "outcome": "passed"}, "call": {"duration": 1.105237554999917, "outcome": "passed"}, "teardown": {"duration": 0.00021552199996222043, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "saturn"}, "setup": {"duration": 0.024411944999997104, "outcome": "passed"}, "call": {"duration": 0.554288941999971, "outcome": "passed"}, "teardown": {"duration": 0.00024387499991007644, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "earth"}, "setup": {"duration": 0.02367008100009116, "outcome": "passed"}, "call": {"duration": 0.46061060499994255, "outcome": "passed"}, "teardown": {"duration": 0.0002497560000165322, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "saturn"}, "setup": {"duration": 0.02390220400002363, "outcome": "passed"}, "call": {"duration": 0.9889334169999984, "outcome": "passed"}, "teardown": {"duration": 0.000214450000044053, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "earth"}, "setup": {"duration": 0.0230733970000756, "outcome": "passed"}, "call": {"duration": 1.7543001800000866, "outcome": "passed"}, "teardown": {"duration": 0.00021878899997318513, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "saturn"}, "setup": {"duration": 0.03044540100006543, "outcome": "passed"}, "call": {"duration": 0.3321511139999984, "outcome": "passed"}, "teardown": {"duration": 0.0003035359999330467, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "earth"}, "setup": {"duration": 0.023014246999991883, "outcome": "passed"}, "call": {"duration": 1.1350579840000137, "outcome": "passed"}, "teardown": {"duration": 0.00021390899996731605, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "saturn"}, "setup": {"duration": 0.022870668999985355, "outcome": "passed"}, "call": {"duration": 0.3510176510000065, "outcome": "passed"}, "teardown": {"duration": 0.00025860300002022996, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_missing"}, "setup": {"duration": 0.022343535000004522, "outcome": "passed"}, "call": {"duration": 0.00637211900004786, "outcome": "passed"}, "teardown": {"duration": 0.00024508700005299033, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022455525000054877, "outcome": "passed"}, "call": {"duration": 0.006973727000058716, "outcome": "passed"}, "teardown": {"duration": 0.0001836420000245198, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022351361000005454, "outcome": "passed"}, "call": {"duration": 0.16823595000005298, "outcome": "passed"}, "teardown": {"duration": 0.0003748589999759133, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023206494999953975, "outcome": "passed"}, "call": {"duration": 0.09601570900008483, "outcome": "passed"}, "teardown": {"duration": 0.00024435600005290325, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022664104000000407, "outcome": "passed"}, "call": {"duration": 0.126383369999985, "outcome": "passed"}, "teardown": {"duration": 0.0002547550000144838, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022738903999993454, "outcome": "passed"}, "call": {"duration": 0.006389391999960026, "outcome": "passed"}, "teardown": {"duration": 0.00023469699999623117, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.023323934999893936, "outcome": "passed"}, "call": {"duration": 0.006238166999992245, "outcome": "passed"}, "teardown": {"duration": 0.00018210000007456983, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.024285417999976744, "outcome": "passed"}, "call": {"duration": 0.11837102399999822, "outcome": "passed"}, "teardown": {"duration": 0.0003505040000391091, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.03194715300003281, "outcome": "passed"}, "call": {"duration": 0.096800488000099, "outcome": "passed"}, "teardown": {"duration": 0.00023135100002491527, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022832578000020476, "outcome": "passed"}, "call": {"duration": 0.11772166200000811, "outcome": "passed"}, "teardown": {"duration": 0.0003385820000403328, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_missing"}, "setup": {"duration": 0.022854318999975476, "outcome": "passed"}, "call": {"duration": 0.005603976000088551, "outcome": "passed"}, "teardown": {"duration": 0.00021660400000200752, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022267784999939977, "outcome": "passed"}, "call": {"duration": 0.0061182580000149756, "outcome": "passed"}, "teardown": {"duration": 0.00022058199999719363, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02254042300000947, "outcome": "passed"}, "call": {"duration": 0.11751925600003688, "outcome": "passed"}, "teardown": {"duration": 0.00023909599997296027, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.02315297699999519, "outcome": "passed"}, "call": {"duration": 0.09484304000000066, "outcome": "passed"}, "teardown": {"duration": 0.00024647999998705927, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.022511269000005996, "outcome": "passed"}, "call": {"duration": 0.11833118300000933, "outcome": "passed"}, "teardown": {"duration": 0.00020080499996311119, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_missing"}, "setup": {"duration": 0.022499529000015173, "outcome": "passed"}, "call": {"duration": 0.005631995000044299, "outcome": "passed"}, "teardown": {"duration": 0.00019577499995193648, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0220571419999942, "outcome": "passed"}, "call": {"duration": 0.006089861999953428, "outcome": "passed"}, "teardown": {"duration": 0.00021385900004133873, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022087298000087685, "outcome": "passed"}, "call": {"duration": 0.10434262499995839, "outcome": "passed"}, "teardown": {"duration": 0.0002138490000334059, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.022851472999946054, "outcome": "passed"}, "call": {"duration": 0.08040228500010471, "outcome": "passed"}, "teardown": {"duration": 0.0002691220000770045, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.034445855000058145, "outcome": "passed"}, "call": {"duration": 0.10244905299998663, "outcome": "passed"}, "teardown": {"duration": 0.00022548100002950378, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_missing"}, "setup": {"duration": 0.022945529000026, "outcome": "passed"}, "call": {"duration": 0.005483808999997564, "outcome": "passed"}, "teardown": {"duration": 0.00022505000004002795, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.02319175900004211, "outcome": "passed"}, "call": {"duration": 0.006875076999904195, "outcome": "passed"}, "teardown": {"duration": 0.0002261319999661282, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.02222155799995562, "outcome": "passed"}, "call": {"duration": 0.1032959369999844, "outcome": "passed"}, "teardown": {"duration": 0.0002591840000150114, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.023036409000042113, "outcome": "passed"}, "call": {"duration": 0.22263024200003656, "outcome": "passed"}, "teardown": {"duration": 0.0002874159999919357, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.024062092000008306, "outcome": "passed"}, "call": {"duration": 0.10233534300004976, "outcome": "passed"}, "teardown": {"duration": 0.0003440419999378719, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_missing"}, "setup": {"duration": 0.02332978499998717, "outcome": "passed"}, "call": {"duration": 0.005893044999993435, "outcome": "passed"}, "teardown": {"duration": 0.00023256399992988008, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.022336342000016884, "outcome": "passed"}, "call": {"duration": 0.006197471999939808, "outcome": "passed"}, "teardown": {"duration": 0.00021759599997039913, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.022262134000015976, "outcome": "passed"}, "call": {"duration": 0.1027851119999923, "outcome": "passed"}, "teardown": {"duration": 0.00032831299995450536, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.024377730999958658, "outcome": "passed"}, "call": {"duration": 0.08006875699993543, "outcome": "passed"}, "teardown": {"duration": 0.00023775399995429325, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.02362223200009339, "outcome": "passed"}, "call": {"duration": 0.10174246400003994, "outcome": "passed"}, "teardown": {"duration": 0.00022785600003771833, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 183, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.030094105999978638, "outcome": "passed"}, "call": {"duration": 0.00016804400002001785, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 192, 'Skipped: Skipping test_chat_non_streaming_image for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.00016410599994287622, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 183, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.022318248000033236, "outcome": "passed"}, "call": {"duration": 1.7532953339999722, "outcome": "passed"}, "teardown": {"duration": 0.000249956000061502, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 183, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.023577016999979605, "outcome": "passed"}, "call": {"duration": 5.2990368210000725, "outcome": "passed"}, "teardown": {"duration": 0.00023819400007596414, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 202, "outcome": "skipped", "keywords": ["test_chat_streaming_image[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.024145676999978605, "outcome": "passed"}, "call": {"duration": 0.00018810099993515905, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 211, 'Skipped: Skipping test_chat_streaming_image for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.00017320299991752108, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 202, "outcome": "passed", "keywords": ["test_chat_streaming_image[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02219342599994434, "outcome": "passed"}, "call": {"duration": 2.1722407219999695, "outcome": "passed"}, "teardown": {"duration": 0.0002999499999987165, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 202, "outcome": "passed", "keywords": ["test_chat_streaming_image[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.0224561759999915, "outcome": "passed"}, "call": {"duration": 4.066889005000007, "outcome": "passed"}, "teardown": {"duration": 0.00024033800002598582, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "calendar"}, "setup": {"duration": 0.02270578199988904, "outcome": "passed"}, "call": {"duration": 1.2448209110000334, "outcome": "passed"}, "teardown": {"duration": 0.00035102599997571815, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "math"}, "setup": {"duration": 0.023713442000030227, "outcome": "passed"}, "call": {"duration": 8.234111352000014, "outcome": "passed"}, "teardown": {"duration": 0.0003163210000138861, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "calendar"}, "setup": {"duration": 0.024510107999958564, "outcome": "passed"}, "call": {"duration": 0.45523854600003233, "outcome": "passed"}, "teardown": {"duration": 0.00035391999995226797, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "math"}, "setup": {"duration": 0.023696459999996478, "outcome": "passed"}, "call": {"duration": 2.9675324470000533, "outcome": "passed"}, "teardown": {"duration": 0.00027176699995834497, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "calendar"}, "setup": {"duration": 0.023958459000027688, "outcome": "passed"}, "call": {"duration": 0.6390657279999914, "outcome": "passed"}, "teardown": {"duration": 0.00024596900004780764, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "math"}, "setup": {"duration": 0.023857269999894015, "outcome": "passed"}, "call": {"duration": 2.5572099229999594, "outcome": "passed"}, "teardown": {"duration": 0.0002510680000114007, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "calendar"}, "setup": {"duration": 0.03303147600001921, "outcome": "passed"}, "call": {"duration": 4.067537778999963, "outcome": "passed"}, "teardown": {"duration": 0.0002937180000799344, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "math"}, "setup": {"duration": 0.022811253999975634, "outcome": "passed"}, "call": {"duration": 3.859979328999998, "outcome": "passed"}, "teardown": {"duration": 0.00028444999998100684, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "calendar"}, "setup": {"duration": 0.022626597999988007, "outcome": "passed"}, "call": {"duration": 0.4798718849999659, "outcome": "passed"}, "teardown": {"duration": 0.00024739200000567507, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "math"}, "setup": {"duration": 0.022589459000073475, "outcome": "passed"}, "call": {"duration": 3.1782739969999056, "outcome": "passed"}, "teardown": {"duration": 0.00022457899990513397, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "calendar"}, "setup": {"duration": 0.02278636899995945, "outcome": "passed"}, "call": {"duration": 0.72403324600009, "outcome": "passed"}, "teardown": {"duration": 0.00021810699990965077, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "math"}, "setup": {"duration": 0.022627490999980182, "outcome": "passed"}, "call": {"duration": 2.410438026999941, "outcome": "passed"}, "teardown": {"duration": 0.00028695499997866136, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02255124999999225, "outcome": "passed"}, "call": {"duration": 0.6256033839999873, "outcome": "passed"}, "teardown": {"duration": 0.0002619690000074115, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02316375199995946, "outcome": "passed"}, "call": {"duration": 0.3473456920000899, "outcome": "passed"}, "teardown": {"duration": 0.00026233899995986576, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.023141700999985915, "outcome": "passed"}, "call": {"duration": 7.980674398000019, "outcome": "passed"}, "teardown": {"duration": 0.0002343979999750445, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.0228184909999527, "outcome": "passed"}, "call": {"duration": 1.1392788809999956, "outcome": "passed"}, "teardown": {"duration": 0.00022161300000789197, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.023048361000064688, "outcome": "passed"}, "call": {"duration": 0.3546487009999737, "outcome": "passed"}, "teardown": {"duration": 0.0003505040000391091, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02282708799998545, "outcome": "passed"}, "call": {"duration": 0.7303142329999446, "outcome": "passed"}, "teardown": {"duration": 0.0003659420000303726, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.03334651899990604, "outcome": "passed"}, "call": {"duration": 0.4195019940000293, "outcome": "passed"}, "teardown": {"duration": 0.00025252000000364205, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.023831221000023106, "outcome": "passed"}, "call": {"duration": 0.2892287330000727, "outcome": "passed"}, "teardown": {"duration": 0.00025582800003576267, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.023157132999926944, "outcome": "passed"}, "call": {"duration": 0.8939535299999761, "outcome": "passed"}, "teardown": {"duration": 0.00024474600002122315, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022939848999953938, "outcome": "passed"}, "call": {"duration": 0.46418024199999763, "outcome": "passed"}, "teardown": {"duration": 0.00020985200001177873, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02300550099994325, "outcome": "passed"}, "call": {"duration": 0.33348272099999576, "outcome": "passed"}, "teardown": {"duration": 0.00020334900000307243, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02253933099996175, "outcome": "passed"}, "call": {"duration": 1.0527554089999285, "outcome": "passed"}, "teardown": {"duration": 0.0002412800000684001, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.02227909600003386, "outcome": "passed"}, "call": {"duration": 0.5040657649999503, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] is None\n +  where [ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]).tool_calls\n +    where ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]), seed=15240459574927841000).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fd3970c7160>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] is None\nE        +  where [ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]).tool_calls\nE        +    where ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_cry745ipfshciehzh8t10kbx', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]), seed=15240459574927841000).message\n\ntests/verifications/openai_api/test_chat_completion.py:394: AssertionError"}, "teardown": {"duration": 0.00024088999998639338, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02238961200009726, "outcome": "passed"}, "call": {"duration": 0.2876468050000085, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] is None\n +  where [ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]).tool_calls\n +    where ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]), seed=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fd396f678b0>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] is None\nE        +  where [ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]).tool_calls\nE        +    where ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q9m4p4911h0e13cq7z2j0jh5', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]), seed=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:394: AssertionError"}, "teardown": {"duration": 0.0002200400000447189, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02237885199997436, "outcome": "passed"}, "call": {"duration": 0.3993402880000758, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] is None\n +  where [ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]).tool_calls\n +    where ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]), seed=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fd39701ea10>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] is None\nE        +  where [ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]).tool_calls\nE        +    where ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_64wrwpukfq7ro5ibfdm9e5le', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_weather'), type='function', index=0)]), seed=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:394: AssertionError"}, "teardown": {"duration": 0.00022951799996917543, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "lineno": 397, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "case0"}, "setup": {"duration": 0.022729195000010805, "outcome": "passed"}, "call": {"duration": 0.5303840010000158, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError: Expected no tool call chunks when tool_choice='none'\nassert not [ChoiceDeltaToolCall(index=0, id='call_u0p5iktmfrmy2dh0x6r2ud1r', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\n +  where [ChoiceDeltaToolCall(index=0, id='call_u0p5iktmfrmy2dh0x6r2ud1r', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')] = ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_u0p5iktmfrmy2dh0x6r2ud1r', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fd397195c90>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                content += delta.content\n>           assert not delta.tool_calls, \"Expected no tool call chunks when tool_choice='none'\"\nE           AssertionError: Expected no tool call chunks when tool_choice='none'\nE           assert not [ChoiceDeltaToolCall(index=0, id='call_u0p5iktmfrmy2dh0x6r2ud1r', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\nE            +  where [ChoiceDeltaToolCall(index=0, id='call_u0p5iktmfrmy2dh0x6r2ud1r', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')] = ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_u0p5iktmfrmy2dh0x6r2ud1r', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:421: AssertionError"}, "teardown": {"duration": 0.00021176500001729437, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "lineno": 397, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "case0"}, "setup": {"duration": 0.02263840600005551, "outcome": "passed"}, "call": {"duration": 0.3523571550000497, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError: Expected no tool call chunks when tool_choice='none'\nassert not [ChoiceDeltaToolCall(index=0, id='call_5iyo226wzcrpthebew7gigd1', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\n +  where [ChoiceDeltaToolCall(index=0, id='call_5iyo226wzcrpthebew7gigd1', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')] = ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_5iyo226wzcrpthebew7gigd1', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fd3970c62f0>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                content += delta.content\n>           assert not delta.tool_calls, \"Expected no tool call chunks when tool_choice='none'\"\nE           AssertionError: Expected no tool call chunks when tool_choice='none'\nE           assert not [ChoiceDeltaToolCall(index=0, id='call_5iyo226wzcrpthebew7gigd1', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\nE            +  where [ChoiceDeltaToolCall(index=0, id='call_5iyo226wzcrpthebew7gigd1', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')] = ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_5iyo226wzcrpthebew7gigd1', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:421: AssertionError"}, "teardown": {"duration": 0.000241100000039296, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "lineno": 397, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "case0"}, "setup": {"duration": 0.02249949800000195, "outcome": "passed"}, "call": {"duration": 0.5536362819999567, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError: Expected no tool call chunks when tool_choice='none'\nassert not [ChoiceDeltaToolCall(index=0, id='call_xidxiwsisqfpr2gcvcwuuvqm', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\n +  where [ChoiceDeltaToolCall(index=0, id='call_xidxiwsisqfpr2gcvcwuuvqm', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')] = ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_xidxiwsisqfpr2gcvcwuuvqm', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fd39706f430>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                content += delta.content\n>           assert not delta.tool_calls, \"Expected no tool call chunks when tool_choice='none'\"\nE           AssertionError: Expected no tool call chunks when tool_choice='none'\nE           assert not [ChoiceDeltaToolCall(index=0, id='call_xidxiwsisqfpr2gcvcwuuvqm', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]\nE            +  where [ChoiceDeltaToolCall(index=0, id='call_xidxiwsisqfpr2gcvcwuuvqm', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')] = ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id='call_xidxiwsisqfpr2gcvcwuuvqm', function=ChoiceDeltaToolCallFunction(arguments='', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:421: AssertionError"}, "teardown": {"duration": 0.00022277499999745487, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02362520700000914, "outcome": "passed"}, "call": {"duration": 1.056520528999954, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 512, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'I am not able to execute this task as it exceeds the limitations of the functions I have been given.'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fd3970d93f0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 512, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd3971d5c00>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'I am not able to execute this task as it exceeds the limitations of the functions I have been given.'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fd3970d93f0>)\n\ntests/verifications/openai_api/test_chat_completion.py:512: AssertionError"}, "teardown": {"duration": 0.0002186670000128288, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02228393899997627, "outcome": "passed"}, "call": {"duration": 0.6937453340000275, "outcome": "passed"}, "teardown": {"duration": 0.00027069500004017755, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "add_product_tool"}, "setup": {"duration": 0.023664700999916022, "outcome": "passed"}, "call": {"duration": 2.1915911570000617, "outcome": "passed"}, "teardown": {"duration": 0.00023584899997786124, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022717467000006764, "outcome": "passed"}, "call": {"duration": 4.179911853000021, "outcome": "passed"}, "teardown": {"duration": 0.00022867600000608945, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.022988445999999385, "outcome": "passed"}, "call": {"duration": 2.905252833000077, "outcome": "passed"}, "teardown": {"duration": 0.00020846899997195578, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022868212000048516, "outcome": "passed"}, "call": {"duration": 0.449680613000055, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4mj41kpnwyj7htvtjjvrkbs4', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_4mj41kpnwyj7htvtjjvrkbs4', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4mj41kpnwyj7htvtjjvrkbs4', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd396effd60>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4mj41kpnwyj7htvtjjvrkbs4', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_4mj41kpnwyj7htvtjjvrkbs4', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4mj41kpnwyj7htvtjjvrkbs4', function=Function(arguments='{\"location\":\"Sol, Latin\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.00023969699998360738, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.02246639099996628, "outcome": "passed"}, "call": {"duration": 1.1418823159999647, "outcome": "passed"}, "teardown": {"duration": 0.0002081579999639871, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.02388352399998439, "outcome": "passed"}, "call": {"duration": 1.1756561110000803, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4xw8am68y8cka0pqpdvrsipk', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_4xw8am68y8cka0pqpdvrsipk', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4xw8am68y8cka0pqpdvrsipk', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd3971a9390>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_4xw8am68y8cka0pqpdvrsipk', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_4xw8am68y8cka0pqpdvrsipk', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_4xw8am68y8cka0pqpdvrsipk', function=Function(arguments='{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', name='addProduct'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.0002365620000546187, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02260105300001669, "outcome": "passed"}, "call": {"duration": 2.9999741649999123, "outcome": "passed"}, "teardown": {"duration": 0.00023097099995084136, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.023758025000006455, "outcome": "passed"}, "call": {"duration": 2.2946724199999835, "outcome": "passed"}, "teardown": {"duration": 0.0002304500000036569, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022878904999970473, "outcome": "passed"}, "call": {"duration": 2.0357733550000603, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 0 tool calls, but got 2\nassert 2 == 0\n +  where 2 = len(([ChatCompletionMessageToolCall(id='call_8b50v3yp9gaiif1hvhq6332m', function=Function(arguments='{\"location\":\"None\"}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='call_bc1ffdivzekhip0j4d4kpue8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=1)]))\n +    where [ChatCompletionMessageToolCall(id='call_8b50v3yp9gaiif1hvhq6332m', function=Function(arguments='{\"location\":\"None\"}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='call_bc1ffdivzekhip0j4d4kpue8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=1)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_8b50v3yp9gaiif1hvhq6332m', function=Function(arguments='{\"location\":\"None\"}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='call_bc1ffdivzekhip0j4d4kpue8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=1)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd3971aace0>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 2\nE           assert 2 == 0\nE            +  where 2 = len(([ChatCompletionMessageToolCall(id='call_8b50v3yp9gaiif1hvhq6332m', function=Function(arguments='{\"location\":\"None\"}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='call_bc1ffdivzekhip0j4d4kpue8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=1)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_8b50v3yp9gaiif1hvhq6332m', function=Function(arguments='{\"location\":\"None\"}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='call_bc1ffdivzekhip0j4d4kpue8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=1)] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_8b50v3yp9gaiif1hvhq6332m', function=Function(arguments='{\"location\":\"None\"}', name='get_weather'), type='function', index=0), ChatCompletionMessageToolCall(id='call_bc1ffdivzekhip0j4d4kpue8', function=Function(arguments='{\"location\":\"\"}', name='get_weather'), type='function', index=1)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.0002703239999846119, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022700140000097235, "outcome": "passed"}, "call": {"duration": 0.99599983600001, "outcome": "passed"}, "teardown": {"duration": 0.00023451799995655165, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "add_product_tool"}, "setup": {"duration": 0.023448185999995985, "outcome": "passed"}, "call": {"duration": 1.7954129400000056, "outcome": "passed"}, "teardown": {"duration": 0.00024869399999261077, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02290936800000054, "outcome": "passed"}, "call": {"duration": 17.12391349199993, "outcome": "passed"}, "teardown": {"duration": 0.00023377599995910714, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02628782700003285, "outcome": "passed"}, "call": {"duration": 4.852209033999998, "outcome": "passed"}, "teardown": {"duration": 0.00024530700000013894, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02325612100003127, "outcome": "passed"}, "call": {"duration": 0.4222592739999982, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 566, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_escjnz3241ev4pbsr4w9l1un', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 566, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd3970a7b20>\nmodel = 'together/meta-llama/Llama-3.3-70B-Instruct-Turbo'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"location\":\"San Francisco, CA\"}', 'name': 'get_weather'}, 'id': 'call_escjnz3241ev4pbsr4w9l1un', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:566: AssertionError"}, "teardown": {"duration": 0.00023474800002531993, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0224987090000468, "outcome": "passed"}, "call": {"duration": 1.236348330999931, "outcome": "passed"}, "teardown": {"duration": 0.00021955900001557893, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "add_product_tool"}, "setup": {"duration": 0.022847157999990486, "outcome": "passed"}, "call": {"duration": 2.41848853099998, "outcome": "passed"}, "teardown": {"duration": 0.00020792800000890566, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.02314269200007857, "outcome": "passed"}, "call": {"duration": 4.751464042999942, "outcome": "passed"}, "teardown": {"duration": 0.00020766700004060112, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.02253345500002979, "outcome": "passed"}, "call": {"duration": 5.835556485000097, "outcome": "passed"}, "teardown": {"duration": 0.00021715499997299048, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.022603306999940287, "outcome": "passed"}, "call": {"duration": 0.6609237679999751, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'I am not equipped to handle this task with the functions I have at hand.'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fd3970db3e0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd396f66d70>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'I am not equipped to handle this task with the functions I have at hand.'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fd3970db3e0>)\n\ntests/verifications/openai_api/test_chat_completion.py:595: AssertionError"}, "teardown": {"duration": 0.00023303399996166263, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022475146999909157, "outcome": "passed"}, "call": {"duration": 1.454333719000033, "outcome": "passed"}, "teardown": {"duration": 0.0002978759999905378, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "add_product_tool"}, "setup": {"duration": 0.022459066999999777, "outcome": "passed"}, "call": {"duration": 1.3864314950000107, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 566, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', 'name': 'addProduct'}, 'id': 'call_9yqxm3z15oc7ecmctpn5qfo5', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 566, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd39706ece0>\nmodel = 'together/meta-llama/Llama-4-Scout-17B-16E-Instruct'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"inStock\":true,\"name\":\"Widget\",\"price\":19.99,\"tags\":[\"new\",\"sale\"]}', 'name': 'addProduct'}, 'id': 'call_9yqxm3z15oc7ecmctpn5qfo5', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:566: AssertionError"}, "teardown": {"duration": 0.00023585999997521867, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022225806999927045, "outcome": "passed"}, "call": {"duration": 2.684371073999955, "outcome": "passed"}, "teardown": {"duration": 0.00033641700008502085, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0232543549999491, "outcome": "passed"}, "call": {"duration": 1.5951176969999779, "outcome": "passed"}, "teardown": {"duration": 0.0002499870000747251, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.02277530099991054, "outcome": "passed"}, "call": {"duration": 0.4671666539999251, "outcome": "failed", "crash": {"path": "/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError: Expected one of ['sol'] in content, but got: '{\"name\": null, \"parameters\": null}'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fd3970d8820>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fd397196b90>\nmodel = 'together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8'\nprovider = 'together-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: '{\"name\": null, \"parameters\": null}'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fd3970d8820>)\n\ntests/verifications/openai_api/test_chat_completion.py:595: AssertionError"}, "teardown": {"duration": 0.00023740300002828008, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.022050949999993463, "outcome": "passed"}, "call": {"duration": 22.374241494000103, "outcome": "passed"}, "teardown": {"duration": 0.0002794709999989209, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "add_product_tool"}, "setup": {"duration": 0.022409786000025633, "outcome": "passed"}, "call": {"duration": 8.529052062000005, "outcome": "passed"}, "teardown": {"duration": 0.0003278309999359408, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.022416839000015898, "outcome": "passed"}, "call": {"duration": 4.409467990999929, "outcome": "passed"}, "teardown": {"duration": 0.0002296680000881679, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0222027549999666, "outcome": "passed"}, "call": {"duration": 9.981282055999941, "outcome": "passed"}, "teardown": {"duration": 0.00019589599992286821, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "stream=False"}, "setup": {"duration": 0.02289777500004675, "outcome": "passed"}, "call": {"duration": 0.00015369699997336284, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.000160359000005883, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True]", "parametrize", "pytestmark", "together/meta-llama/Llama-3.3-70B-Instruct-Turbo-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-3.3-70B-Instruct-Turbo", "case_id": "stream=True"}, "setup": {"duration": 0.022934814999985065, "outcome": "passed"}, "call": {"duration": 0.00013199600005009415, "outcome": "skipped", "longrepr": "('/home/runner/work/llama-stack-tests/llama-stack-tests/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model together/meta-llama/Llama-3.3-70B-Instruct-Turbo on provider together-llama-stack based on config.')"}, "teardown": {"duration": 0.00017320400002063252, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "stream=False"}, "setup": {"duration": 0.022638491000066097, "outcome": "passed"}, "call": {"duration": 3.5545140540000375, "outcome": "passed"}, "teardown": {"duration": 0.00027604500007782917, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Scout-17B-16E-Instruct-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Scout-17B-16E-Instruct", "case_id": "stream=True"}, "setup": {"duration": 0.02658241899996483, "outcome": "passed"}, "call": {"duration": 3.180865625000024, "outcome": "passed"}, "teardown": {"duration": 0.00030960799995227717, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "stream=False"}, "setup": {"duration": 0.02273752400003559, "outcome": "passed"}, "call": {"duration": 3.56732260199999, "outcome": "passed"}, "teardown": {"duration": 0.00019692699993356655, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "lineno": 599, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True]", "parametrize", "pytestmark", "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "llama-stack-tests", ""], "metadata": {"model": "together/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8", "case_id": "stream=True"}, "setup": {"duration": 0.02592363700000533, "outcome": "passed"}, "call": {"duration": 4.499118057000032, "outcome": "passed"}, "teardown": {"duration": 0.002629054000067299, "outcome": "passed"}}]}